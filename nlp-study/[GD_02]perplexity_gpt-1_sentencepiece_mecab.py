# -*- coding: utf-8 -*-
"""[GD_02]Perplexity_GPT-1_SentencePiece_MeCab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uni-LMRWVGhk2UxmRpCxrKvi11T47pMf

##멋진 단어사전 만들기

Korean Parallel Corpora
<br/>데이터 정보
<br/>데이터 탐색
<br/>SentencePiece
<br/>Tokenizer

Naver sentiment movie corpus
<br/>데이터 정보
<br/>데이터 탐색

SentencePiece
<br/>Corpus
<br/>Pretrained Model
<br/>Config
<br/>Common Class
<br/>Decoder
<br/>GPT-1
<br/>Data Loader
<br/>모델 학습
<br/>모델 평가

MeCab
<br/>Corpus
<br/>GPT-1
<br/>모델 학습
<br/>모델 평가

결론
<br/>참고문헌

#개발 환경
"""

!pip install -q sentencepiece wget

import os
# install konlpy, jdk, JPype
!pip install konlpy
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip3 install JPype1-py3

# install mecab-ko
os.chdir('/tmp/')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
!tar zxfv mecab-0.996-ko-0.9.2.tar.gz
os.chdir('/tmp/mecab-0.996-ko-0.9.2')
!./configure
!make
!make check
!make install

# install mecab-ko-dic
!apt-get install automake
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
!tar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.1.1-20180720')
!./autogen.sh
!./configure
!make
!make install

# install mecab-python
os.chdir('/content')
!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git
os.chdir('/content/mecab-python-0.996')
!python3 setup.py build
!python3 setup.py install

import random
import json
from collections import Counter
import warnings
warnings.filterwarnings(action='ignore')

import tensorflow as tf
import pandas as pd
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
import wget
import matplotlib.pyplot as plt
# %matplotlib inline
from IPython.display import Image
from IPython.display import display
from tqdm import tqdm, tqdm_notebook, trange

"""wget은 모델 학습의 bar 디자인과 모델의 다운로드 위치를 변경한다.
<br/>tqdm은 for문의 작업진행률을 표시한다.
<br/>IPython은 Jupyter Notebook의 파일에서 이미지를 표시한다.
"""

import re
import sentencepiece as spm
from konlpy.tag import Mecab

"""토크나이저의 방식은 크게 두가지 방식으로 나뉜다.
<br/><br/>Wordpiece
<br/>한 단어를 세부 단어로 분리한다.
<br/>SetencePiece. BPE. BERT WordPiece. 
<br/><br/>Morphology
<br/>형태소 분석을 한다.
<br/>Mecab. Okt. Komoran. Kkma. Hannanum. Khaiii
<br/>*Package : Huggingface Tokenizers
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import argparse
import collections
import shutil
import zipfile

from google.colab import drive
drive.mount('/content/drive')

pip freeze > '/content/drive/MyDrive/lms/library_version.txt'

library_name = ['random', 'jsonschema=', 'tensorflow=', 'pandas=', 
                 'numpy=', 'wget', 'tqdm', 'matplotlib=',
                'regex=', 'sentencepiece=', 'konlpy', 'mecab', 'torch @']
library_version = []
count = 0

import sys
print(sys.version)
print()

with open('/content/drive/MyDrive/lms/library_version.txt', 'r') as f:
    lines = f.read().splitlines() 

for i in range(len(lines)):
  for line in lines[i:i+1]:
    for library in library_name:
      if library in line:
        library_version.append(line)
        count += 1
        print(line, end = '    ')
        if count % 3 == 0:
          print()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Google Colab에서 할당된 GPU를 확인한다.
<br/>고용량 메모리 VM에 액세스한다.

#Korean Parallel Corpora

##데이터 정보

[Korean Parallel Corpora](https://github.com/jungyeul/korean-parallel-corpora)

한국어의 형태소 분석과 품사 태깅, 기계 번역 연구를 위해 공개된 데이터이다.

한국어-영어 뉴스 병렬 코퍼스
<br/>한국어-영어 Junior High Evaluation 병렬 코퍼스
<br/>한국어-프랑스 병렬 코퍼스
<br/>북한어-영어 병렬 코퍼스

[Korean English News V1](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)

한국어-영어 뉴스 병렬 코퍼스에서 한국어 부분을 데이터로 사용한다.

##데이터 탐색
"""

path_to_file = '/content/drive/MyDrive/lms/sp_tokenizer/korean_parallel_corpora/korean-english-park.train.ko'

with open(path_to_file, "r") as f:
    raw = f.read().splitlines()

print("Data Size :", len(raw))
print()
print("Example")
for sen in raw[600:700][::20]: print("", sen)

warnings.filterwarnings(action='ignore')

min_len = 999
max_len = 0
sum_len = 0

for sen in raw:
    length = len(sen)
    if min_len > length: min_len = length
    if max_len < length: max_len = length
    sum_len += length

print("문장의 최소 길이 :", min_len)
print("문장의 최대 길이 :", max_len)
print("문장의 평균 길이 :", sum_len // len(raw))

sentence_length = np.zeros((max_len), dtype=np.int)

for sen in raw:
    sentence_length[len(sen)-1] += 1

print()
plt.boxplot(sentence_length)
plt.title("Sentence Length Boxplot")
plt.show()

print()
plt.bar(range(max_len), sentence_length, width=1.0)
plt.title("Sentence Length Distribution")
plt.show()

for idx, _sum in enumerate(sentence_length):
    if _sum > 1500:
        print("Outlier Index:", idx+1)

"""문장의 수가 1500을 초과하는 문장의 인덱스를 확인한다."""

def check_sentence_with_length_list(raw, length):
    sentence = []
    count = 0

    for sen in raw:
        if len(sen) == length:
            sentence.append(sen)
            sentence.append('    ')
            count += 1
            if count > 100: return sentence

    return sentence

def check_sentence_with_length_print(raw, length):

  sentence = check_sentence_with_length_list(raw, length)

  for i in range(0, len(sentence) - 1, 6):
    print(str(sentence[i : i+6]).replace("[","").replace("]","").replace("'","").replace("\\n","").replace(",",""), end='') 
    if i % 6 == 0:
      print()
    
  for i in range(len(sentence) - 1):
    if (i-1) % 6 == 0 and i == len(sentence) - 6:
      print(str(sentence[-1]).replace("[","").replace("]","").replace("'","").replace("\\n","").replace(",",""), end='')

check_sentence_with_length_print(raw, 1)

"""길이가 1인 문장을 출력한다.
<br/>아무것도 나오지 않는 결측치이다.
"""

check_sentence_with_length_print(raw, 11)

"""길이가 11인 문장을 출력한다.
<br/>중복된 문장을 확인한다.
"""

cleaned_corpus = list(set(raw))

"""set을 사용해서 중복을 제거한다."""

check_sentence_with_length_print(cleaned_corpus, 11)

min_len = 999
max_len = 0
sum_len = 0

print("Data Size :", len(cleaned_corpus))

for sen in cleaned_corpus:
    length = len(sen)
    if min_len > length: min_len = length
    if max_len < length: max_len = length
    sum_len += length

print("문장의 최소 길이 :", min_len)
print("문장의 최대 길이 :", max_len)
print("문장의 평균 길이 :", sum_len // len(cleaned_corpus))

sentence_length = np.zeros((max_len), dtype=np.int)

for sen in cleaned_corpus:   # 중복이 제거된 코퍼스 기준
    sentence_length[len(sen)-1] += 1


print()
plt.boxplot(sentence_length)
plt.title("Sentence Length Boxplot")
plt.show()

print()
plt.bar(range(max_len), sentence_length, width=1.0)
plt.title("Sentence Length Distribution")
plt.show()

max_len = 150
min_len = 10

filtered_corpus = [i for i in cleaned_corpus if (len(i) < max_len) & (len(i) >= min_len)]

sentence_length = np.zeros((max_len), dtype=np.int)

for sen in filtered_corpus:
    sentence_length[len(sen)-1] += 1

print()
plt.boxplot(sentence_length)
plt.title("Sentence Length Boxplot")
plt.show()

print()
plt.bar(range(max_len), sentence_length, width=1.0)
plt.title("Sentence Length Distribution")
plt.show()

"""길이 조건에 맞는 문장을 선택한다.
<br/>문장의 최소 길이 10, 최대 길이 150으로 설정한다.

##SentencePiece
"""

temp_file = '/content/drive/MyDrive/lms/sp_tokenizer/korean_parallel_corpora/korean-english-park.train.ko.temp'

vocab_size = 8000

with open(temp_file, 'w') as f:
    for row in cleaned_corpus:   
        f.write(str(row) + '\n')

spm.SentencePieceTrainer.Train(
    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    
)

!ls -l korean_spm*

"""corpus를 활용해 sentencepiece 학습한 모델과 단어 사전을 만든다.
<br/>Train에서  model_type = 'unigram'이 디폴트 적용된다.
<br/>model_type = 'bpe' 로 옵션을 주어 변경할 수 있다.
"""

s = spm.SentencePieceProcessor()
s.Load('korean_spm.model')

def sentencepiece_process_input(sentence):
  number_list = []
  sentence_list = []
  
  for i in range(0, len(sentence), 2):
    number_list.append(i)

  for i in range(3):
    j = int(random.choice(number_list))
    sentence_list.append("".join(str(j) for j in sentence[j:j+1]))
  return sentence_list

cleaned_sentence_list = check_sentence_with_length_list(cleaned_corpus, 13)
sentence = sentencepiece_process_input(cleaned_sentence_list)

print("Encoding")
for i in sentence:
  print(s.EncodeAsIds(i))
print()

print("Encoded Pieces")
for i in sentence:
  print(s.SampleEncodeAsPieces(i ,1, 0.0))
print()

print("Decoding")
for i in sentence:
  tokensIDs = s.EncodeAsIds(i)
  print(s.DecodeIds(tokensIDs))

"""길이가 13인 문장을 무작위로 3개 추출해 인코딩과 디코딩을 한다.

##Tokenizer
"""

def sp_tokenize(s, corpus):

    tensor = []

    for sen in corpus:
        tensor.append(s.EncodeAsIds(sen))

    with open("./korean_spm.vocab", 'r') as f:
        vocab = f.readlines()

    word_index = {}
    index_word = {}

    for idx, line in enumerate(vocab):
        word = line.split("\t")[0]

        word_index.update({idx:word})
        index_word.update({word:idx})

    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

    return tensor, word_index, index_word

cleaned_sentence_list = check_sentence_with_length_list(cleaned_corpus, 9)
sentence = sentencepiece_process_input(cleaned_sentence_list)

print(sentence)
tensor, word_index, index_word = sp_tokenize(s, sentence)
print()
print(tensor)

"""길이가 9인 문장을 무작위로 3개 추출해 인코딩한다."""

def word_with_index_print(word_with_index):

  r = list(range(len(word_with_index)))
  random.shuffle(r)

  print("Word Index")

  for i in r[:10]:
    print(i, ':', word_with_index[i])
  print()

  print("Index Word")
  
  for i in r[:10]:
    print(word_with_index[i], ":", i)

word_with_index_print(word_index)

"""무작위로 word_index와 index_word를 추출한다.

#Naver sentiment movie corpus

##데이터 정보

[Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc)

네이버 영화 리뷰 댓글을 웹크롤링한 한국어 말뭉치 데이터셋이다.

데이터셋 구축 과정은 [Large Movie Review Dataset v1.0](https://ai.stanford.edu/~amaas/data/sentiment/)의 구성 방식에 기초한다. 
<br/>Large Movie Review Dataset은 IMDB에서 영화 5만편의 작품 리뷰를 웹크롤링한 말뭉치 데이터셋이다.
<br/>그 중에 훈련 데이터로 쓰일 2만5천개의 리뷰, 시험 데이터로 쓰일 2만5천개의 리뷰로 구성된다.
<br/>그리고 긍정적인 리뷰 2만5천개, 부정적인 리뷰 2만5천개로 구성된다.

데이터셋 분석 결과를 통해 논문 Learning Word Vectors for Sentiment Analysis을 작성했다.
<br/>이 데이터로 학습시킨 모델의 성능은 88.89%의 정확도에 달한다.

Naver Sentiment Movie Corpus v1.0(NSMC)은 훈련 데이터로 쓰일 15만개의 리뷰,
<br/>시험 데이터로 쓰일 5만개의 리뷰로 구성된다.
<br/>그리고 긍정적인 리뷰 1만개, 부정적인 리뷰 1만개로 구성된다.

모든 리뷰의 글자수는 140자 이하이다.
<br/>긍정과 부정의 수치는 1부터 10까지로 정수로 부여한다.
<br/>중립적인 의견의 리뷰는 데이터셋에서 제외한다.

##데이터 탐색
"""

train_data = pd.read_table("/content/drive/MyDrive/lms/film_review/ratings_train.txt")
test_data = pd.read_table("/content/drive/MyDrive/lms/film_review/ratings_test.txt")

train_data.head()

train_data['document'].to_csv("/content/drive/MyDrive/lms/film_review/sp_tokenizer/train.csv", index = False)

"""문자열이 포함된 행을 추출하여 데이터프레임을 만든다."""

with open("/content/drive/MyDrive/lms/film_review/sp_tokenizer/train.csv", "r") as f:
    raw = f.read().splitlines()

movie_raw = raw[1:]
print("Data Size :", len(movie_raw))
print()
print("Example")
for sen in movie_raw[:100][::20]: print("", sen)

warnings.filterwarnings(action='ignore')

min_len = 999
max_len = 0
sum_len = 0

for sen in movie_raw :
    length = len(sen)
    if min_len > length: min_len = length
    if max_len < length: max_len = length
    sum_len += length

print("문장의 최소 길이 :", min_len)
print("문장의 최대 길이 :", max_len)
print("문장의 평균 길이 :", sum_len // len(movie_raw))

sentence_length = np.zeros((max_len), dtype=np.int)

for sen in movie_raw:
    sentence_length[len(sen)-1] += 1

print()
plt.boxplot(sentence_length)
plt.title("Sentence Length Boxplot")
plt.show()

print()
plt.bar(range(max_len), sentence_length, width=1.0)
plt.title("Sentence Length Distribution")
plt.show()

"""##SentencePiece

###Corpus

[kowiki.txt](https://paul-hyun.github.io/nlp-tutorial-02-02-tokenizer/)

한글 [나무위키](https://namu.wiki/w/%EB%82%98%EB%AC%B4%EC%9C%84%ED%82%A4:%EB%8C%80%EB%AC%B8) 코퍼스이다.
"""

path_to_file = "/content/drive/MyDrive/lms/sp_tokenizer/kowikitext/kowiki.txt"

with open(path_to_file, "r") as f:
    kowiki = f.read().splitlines()

print("Data Size :", len(kowiki))
print()
print("Example")
for sen in kowiki[80:81]:
    print("", sen)

"""###Pretrained Model"""

corpus = "/content/drive/MyDrive/lms/sp_tokenizer/kowikitext/kowiki.txt"
prefix = "kowiki"
vocab_size = 8000
spm.SentencePieceTrainer.train(
    f"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}" + 
    " --model_type=bpe" +
    " --max_sentence_length=999999" + 
    " --pad_id=0 --pad_piece=[PAD]" + 
    " --unk_id=1 --unk_piece=[UNK]" + 
    " --bos_id=2 --bos_piece=[BOS]" + 
    " --eos_id=3 --eos_piece=[EOS]" + 
    " --user_defined_symbols=[SEP],[CLS],[MASK]")

"""corpus는 학습용 말뭉치이다.
<br/>prefix는 저장할 vocab 이름이다.
<br/>vocab_size에 7를 더한다. 7은 특수문자 개수이다.
<br/>model_type은 bpe이다.
<br/>max_sentence_length를 default 999999로 설정한다.
<br/>token은 [PAD], [UNK], [BOS], [EOS], [SEP], [CLS], [MASK]으로 구성된다.
<br/>pad_id는 0, unk_id는 1, bos_id는 2, eos_id는 3으로 설정한다.
<br/>bos는 begin of sequence, eos는 end of sequence를 의미한다.
<br/>user_defined_symbols는 모델 구성을 위한 [SEP] : 4, [CLS] : 5, [MASK] : 6으로 설정한다.
"""

vocab = spm.SentencePieceProcessor()
vocab.load("kowiki.model")

sentence_list = check_sentence_with_length_list(movie_raw, 19)
sentence = sentencepiece_process_input(sentence_list)

for line in sentence:
  pieces = vocab.encode_as_pieces(line)
  ids = vocab.encode_as_ids(line)
  print(line)
  print(pieces)
  print(ids)
  print()

"""길이가 19인 문장을 무작위로 3개 추출해 토크나이징과 인코딩을 한다."""

def prepare_train(vocab, infile, outfile):
    df = pd.read_csv(infile, sep="\t", engine="python")
    with open(outfile, "w") as f:
        for index, row in df.iterrows():
            document = row["document"]
            if type(document) != str:
                continue
            instance = { "id": row["id"], "doc": vocab.encode_as_pieces(document), "label": row["label"] }
            f.write(json.dumps(instance))
            f.write("\n")

prepare_train(vocab, "/content/drive/MyDrive/lms/film_review/ratings_train.txt",
              "/content/drive/MyDrive/lms/sp_tokenizer/film_review/sentencepiece_ratings_train.json")

prepare_train(vocab, "/content/drive/MyDrive/lms/film_review/ratings_test.txt",
              "/content/drive/MyDrive/lms/sp_tokenizer/film_review/sentencepiece_ratings_test.json")

""" ### Config"""

class Config(dict): 
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__

    @classmethod
    def load(cls, file):
        with open(file, 'r') as f:
            config = json.loads(f.read())
            return Config(config)

config = Config({
    "n_dec_vocab": len(vocab),
    "n_dec_seq": 256,
    "n_layer": 6,
    "d_hidn": 256,
    "i_pad": 0,
    "d_ff": 1024,
    "n_head": 4,
    "d_head": 64,
    "dropout": 0.1,
    "layer_norm_epsilon": 1e-12
})
print(config)

"""###Common Class"""

def get_sinusoid_encoding_table(n_seq, d_hidn):
    def cal_angle(position, i_hidn):
        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)
    def get_posi_angle_vec(position):
        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]

    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])
    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin 
    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos

    return sinusoid_table

def get_attn_pad_mask(seq_q, seq_k, i_pad):
    batch_size, len_q = seq_q.size()
    batch_size, len_k = seq_k.size()
    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>
    return pad_attn_mask

def get_attn_decoder_mask(seq):
    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))
    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)
    return subsequent_mask

class ScaledDotProductAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.scale = 1 / (self.config.d_head ** 0.5)
    
    def forward(self, Q, K, V, attn_mask):
        # (bs, n_head, n_q_seq, n_k_seq)
        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)
        scores.masked_fill_(attn_mask, -1e9)
        # (bs, n_head, n_q_seq, n_k_seq)
        attn_prob = nn.Softmax(dim=-1)(scores)
        attn_prob = self.dropout(attn_prob)
        # (bs, n_head, n_q_seq, d_v)
        context = torch.matmul(attn_prob, V)
        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)
        return context, attn_prob

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)
        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)
        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)
        self.scaled_dot_attn = ScaledDotProductAttention(self.config)
        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, Q, K, V, attn_mask):
        batch_size = Q.size(0)
        # (bs, n_head, n_q_seq, d_head)
        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)
        # (bs, n_head, n_k_seq, d_head)
        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)
        # (bs, n_head, n_v_seq, d_head)
        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)

        # (bs, n_head, n_q_seq, n_k_seq)
        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)

        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)
        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)
        # (bs, n_head, n_q_seq, h_head * d_head)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)
        # (bs, n_head, n_q_seq, e_embd)
        output = self.linear(context)
        output = self.dropout(output)
        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)
        return output, attn_prob

class PoswiseFeedForwardNet(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)
        self.active = F.gelu
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, inputs):
        # (bs, d_ff, n_seq)
        output = self.active(self.conv1(inputs.transpose(1, 2)))
        # (bs, n_seq, d_hidn)
        output = self.conv2(output).transpose(1, 2)
        output = self.dropout(output)
        # (bs, n_seq, d_hidn)
        return output

"""###Decoder"""

class DecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.self_attn = MultiHeadAttention(self.config)
        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)
        self.pos_ffn = PoswiseFeedForwardNet(self.config)
        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)
    
    def forward(self, dec_inputs, self_attn_mask):
        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)
        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)
        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)
        # (bs, n_dec_seq, d_hidn)
        ffn_outputs = self.pos_ffn(self_att_outputs)
        ffn_outputs = self.layer_norm3(self_att_outputs + ffn_outputs)
        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)
        return ffn_outputs, self_attn_prob

class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)
        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))
        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)

        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])
    
    def forward(self, dec_inputs):
        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1
        pos_mask = dec_inputs.eq(self.config.i_pad)
        positions.masked_fill_(pos_mask, 0)
    
        # (bs, n_dec_seq, d_hidn)
        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)

        # (bs, n_dec_seq, n_dec_seq)
        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)
        # (bs, n_dec_seq, n_dec_seq)
        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)
        # (bs, n_dec_seq, n_dec_seq)
        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)

        self_attn_probs = []
        for layer in self.layers:
            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq)
            dec_outputs, self_attn_prob = layer(dec_outputs, dec_self_attn_mask)
            self_attn_probs.append(self_attn_prob)
        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)]
        return dec_outputs, self_attn_probs

"""### GPT-1"""

Image(filename='/content/drive/MyDrive/lms/sp_tokenizer/gpt-1.png')

class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.decoder = Decoder(self.config)
    
    def forward(self, dec_inputs):
        # (bs, n_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)]
        dec_outputs, dec_self_attn_probs = self.decoder(dec_inputs)
        # (bs, n_dec_seq, n_dec_vocab), [(bs, n_head, n_dec_seq, n_dec_seq)]
        return dec_outputs, dec_self_attn_probs
    
    def save(self, epoch, loss, path):
        torch.save({
            "epoch": epoch,
            "loss": loss,
            "state_dict": self.state_dict()
        }, path)
    
    def load(self, path):
        save = torch.load(path)
        self.load_state_dict(save["state_dict"])
        return save["epoch"], save["loss"]

class MovieClassification(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.gpt = GPT(self.config)
        # lm
        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab, bias=False)
        self.projection_lm.weight = self.gpt.decoder.dec_emb.weight
        # classfier
        self.projection_cls = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)
    
    def forward(self, dec_inputs):
        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)]
        dec_outputs, dec_self_attn_probs = self.gpt(dec_inputs)
        # (bs, n_dec_seq, n_dec_vocab)
        logits_lm = self.projection_lm(dec_outputs)
        # (bs, d_hidn)
        dec_outputs = dec_outputs[:, -1].contiguous()
        # (bs, n_output)
        logits_cls = self.projection_cls(dec_outputs)
        # (bs, n_dec_seq - 1, n_dec_vocab), (bs, n_output), [(bs, n_head, n_dec_seq, n_dec_seq)]
        return logits_lm[:, :-1, :].contiguous(), logits_cls, dec_self_attn_probs

"""###Data Loader"""

class MovieDataSet(torch.utils.data.Dataset):
    def __init__(self, vocab, infile):
        self.vocab = vocab
        self.labels = []
        self.sentences = []

        line_cnt = 0
        with open(infile, "r") as f:
            for line in f:
                line_cnt += 1

        with open(infile, "r") as f:
            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f"Loading {infile}", unit=" lines")):
                data = json.loads(line)
                self.labels.append(data["label"])
                self.sentences.append([vocab.piece_to_id("[BOS]")] + [vocab.piece_to_id(p) for p in data["doc"]] + [vocab.piece_to_id("[EOS]")])
    
    def __len__(self):
        assert len(self.labels) == len(self.sentences)
        return len(self.labels)
    
    def __getitem__(self, item):
        return (torch.tensor(self.labels[item]),
                torch.tensor(self.sentences[item]))

def movie_collate_fn(inputs):
    labels, dec_inputs = list(zip(*inputs))

    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)

    batch = [
        torch.stack(labels, dim=0),
        dec_inputs,
    ]
    return batch

batch_size = 128
train_dataset = MovieDataSet(vocab, "/content/drive/MyDrive/lms/sp_tokenizer/film_review/sentencepiece_ratings_train.json")
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,
                                           shuffle=True, collate_fn=movie_collate_fn)

test_dataset = MovieDataSet(vocab, "/content/drive/MyDrive/lms/sp_tokenizer/film_review/sentencepiece_ratings_test.json")
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,
                                          shuffle=False, collate_fn=movie_collate_fn)

"""###모델 학습"""

def train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader):
    losses = []
    model.train()

    with tqdm(total=len(train_loader), desc=f"Train({epoch})") as pbar:
        for i, value in enumerate(train_loader):
            labels, dec_inputs = map(lambda v: v.to(config.device), value)

            optimizer.zero_grad()
            outputs = model(dec_inputs)
            logits_cls = outputs[1]

            loss_cls = criterion_cls(logits_cls, labels)
            loss = loss_cls

            loss_val = loss_cls.item()
            losses.append(loss_val)

            loss.backward()
            optimizer.step()

            pbar.update(1)
            pbar.set_postfix_str(f"Loss: {loss_val:.3f} ({np.mean(losses):.3f})")
    return np.mean(losses)

def eval_epoch(config, model, data_loader):
    matchs = []
    model.eval()

    n_word_total = 0
    n_correct_total = 0
    with tqdm(total=len(data_loader), desc=f"Valid") as pbar:
        for i, value in enumerate(data_loader):
            labels, dec_inputs = map(lambda v: v.to(config.device), value)

            outputs = model(dec_inputs)
            logits_cls = outputs[1]
            _, indices = logits_cls.max(1)

            match = torch.eq(indices, labels).detach()
            matchs.extend(match.cpu())
            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0

            pbar.update(1)
            pbar.set_postfix_str(f"Acc: {accuracy:.3f}")
    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0

config.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
config.n_output = 2
print(config)

learning_rate = 5e-5
n_epoch = 10

def train(model):
    model.to(config.device)

    criterion_cls = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    best_epoch, best_loss, best_score = 0, 0, 0
    losses, scores = [], []
    for epoch in range(n_epoch):
        loss = train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader)
        score = eval_epoch(config, model, test_loader)

        losses.append(loss)
        scores.append(score)

        if best_score < score:
            best_epoch, best_loss, best_score = epoch, loss, score
    print(f">>>> epoch={best_epoch}, loss={best_loss:.5f}, socre={best_score:.5f}")
    return losses, scores

model = MovieClassification(config)
sentencepiece_losses, sentencepiece_accuracy = train(model)

"""###모델 평가"""

data = {
    "Loss": sentencepiece_losses,
    "Accuracy": sentencepiece_accuracy,
}
df = pd.DataFrame(data)
display(df)
print()

plt.figure(figsize=[12, 4])
plt.plot(sentencepiece_losses, label="Loss")
plt.plot(sentencepiece_accuracy, label="Accuracy")
plt.legend()
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.show()

"""##MeCab

###Corpus
"""

kowiki_file = "/content/drive/MyDrive/lms/sp_tokenizer/kowikitext/kowiki.zip"

args = {
    "seed": 1234,
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    "corpus": kowiki_file,
}
args = argparse.Namespace(**args)

print(args)

"""환경 설정을 한다.
<br/>random seed value를 1234로 설정한다.
<br/>CPU 또는 GPU 사용여부를 결정한다.
<br/>말뭉치 파일을 만든다.
"""

random.seed(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)

mecab = Mecab()

with open("/content/drive/MyDrive/lms/sp_tokenizer/kowikitext/kowiki_mecab.txt", "w") as o_f:
    with zipfile.ZipFile(args.corpus) as z:
        with z.open("kowiki.txt") as f:
            for i, line in enumerate(f):
                line = line.decode('utf-8').strip()
                tokens = mecab.morphs(line)
                string = " ".join(tokens)
                o_f.write(string)
                o_f.write("\n")

"""morph 단위로 분할된 말뭉치를 생성한다."""

print(os.listdir("./"))

def train_sentencepiece(corpus, prefix, vocab_size=8000):

    spm.SentencePieceTrainer.train(
        f"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}" +  
        " --model_type=unigram" +
        " --max_sentence_length=999999" +  
        " --pad_id=0 --pad_piece=[PAD]" + 
        " --unk_id=1 --unk_piece=[UNK]" +  
        " --bos_id=2 --bos_piece=[BOS]" +  
        " --eos_id=3 --eos_piece=[EOS]" +  
        " --user_defined_symbols=[SEP],[CLS],[MASK]" +  
        " --input_sentence_size=100000" +  
        " --shuffle_input_sentence=true")

"""corpus는 학습용 말뭉치이다.
<br/>prefix는 저장할 vocab 이름이다.
<br/>vocab_size에 7를 더한다. 7은 특수문자 개수이다.
<br/>model_type은 bpe이다.
<br/>max_sentence_length를 default 999999로 설정한다.
<br/>token은 [PAD], [UNK], [BOS], [EOS], [SEP], [CLS], [MASK]으로 구성된다.
<br/>pad_id는 0, unk_id는 1, bos_id는 2, eos_id는 3으로 설정한다.
<br/>bos는 begin of sequence, eos는 end of sequence를 의미한다.
<br/>user_defined_symbols는 모델 구성을 위한 [SEP] : 4, [CLS] : 5, [MASK] : 6으로 설정한다.
<br/>input_sentence_size는 말뭉치에서 샘플링하여 학습한다.
<br/>shuffle_input_sentence는 샘플링한 말뭉치를 shuffle한다.
"""

train_sentencepiece("/content/drive/MyDrive/lms/sp_tokenizer/kowikitext/kowiki_mecab.txt", "kowiki_mecab")

print(os.listdir("./"))

corpus_dir = os.path.dirname(args.corpus)
shutil.copy("kowiki_mecab.model", corpus_dir)
shutil.copy("kowiki_mecab.vocab", corpus_dir)

print(os.listdir(corpus_dir))

spm_vocab = spm.SentencePieceProcessor()
spm_vocab.load(os.path.join(corpus_dir, "kowiki_mecab.model"))

sentence_list = check_sentence_with_length_list(movie_raw, 20)
sentence = sentencepiece_process_input(sentence_list)

for line in sentence:
  pieces = spm_vocab.encode_as_pieces(line)
  ids = spm_vocab.encode_as_ids(line)
  print(line)
  print(pieces)
  print(ids)
  print()

"""### GPT-1"""

config = Config({
    "n_dec_vocab": len(spm_vocab),
    "n_dec_seq": 256,
    "n_layer": 6,
    "d_hidn": 256,
    "i_pad": 0,
    "d_ff": 1024,
    "n_head": 4,
    "d_head": 64,
    "dropout": 0.1,
    "layer_norm_epsilon": 1e-12
})
print(config)

prepare_train(spm_vocab, "/content/drive/MyDrive/lms/film_review/ratings_train.txt",
              "/content/drive/MyDrive/lms/sp_tokenizer/film_review/mecab_ratings_train.json")

prepare_train(spm_vocab, "/content/drive/MyDrive/lms/film_review/ratings_test.txt",
              "/content/drive/MyDrive/lms/sp_tokenizer/film_review/mecab_ratings_test.json")

batch_size = 128
train_dataset = MovieDataSet(spm_vocab, "/content/drive/MyDrive/lms/sp_tokenizer/film_review/mecab_ratings_train.json")
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,
                                           shuffle=True, collate_fn=movie_collate_fn)

test_dataset = MovieDataSet(spm_vocab, "/content/drive/MyDrive/lms/sp_tokenizer/film_review/mecab_ratings_test.json")
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,
                                          shuffle=False, collate_fn=movie_collate_fn)

"""###모델 학습"""

config.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
config.n_output = 2
print(config)

learning_rate = 5e-5
n_epoch = 10

model = MovieClassification(config)
mecab_losses, mecab_accuracy = train(model)

"""### 모델 평가"""

model_history_data = {
    "Loss": mecab_losses,
    "Accuracy": mecab_accuracy,
}
model_history_df = pd.DataFrame(model_history_data)
display(model_history_df)
print()

plt.figure(figsize=[12, 4])
plt.plot(mecab_losses, label="Loss")
plt.plot(mecab_accuracy, label="Accuracy")
plt.legend()
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.show()

"""#결론

SentencePiece + GPT-1
<br/>Loss: 0.2558 - Accuracy: 0.8389

MeCab + GPT-1
<br/>Loss: 0.2537 - Accuracy: 0.8414

MeCab + GPT-1 모델의 성능이 더 높다.

#참고문헌

**LMS**
<br/>[dev-sngwn](https://github.com/dev-sngwn)

<br/>**논문**
<br/>[Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 「Improving Language Understanding by Generative Pre-Training」, OpenAI, 2018](https://openai.com/blog/language-unsupervised/)

<br/>**공식 사이트**
<br/>Korpora
<br/>[한국어 위키 텍스트](https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/kowikitext.html)
<br/><br/>파이토치 한국 사용자 모임
<br/>[TORCH.NN 이 실제로 무엇인가요?](https://tutorials.pytorch.kr/beginner/nn_tutorial.html)

<br/>**Github**
<br/>jungyeul
<br/>[jungyeul/korean-parallel-corpora/korean-english-news-v1](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)
<br/><br/>paul-hyun
<br/>[paul-hyun/transformer-evolution/tutorial](https://github.com/paul-hyun/transformer-evolution/tree/master/tutorial)
<br/>[Sentencepiece를 활용해 Vocab 만들기](https://paul-hyun.github.io/vocab-with-sentencepiece/)
<br/>[Naver 영화리뷰 감정분석 데이터 전처리 하기](https://paul-hyun.github.io/preprocess-nsmc/)
<br/>[자연어처리를 위한 Tokenizer & Vocabulary](https://paul-hyun.github.io/nlp-tutorial-02-02-tokenizer/)
<br/>[GPT(Generative Pre-Training) 구현하기 (1/2)](https://paul-hyun.github.io/gpt-01/)
<br/>[GPT(Generative Pre-Training) 구현하기 (2/2)](https://paul-hyun.github.io/gpt-02/)

<br/>**웹사이트**
<br/>[python wget 사용하는 방법](https://my-devblog.tistory.com/18)
<br/>[Python에서 tqdm 라이브러리를 이용한 작업진행률 표시](http://www.gisdeveloper.co.kr/?p=8636)
<br/>[python Jupyter Notebook의 파일에서 이미지를 표시하려면 어떻게해야합니까?](http://daplus.net/python-jupyter-notebook%EC%9D%98-%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%A5%BC-%ED%91%9C%EC%8B%9C%ED%95%98%EB%A0%A4%EB%A9%B4-%EC%96%B4%EB%96%BB%EA%B2%8C%ED%95%B4%EC%95%BC/)
<br/>[Tokenizer 비교 실험 (형태소 분석, word piece)](https://i-am-wendy.tistory.com/27)
<br/>[에러 해결: RuntimeError: CUDA error: device-side assert triggered](https://qlsenddl-lab.tistory.com/39)
"""