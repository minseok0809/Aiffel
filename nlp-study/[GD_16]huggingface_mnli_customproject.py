# -*- coding: utf-8 -*-
"""[GD_16]HuggingFace_MNLI_CustomProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CL3n_5qFjI7ctIuma4vMwEl8L5t77CFR

##HuggingFace 커스텀 프로젝트 만들기

HuggingFrace Framework
<br/>Development Environment
<br/>beomi/kcbert-base MLM 
<br/>HuggingFace Transformers v4.21.1
<br/>bert-base-cased MRPC 
<br/>bert-base-uncased MRPC 
<br/>google/bert_uncased_L-12_H-768_A-12 MRPC
<br/>Debugging

Transformer Pipeline
<br/>Text generation
<br/>Sentiment analysis
<br/>Question Answering
<br/>Text prediction
<br/>Text Summarization
<br/>Translation
<br/>Zero-Shot Learning

HuggingFrace MNLI Custom Project
<br/>facebook/bart-large
<br/>google/electra-base-discriminator

HuggingFrace MRPC Custom Project
<br/>distilbert-base-uncased

Conclusion
<br/>Reference

#HuggingFrace Framework

##Development Environment
"""

import os
import sys
import json
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
from urllib import request
from IPython.display import HTML
from IPython.display import display

from google.colab import drive
drive.mount('/content/drive')

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""<br/>

##beomi/kcbert-base MLM

[Beomi/2021-03-15-kcbert-mlm-finetune-with-petition-dataset.ipynb](https://gist.github.com/Beomi/972c6442a9c15a22dfd1903d0bb0f577)
"""

cd drive/MyDrive/lms/huggingface_glue_task

pwd

!pip install -q Korpora soynlp "kss<2.6" transformers "datasets >= 1.1.3" "sentencepiece != 0.1.92" protobuf

import re
from Korpora import Korpora
from glob import glob
from tqdm.auto import tqdm
from soynlp.normalizer import repeat_normalize
from kss import split_sentences

Korpora.fetch('korean_petitions', root_dir='data/Korpora')
dataset = glob('data/Korpora/korean_petitions/petitions*')

"""[Korpora: Korean Corpora Archives](https://github.com/ko-nlp/Korpora)
<br/>[청와대 국민청원 데이터 아카이브](https://github.com/lovit/petitions_archive)
<br/>[Korpora 청와대 국민청원](https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/korean_petitions.html)
"""

df = pd.concat([pd.read_json(i, lines=True) for i in tqdm(dataset)])
agreed_df = df[df['num_agree'] > 1000]

pattern = re.compile(f'[^ .,?!/@$%~％·∼()\x00-\x7Fㄱ-ㅣ가-힣]+')
url_pattern = re.compile(
    r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x):
    x = pattern.sub(' ', x)
    x = url_pattern.sub('', x)
    x = x.strip()
    x = repeat_normalize(x, num_repeats=2)
    return x

contents = agreed_df['content'].map(clean).to_list()

with open('data/korean_petitions_safe.txt', 'w') as f:
    for doc in tqdm(contents):
        if doc:
            for line in split_sentences(doc):
                f.write(line+'\n')
            f.write('\n')
    f.close()

"""[Error in google colab(python) while transliterating data into Indian language](https://stackoverflow.com/questions/73024976/error-in-google-colabpython-while-transliterating-data-into-indian-language)

AttributeError: module 'emoji' has no attribute 'UNICODE_EMOJI'
<br/>AttributeError로 인해 emoji가 작동하지 않아
<br/>데이터 전처리 과정에서 emoji를 실행하는 이모티콘 전처리 과정을 생략했다.
"""

!wget -O kcbert_base_mlm/run_mlm.py https://raw.githubusercontent.com/huggingface/transformers/72aee83ced5f31302c5e331d896412737287f976/examples/pytorch/language-modeling/run_mlm.py

!python kcbert_base_mlm/run_mlm.py \
    --model_name_or_path beomi/kcbert-base \
    --train_file data/korean_petitions_safe.txt \
    --do_train \
    --output_dir kcbert_base_mlm/test_mlm

"""모델 [beomi/kcbert-base](https://huggingface.co/beomi/kcbert-base)를 사용한다."""

def read_model_result_json(json_path):
    with open(json_path, 'r') as f:
        json_data = json.load(f)
    return json_data

CSS = """.output {flex-direction: row;}"""
HTML('<style>{}</style>'.format(CSS))

json_path_list = ['kcbert_base_mlm/test_mlm/trainer_state.json', 'kcbert_base_mlm/test_mlm/train_results.json', 
                  'kcbert_base_mlm/test_mlm/config.json', 'kcbert_base_mlm/test_mlm/tokenizer_config.json']

for i in json_path_list:
  if 'trainer_state' in i:
      json_data = read_model_result_json(i)
      log_history_df = pd.DataFrame.from_dict(json_data['log_history']).fillna(" ")
  elif 'train_results' in i:
      json_data = read_model_result_json(i)
      train_results_df = pd.DataFrame([json_data])
  elif '/config' in i:
      json_data = read_model_result_json(i)
      config_json_df = pd.DataFrame.from_dict(json_data)
  elif 'tokenizer_config' in i:
      json_data = read_model_result_json(i)
      tokenizer_config_df = pd.DataFrame([json_data])

for i in [log_history_df, train_results_df]:
  display(i)
  print("\n" * 2)

for i in range(4):
  display(config_json_df.iloc[:,6*i:6*(i+1)])
  print("\n" * 2)
  if i == 3:
    display(config_json_df.iloc[:,6*(i+1):])
    print("\n" * 2)

for i in range(2):
  display(tokenizer_config_df.iloc[:,6*i:6*(i+1)])
  print("\n" * 2)

"""<br/><br/>

##HuggingFace Transformers v4.21.1
"""

pwd

!git clone -b v4.21.1 https://github.com/huggingface/transformers.git

mv transformers transfomers_v4.21.1

cd transfomers_v4.21.1

!pip install -e .

!pip install -q datasets huggingface_hub evaluate transformers

!pip install -q protobuf sentencepiece

"""[HuggingFace Installation](https://huggingface.co/docs/transformers/installation#installing-from-source%60)
<br/>[huggingface/transformers](https://github.com/huggingface/transformers)
<br/>[huggingface/transformers/releases](https://github.com/huggingface/transformers/releases)
<br/>[# v4.21.1: Patch release](https://github.com/huggingface/transformers/releases/tag/v4.21.1)

run_glue.py 파일의 check_min_version에 dev가 붙어있다면 Github의 main branch가 아니라 최근 release 된 Tag를 찾아 다운로드한다.

##bert-base-cased MRPC
"""

pwd

!python examples/tensorflow/text-classification/run_glue.py \
	--model_name_or_path bert-base-cased \
	--task_name mrpc \
	--output_dir bert_base_cased_mrpc \
	--overwrite_output_dir \
	--do_train \
	--do_eval \
	--num_train_epochs 1 \
	--save_steps 20000

"""모델 [bert-base-cased](https://huggingface.co/bert-base-cased)를 사용한다.

AssertionError로 인해 실행되지 않는다.

AssertionError: in user code:
<br/>File "/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py", line 475, in split_features_and_labels  *
<br/>assert set(features.keys()).union(labels.keys()) == set(input_batch.keys())

10가지 GLUE Task 중 MRPC를 수행하는 예제 코드이다.
<br/>Huggingface의 Framework 기반으로 모델 [bert-base-uncased](https://huggingface.co/bert-base-cased)을 활용하여 MRPC Task를 수행한다.
<br/>만약 task_name 및 다른 파라미터를 적절히 변경한 후 수행하면 다른 GLUE task도 수행할 수 있다.

<br/>

##bert-base-uncased MRPC
"""

cd ..

pwd

!git clone https://github.com/google-research/bert.git

"""[google-research/bert](https://github.com/google-research/bert)"""

url = "https://gist.githubusercontent.com/vlasenkoalexey/fef1601580f269eca73bf26a198595f3/raw/db67cdf22eb5bd7efe376205e8a95028942e263d/download_glue_data.py"
savename = "data/download_glue_data.py"

request.urlretrieve(url, savename)

!python data/download_glue_data.py --data_dir='data' --tasks='MRPC'

"""[W4ngatang/download_glue_data.py](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)을 실행하여 MRPC 데이터를 다운로드 받을 때
<br/>AttributeError: 'NoneType' object has no attribute 'urlretrieve'가 발생한다.
<br/>이를 디버깅하여 코드를 수정한 파일이 [vlasenkoalexe/download_glue_data.py](https://gist.github.com/vlasenkoalexey/fef1601580f269eca73bf26a198595f3)이다.
"""

# original code file
with open("bert/optimization.py", "r") as f:
    code_line = f.read().splitlines()
for code in code_line:
    if  "class AdamWeightDecayOptimizer" in code:
      print("", code)

# revised code file
with open("bert/optimization.py", "r") as f:
    code_line = f.read().splitlines()
for code in code_line:
    if  "class AdamWeightDecayOptimizer" in code:
      print("", code)

"""[Error importing BERT: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'](https://stackoverflow.com/questions/61250311/error-importing-bert-module-tensorflow-api-v2-train-has-no-attribute-optimi)

AttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'가 발생하기 때문에
<br/>파일 optimization.py에서 class AdamWeightDecayOptimizer(tf.train.Optimizer)를 
<br/>class AdamWeightDecayOptimizer(tf.compat.v1.train.Optimizer)로 수정한다.
"""

# original code file
with open("bert/run_classifier.py", "r") as f:
    code_line = f.read().splitlines()
for code in code_line:
    if  "tf.flags" in code:
      print("", code)
    elif "flags.FLAGS" in code:
      print("", code)
    elif "tf.app" in code:
      print("", code)

# revised code file
with open("bert/run_classifier.py", "r") as f:
    code_line = f.read().splitlines()
for code in code_line:
    if  "tf.compat.v1" in code:
      print("", code)

"""[AttributeError: module 'tensorflow' has no attribute 'flags' #1754](https://github.com/tensorflow/tensor2tensor/issues/1754)

AttributeError: module 'tensorflow' has no attribute 'flags'가 발생하기 때문에
<br/>파일 run_classifier.py에서 tf.flags.FLAGS를 tf.compat.v1.flags.Flag로 수정한다.

[AttributeError: module 'tensorflow' has no attribute 'app' #34431](https://github.com/tensorflow/tensorflow/issues/34431)

AttributeError: module 'tensorflow' has no attribute 'app'가 발생하기 때문에
<br/>파일 run_classifier.py에서 tf.app를 tf.compat.v1.app로 수정한다.
"""

!python bert/run_classifier.py \
  --task_name MRPC \
  --do_train \
  --do_eval \
  --do_lower_case \
  --data_dir data/glue_data/MRPC \
  --bert_model bert-base-uncased \
  --max_seq_length 128 \
  --train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3.0 \
  --output_dir test_mrpc

"""모델 [bert-base-uncased](https://huggingface.co/bert-base-uncased)를 사용한다.

[Zhen-Dong/QBERT/NLP-quantization/Fine-tuning with BERT: examples](https://githubmemory.com/index.php/repo/Zhen-Dong/QBERT)

absl.flags._exceptions.IllegalFlagValueError가 발생한다.

<br/>

##google/bert_uncased_L-12_H-768_A-12 MRPC
"""

pwd

# !git clone https://github.com/google-research/bert.git
ls

"""[google-research/bert](https://github.com/google-research/bert)"""

url_path_dict = {"https://huggingface.co/google/bert_uncased_L-12_H-768_A-12/blob/main/vocab.txt" : "data/bert_uncased_L-12_H-768_A-12/vocab.txt",
            "https://huggingface.co/google/bert_uncased_L-12_H-768_A-12/blob/main/bert_model.ckpt.data-00000-of-00001" : "data/bert_uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001",
            "https://huggingface.co/google/bert_uncased_L-12_H-768_A-12/blob/main/config.json" : "data/bert_uncased_L-12_H-768_A-12/config.json"}

for url, save_path in url_path_dict.items():
  request.urlretrieve(url, save_path)

"""모델 [google/bert_uncased_L-12_H-768_A-12](https://huggingface.co/google/bert_uncased_L-12_H-768_A-12/tree/main)를 사용한다."""

# original code file
with open("bert/run_classifier.py", "r") as f:
    code_line = f.read().splitlines()
for code in code_line:
    if "tf.logging" in code:
      print("", code)
    elif "do_lower_case" in code:
      print("", code)

#revised code file
with open("bert/run_classifier.py", "r") as f:
    code_line = f.read().splitlines()
for code in code_line:
    if "tf.compat.v1.logging" in code:
      print("", code)
    elif "do_lower_case" in code:
      print("", code)

"""[AttributeError: module 'tensorflow' has no attribute 'logging'](https://systemout.tistory.com/8)

AttributeError: module 'tensorflow' has no attribute 'logging'이 발생하기 때문에
<br/>파일 run_classifier.py에서 tf.logging 대신 tf.compat.v1.logging로 수정한다.

AttributeError: type object 'Flag' has no attribute 'do_lower_case'가 발생하기 때문에
<br/>파일 run_classifier.py에서 do_lower_case를 제외한다.
"""

!python bert/run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir data/MRPC \
  --vocab_file data/bert_uncased_L-12_H-768_A-12/vocab.txt \
  --bert_config_file data/bert_uncased_L-12_H-768_A-12/config.json \
  --init_checkpoin data/bert_uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001 \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir bert_uncased_L-12_H-768_A-12_mrpc \

"""[google-research/bert/Sentence (and sentence-pair) classification tasks](https://github.com/google-research/bert)

TypeError: DEFINE_boolean() missing 2 required positional arguments: 'default' and 'help'가 발생한다.

<br/>

##Debugging

HuggingFace에 등록된 MRPC 모델의 CLI 코드를 실행했다.
<br/>bert-base-cased MRPC
<br/>bert-base-uncased MRPC
<br/>bert_uncased_L-12_H-768_A-12 MRPC

Tensorflow 버전 차이 때문에 AttributeError, TyperError가 발생한다.
<br/>py 파일을 여러 번 수정해도 또 다른 에러가 떠서 아예 코드 전체를 바꿔야 하는 디버깅 작업이 요구된다.

이러한 상황에서 그 작동 코드의 옛날 버전에 맞는 가상환경을 구축하거나
<br/>코드 전체를 바꿔서 원작자에게 연락하여 Github Contributor가 되는 문제 해결 방법이 있는 것인가?
<br/>개발자는 모델 개발과 배포 프로세스에서 발생한 이슈에 어떻게 대처하는가?

<br/>

#Transformer Pipeline

[huggingface transformer basic usage](https://www.kaggle.com/code/nageshsingh/huggingface-transformer-basic-usage)

##Text generation
"""

pwd

ls

cd transfomers_v4.21.1

from transformers import pipeline, set_seed

generator = pipeline('text-generation', model='gpt2')
set_seed(123)
text_generation_dict = generator("Hello, I like to play soccer,", max_length=60, num_return_sequences=7)
print("\n")
pd.set_option('display.max_colwidth', None)
text_generation_df = pd.DataFrame.from_dict(text_generation_dict)
display(text_generation_df.style.set_properties(**{'white-space': 'pre-wrap'}))

"""##Sentiment analysis"""

classifier = pipeline('sentiment-analysis', model='gpt2')
set_seed(123)
sentiment_anlysis_dict = classifier('We are very happy to introduce pipeline to the transformers repository.')
print("\n")
sentiment_anlysis_df = pd.DataFrame(sentiment_anlysis_dict).round(4)
sentiment_anlysis_df

"""##Question Answering"""

question_answerer = pipeline('question-answering')
question_answering_dict = question_answerer({
    'question': 'What is the Newtons third law of motion?',
    'context': 'Newton’s third law of motion states that, "For every action there is equal and opposite reaction"'})
print("\n")
question_answering_df = pd.DataFrame([question_answering_dict]).round(4)
question_answering_df

"""##Text prediction"""

unmasker = pipeline('fill-mask', model='bert-base-cased')
fill_mask_dict = unmasker("Hello, My name is [MASK].")
print("\n")
fill_mask_df = pd.DataFrame.from_dict(fill_mask_dict).round(4)
fill_mask_df

"""##Text Summarization

[Wikipedia Apollo program](https://en.wikipedia.org/wiki/Apollo_program)
"""

summarizer = pipeline("summarization")

ARTICLE = """The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.
First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,
Apollo was later dedicated to President John F. Kennedy's national goal of "landing a man on the Moon and returning him safely to the Earth" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. 
Project Mercury was followed by the two-man ProjectGemini (1962–66). 
The first manned flight of Apollo was in 1968.
Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. 
Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.
Apollo used Saturn family rockets as launch vehicles. 
Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973–74, and the Apollo–Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.
 """

summary = summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]

print("\n")
print("Summary")
print(summary['summary_text'])

"""##Translation"""

# English to Spain
translator_ger = pipeline("translation_en_to_de")
ger_text = translator_ger("Spaghetti is the plural form of the Italian word spaghetto, which is a diminutive of spago, meaning thin string or twine.", max_length=40)[0]['translation_text']

# English to French
translator_fr = pipeline('translation_en_to_fr')
fr_text = translator_fr("Spaghetti is the plural form of the Italian word spaghetto, which is a diminutive of spago, meaning thin string or twine.",  max_length=40)[0]['translation_text']

print("\n")
print("German:", ger_text)
print("French:", fr_text)

"""<br/>

##Zero-Shot Learning
"""

classifier_zsl = pipeline("zero-shot-classification")

sequence_to_classify = "Bill gates founded a company called Microsoft in the year 1975"
candidate_labels = ["computer", "sports",'leadership','business', "politics", "startup"]
classifier_zsl_dict = classifier_zsl(sequence_to_classify, candidate_labels)
keys = ('labels', 'scores')
classifier_zsl_dict_slice = {k: classifier_zsl_dict[k] for k in keys}

print("\n")
classifier_zsl_df = pd.DataFrame.from_dict(classifier_zsl_dict_slice).round(4)
classifier_zsl_df

"""<br/>

#HuggingFrace MNLI Custom Project

GLUE MNLI Task를 Custom Project로 구성한다.

##Development Environment
"""

pwd

"""[huggingface/transformers/releases](https://github.com/huggingface/transformers/releases)
<br/>[# v4.21.1: Patch release](https://github.com/huggingface/transformers/releases/tag/v4.21.1)
"""

!pip install tensorflow-datasets -U

"""tensorflow-datasets를 이용하여 glue/mnli를 다운로드하려면 tensorflow-datasets 라이브러리 버전을 업그레이드한다."""

!pip install -e .

!pip install -q datasets huggingface_hub evaluate transformers

!pip install -q protobuf sentencepiece

import os
import json
import time
import random
from datetime import date

import pprint
from termcolor import colored
import matplotlib.pyplot as plt
from IPython.display import HTML
from IPython.display import Image

import re
import numpy as np
import pandas as pd
from pandas import option_context

import tensorflow as tf
import tensorflow_datasets as tfds

import datasets
from datasets import load_dataset

from dataclasses import asdict
from argparse import ArgumentParser
from transformers import Trainer, TrainingArguments
from transformers import get_linear_schedule_with_warmup
from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures

pip freeze > '/content/drive/MyDrive/lms/huggingface_glue_task/library_version.txt'

library_name = ['regex=', 'ujson=', 'numpy=', 'pandas=', 'termcolor=', 'matplotlib=',
                'tensorflow=', 'tensorflow-datasets=', 'transformers=']
library_version = []
count = 0

import sys
print(sys.version)
print()

with open('/content/drive/MyDrive/lms/huggingface_glue_task/library_version.txt', 'r') as f:
    lines = f.read().splitlines() 

for i in range(len(lines)):
  for line in lines[i:i+1]:
    for library in library_name:
      if library in line:
        library_version.append(line)
        count += 1
        print(line, end = '    ')
        if count % 3 == 0:
          print()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Google Colab에서 할당된 GPU를 확인한다.
<br/>고용량 메모리 VM에 액세스한다

##Data Information
"""

data, info = tfds.load('glue/mnli', with_info=True)

info_splits = "Train Data Size: " + str(info.splits['train'].num_examples) + \
              "\nValidation Matched Data Size: " + str(info.splits['validation_matched'].num_examples) + \
              "\nValidation Mismatched Data Size: " + str(info.splits['validation_mismatched'].num_examples) + \
              "\nTest Matched Data Size: " + str(info.splits['test_matched'].num_examples) + \
              "\nTest Matched Data Size: " + str(info.splits['test_mismatched'].num_examples)

reorderlist = ['name', 'full_name', 'description', 'config_description', 'supervised_keys', 'disable_shuffling',
               'homepage', 'data_path', 'file_format', 'download_size', 'dataset_size', 'features', 'supervised_keys',
               'disable_shuffling', 'splits', 'citation', 'comment']

info_dict = pd.DataFrame({'tfds.core.DatasetInfo':{'name':info.name, 'full_name':info.full_name,
                                     'description':info.description.replace('resources', 'resources<br/>'),
                                     'config_description':'''The Multi-Genre Natural Language Inference Corpus is a crowdsourced
                                     collection of sentence pairs with textual entailment annotations. Given a premise sentence
                                     and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis
                                     (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are
                                     gathered from ten different sources, including transcribed speech, fiction, and government reports.
                                     We use the standard test set, for which we obtained private labels from the authors, and evaluate
                                     on both the matched (in-domain) and mismatched (cross-domain) section. We also use and recommend
                                     the SNLI corpus as 550k examples of auxiliary training data.''',
                                     'supervised_keys':info.supervised_keys, 
                                     'disable_shuffling':info.disable_shuffling, 'homepage':info.homepage, 
                                     'data_path': '~/tensorflow_datasets/glue/mnli/2.0.0',
                                     'file_format':info.file_format, 'download_size':info.download_size,
                                     'dataset_size':info.dataset_size, 'features':info.features,
                                      'supervised_keys':info.supervised_keys, 'disable_shuffling': info.disable_shuffling,
                                      'splits': info_splits.replace('\n', '<br/>'),  'citation':info.citation.replace('\n', '<br/>'),
                                      'comment': '''Note that each GLUE dataset has its own citation. Please see the source to see
                                      the correct citation for each contained dataset.'''
                                      }})
info_df = pd.DataFrame(info_dict)
info_df = info_df.reindex(reorderlist).reset_index()

with option_context('display.max_colwidth', None):
  display(info_df.style.set_properties(**{'text-align': 'center'}))

"""<br/><br/><br/>"""

def compact_dict_print(d, indent=''):

    bracket_count = 0
    line_count = 0 
    indent = "    "

    print("{")

    for key, value in d.items():
      bracket_count += 1
      print(indent + "'{}': ".format(key))
      print((indent * 2) + "<PrefetchDataset element_spec={")

      for nested_key in value:
          line_count += 1
          
          for inner_key in nested_key:
              if line_count == 1:
                print((indent * 2) + "'{}': '{}',".format(inner_key, nested_key[inner_key]))
                if inner_key == 'premise':
                  print()

              elif line_count == len(d[key]):
                line_count = 0

      print((indent * 2) + "...")

      if bracket_count < len(data):       
        print((indent * 2) + "}>, \n") 
      else:  
        print((indent * 2) + "}> \n")     

    print("}")

compact_dict_print(data)

"""<br/><br/>"""

examples = data['train'].take(1)
for example in examples:
    hypothesis = example['hypothesis']
    idx = example['idx']
    label = example['label']
    premise = example['premise']
    print('hypothesis')
    print(hypothesis)
    print('\nidx')
    print(idx)
    print('\nlabel')
    print(label)
    print('\npremise')
    print(premise)

"""<br/>

##Processor
"""

class DataProcessor:
    """Base class for data converters for sequence classification data sets."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """
        Gets an example from a dict with tensorflow tensors.

        Args:
            tensor_dict: Keys and values should match the corresponding Glue
                tensorflow_dataset examples.
        """
        raise NotImplementedError()

    def get_train_examples(self, data_dir):
        """Gets a collection of :class:`InputExample` for the train set."""
        raise NotImplementedError()

    def get_dev_examples(self, data_dir):
        """Gets a collection of :class:`InputExample` for the dev set."""
        raise NotImplementedError()

    def get_test_examples(self, data_dir):
        """Gets a collection of :class:`InputExample` for the test set."""
        raise NotImplementedError()

    def get_labels(self):
        """Gets the list of labels for this data set."""
        raise NotImplementedError()

    def tfds_map(self, example):
        """
        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts
        examples to the correct format.
        """
        if len(self.get_labels()) > 1:
            example.label = self.get_labels()[int(example.label)]
        return example

    @classmethod
    def _read_tsv(cls, input_file, quotechar=None):
        """Reads a tab separated value file."""
        with open(input_file, "r", encoding="utf-8-sig") as f:
            return list(csv.reader(f, delimiter="\t", quotechar=quotechar))

"""추상클래스인 Processor를 한번 상속받은 후 Sequence Classification Task를 수행하는 모델의 Processor 추상클래스인 DataProcessor이다.
<br/>추상클래스 상태이기 때문에 그대로 사용하면 NotImplementedError를 발생시키는 메소드들이 포함되어 있다.
<br/>이 메소드들을 Override해야 실제 사용 가능한 클래스가 얻어진다.
"""

class MnliProcessor(DataProcessor):
    """Processor for the MultiNLI data set (GLUE version)."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        warnings.filterwarnings("ignore", category=DeprecationWarning) 

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["premise"].numpy().decode("utf-8"),
            tensor_dict["hypothesis"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        print("LOOKING AT {}".format(os.path.join(data_dir, "train.tsv")))
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")
    
    def get_labels(self):
        """See base class."""
        return ["0", "1", "2"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = f"{set_type}-{line[0]}"
            text_a = line[8]
            text_b = line[9]
            label = None if set_type.startswith("test") else line[-1]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples

class MnliMismatchedProcessor(MnliProcessor):
    """Processor for the MultiNLI Mismatched data set (GLUE version)."""

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev_mismatched.tsv")), "dev_mismatched")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test_mismatched.tsv")), "test_mismatched")

"""MNLI 원본 데이터셋을 처리하여 모델에 입력할 수 있도록 정리해 주는 MnliProcessor, MnliMismatchedProcessor 클래스이다."""

processor = MnliProcessor()
examples = data['train'].take(1)

for example in examples:
    print('Original Data')
    pprint.pprint(example)  
    print("\n")
    example = processor.get_example_from_tensor_dict(example)
    print('Processed Data')
    pprint.pprint(example)

"""Processor는 'Raw Dataset를 Annotated Dataset으로 변환'하는 역할을 한다.
<br/>항목별로 text_a, text_b, label 등의 annotation이 포함된 InputExample로 변환된다.

<br/>
"""

examples = (data['train'].take(1))
for example in examples:
    example = processor.get_example_from_tensor_dict(example)
    example = processor.tfds_map(example)
    print('Processed Data')
    print(example)

"""tfds_map 메소드를 활용한 경우이다. tfds_map는 label을 가공하는 메소드이다.

<br/>
"""

label_list = processor.get_labels()
label_list

label_map = {label: i for i, label in enumerate(label_list)}
label_map

"""실제 label을 확인하여 Multiple Classification 문제로 정의되는 것을 확인한다.

<br/>

##Huggingface Dataset
"""

huggingface_mnli_dataset = load_dataset('glue', 'mnli')

print(huggingface_mnli_dataset)

"""Huggingface Datasets를 사용하면 DataProcessor를 사용할 필요없이 바로 사용가능하게 구성되어 있다.
<br/>Dataset Dictionary은 train dataset, validation dataset, test dataset으로 구성된다.
<br/>Dataset은 ‘premise’, ‘hypothesis’, ‘label’, ‘idx’(인덱스)로 구성된다.

<br/>

##facebook/bart-large

###Tokenizer & Model
"""

from transformers import BartTokenizer, BartModel, AutoConfig, AdamW

tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')
model = BartModel.from_pretrained('facebook/bart-large')

"""MnliProcessor 클래스와 Framework를 결합하는 과정을 진행한다.
<br/>모델 [facebook/bart-large](https://huggingface.co/facebook/bart-large)를 사용한다.
"""

def _glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode="claasification") :
    if max_length is None :
        max_length = tokenizer.max_len
    if label_list is None:
        label_list = processor.get_labels()
        print("Using label list %s" % (label_list))
        print("\n")

    label_map = {label: i for i, label in enumerate(label_list)}
    labels = [label_map[example.label] for example in examples]

    batch_encoding = tokenizer(
        [(example.text_a, example.text_b) for example in examples],
        max_length=max_length,
        padding="max_length",
        truncation=True,
    )

    features = []
    print("Example")
    print()

    for i in range(len(examples)):
        inputs = {k: batch_encoding[k][i] for k in batch_encoding}

        feature = InputFeatures(**inputs, label=labels[i])
        features.append(feature)

        
    for i, example in enumerate(examples[:1]):
        print("guid: %s" % (example.guid))
        pprint.pprint("features: %s" % features[i])
        print("\n")

    return features

def tf_glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode="classification") :
    """
    :param examples: tf.data.Dataset
    :param tokenizer: pretrained tokenizer
    :param max_length: example의 최대 길이(기본값 : tokenizer의 max_len)
    :param task: GLUE task 이름
    :param label_list: 라벨 리스트
    :param output_mode: "regression" or "classification"

    :return: task에 맞도록 feature가 구성된 tf.data.Dataset
    """
    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]
    features = _glue_convert_examples_to_features(examples, tokenizer, max_length, processor)
    label_type = tf.int64

    def gen():
        for ex in features:
            d = {k: v for k, v in asdict(ex).items() if v is not None}
            label = d.pop("label")
            yield (d, label)

    input_names = ["input_ids"] + tokenizer.model_input_names

    return tf.data.Dataset.from_generator(
        gen,
        ({k: tf.int32 for k in input_names}, label_type),
        ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),
    )

import logging
logging.disable(logging.WARNING)

"""Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed."""

train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)
train_dataset_batch = train_dataset.shuffle(100).batch(16)

validation_dataset = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=128, processor=processor)
validation_dataset_batch = validation_dataset.shuffle(100).batch(16)

test_dataset = tf_glue_convert_examples_to_features(data['test_matched'], tokenizer, max_length=128, processor=processor)
test_dataset_batch = test_dataset.shuffle(100).batch(16)

"""###Huggingface Auto Classes"""

# pose sequence as a NLI premise and label as a hypothesis
import transformers
from transformers import AutoModelForSequenceClassification, AutoTokenizer

bart_large_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')
bart_large_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large', num_labels=3)

"""Huggingface의 경우 AutoTokenizer, AutoModel기능을 지원한다.
<br/>AutoTokenizer와 AutoModel은 Huggingface에서 지원하는 Auto Class이다.
<br/>Auto Class는 from_pretrained 메소드를 이용해 Pretrained Model의 경로 혹은 이름만 안다면 자동으로 생성하는 방법이다.

AutoModelForSequenceClassification는 AutoModel을 그대로 사용하기보다 특정 task를 지정하는 방식이다.
<br/>Auto Class는 다양한 모델에 자동으로 맞출 수 있기 떄문에 특정 task와 dataset이 주어져있는 경우 다양한 모델을 실험한다.

<br/>

###Tokenizing
"""

def transform(data):
  return bart_large_tokenizer(
      data['premise'],
      data['hypothesis'],
      truncation = True,
      padding = 'max_length',
      return_token_type_ids = False,
      )

"""토크나이징은 transform이라는 함수를 만들고 이전에 만들어두었던 Tokenizer를 사용하는데 이때 dataset의 형태를 확인하고 바꿀 대상을 지정한다.
<br/>mrpc의 경우 premise, hypothesis가 토크나이징할 대상이므로 data[ 'premise’], data[ 'hypothesis’]로 인덱싱해서 지정한다.
<br/>truncation은 특정 문장이 길어 모델을 다루기 힘들어 질 수 있으므로 짧게 자르는 것을 의미한다.
<br/>return_token_type_ids는 문장이 한개이상일 때 나뉘는걸 보여준다. (해당 내용은 task에 필요없으므로 제거한다.)
"""

examples = huggingface_mnli_dataset['train'][:5]
examples_transformed = transform(examples)

count = 0
print("Example")
print()
pprint.pprint(examples)
print("\n")
print()

print("Tokenization Example")
print()

for i in ['input_ids', 'attention_mask']:
  if i == 'input_ids':
    print('input_ids[0]')
  else:
    count = 0
    print("\n")
    print()
    print('attention_mask[0]')

  for j in examples_transformed[i][0]:
    count += 1
    if count < len(examples_transformed[i][0]):
      print(j, end=', ')
    elif count == len(examples_transformed[i][0]):
      print(j, end=' ')  
    if (i == 'input_ids' and count % 54 == 32) or (i == 'attention_mask' and count % 54 == 0):
      print()

"""<br/><br/><br/><br/>"""

encoded_dataset = huggingface_mnli_dataset.map(transform, batched=True)

"""데이터셋을 한번에 토크나이징할때 자주 사용하는 기법은 map이다.
<br/>map을 사용하면 Data Dictionary에 있는 모든 데이터들이 빠르게 적용시킬 수 있다.
<br/>우리는 map을 사용해 토크나이징을 진행하기 때문에 batch를 적용하므로 batched=True를 설정한다.
"""

encoded_dataset['train'].shape, encoded_dataset['validation_matched'].shape, encoded_dataset['test_matched'].shape

"""<br/>

###HuggingFace Trainer
"""

from huggingface_hub import notebook_login

notebook_login()

task = "mnli"
model_checkpoint = 'facebook/bart-large'
batch_size = 1

"""RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.90 <br/>GiB total capacity; 14.79
<br/>GiB already allocated; 121.75 MiB
<br/>free; 14.90 GiB reserved in total by PyTorch)
<br/>If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. 
<br/>See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

RuntimeError로 인해 batch_size = 1로 설정한다.
"""

metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"
model_name = model_checkpoint.split("/")[-1]
output_dir = 'facebook_bart_large_mnli'

bart_large_args = TrainingArguments(
    output_dir,
    f"{model_name}-finetuned-{task}",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=1,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    push_to_hub=True,
)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if task != "stsb":
        predictions = np.argmax(predictions, axis=1)
    else:
        predictions = predictions[:, 0]
    return metric.compute(predictions=predictions, references=labels)

validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"
bart_large_trainer = Trainer(
    bart_large_model,
    bart_large_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=bart_large_tokenizer,
    compute_metrics=compute_metrics
)

bart_large_trainer.train()

"""RuntimeError로 인해 batch_size = 1, num_train_epochs=1로 설정했는데도
<br/>모델 학습 예상 시간이 106시간이 넘어서 작업을 중지하고 다른 방법을 모색한다.
<br/>[facebook/bart-large](https://huggingface.co/facebook/bart-large)가 대용량 모델이기 때문에
<br/>분산 컴퓨팅을 설정하거나 소형 모델을 만드는 결정을 내려야 한다.

<br/><br/>
"""

cd ..

pwd

Image(filename='google_bart_large_gpu.JPG')

"""<br/>"""

bart_large_trainer.evaluate()

"""<br/>

##google/electra-base-discriminator
"""

cd transfomers_v4.21.1

pwd

"""###Tokenizer & Model"""

from transformers import ElectraTokenizerFast, ElectraForSequenceClassification, AutoConfig

tokenizer = ElectraTokenizerFast.from_pretrained('google/electra-base-discriminator')
model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator')

"""MnliProcessor 클래스와 Framework를 결합하는 과정을 진행한다.
<br/>모델 [google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator)를 사용한다.

"""

import logging
logging.disable(logging.WARNING)

"""Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed."""

train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)
train_dataset_batch = train_dataset.shuffle(100).batch(16)

validation_dataset = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=128, processor=processor)
validation_dataset_batch = validation_dataset.shuffle(100).batch(16)

test_dataset = tf_glue_convert_examples_to_features(data['test_matched'], tokenizer, max_length=128, processor=processor)
test_dataset_batch = test_dataset.shuffle(100).batch(16)

"""<br/>

###Huggingface Auto Classes
"""

import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification

electra_base_discriminator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')
electra_base_discriminator_model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels = 3)

"""<br/>

###Tokenizing
"""

def transform(data):
  return electra_base_discriminator_tokenizer(
      data['premise'],
      data['hypothesis'],
      truncation = True,
      padding = 'max_length',
      return_token_type_ids = False,
      )

examples = huggingface_mnli_dataset['train'][:5]
examples_transformed = transform(examples)

count = 0
print("Example")
print()
pprint.pprint(examples)
print("\n")
print()

print("Tokenization Example")
print()

for i in ['input_ids', 'attention_mask']:
  if i == 'input_ids':
    print('input_ids[0]')
  else:
    count = 0
    print("\n")
    print()
    print('attention_mask[0]')

  for j in examples_transformed[i][0]:
    count += 1
    if count < len(examples_transformed[i][0]):
      print(j, end=', ')
    elif count == len(examples_transformed[i][0]):
      print(j, end=' ')  
    if (i == 'input_ids' and count % 54 == 27) or (i == 'attention_mask' and count % 54 == 0):
      print()

"""<br/><br/><br/><br/>"""

encoded_dataset = huggingface_mnli_dataset.map(transform, batched=True)

encoded_dataset['train'].shape, encoded_dataset['validation_matched'].shape, encoded_dataset['test_matched'].shape

"""<br/>

###HuggingFace Trainer
"""

from huggingface_hub import notebook_login

notebook_login()

task = "mnli"
model_checkpoint = 'google/electra-base-discriminator'
batch_size = 16

"""RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.90 <br/>GiB total capacity; 14.79
<br/>GiB already allocated; 121.75 MiB
<br/>free; 14.90 GiB reserved in total by PyTorch)
<br/>If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. 
<br/>See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

RuntimeError로 인해 batch_size = 16으로 설정한다.
"""

metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"
model_name = model_checkpoint.split("/")[-1]
output_dir = 'google_electra_base_discriminator_mnli'

electra_base_discriminator_args = TrainingArguments(
    output_dir,
    f"{model_name}-finetuned-{task}",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    push_to_hub=True,
)

validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"
electra_base_discriminator_trainer = Trainer(
    electra_base_discriminator_model,
    electra_base_discriminator_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer= electra_base_discriminator_tokenizer,
    compute_metrics=compute_metrics
)

electra_base_discriminator_trainer.train()

"""RuntimeError로 인해 batch_size = 16, num_train_epochs=3으로 설정했는데도 
<br/>모델 학습 예상 시간이 17시간이 넘어서 작업을 중지하고 다른 방법을 모색한다.
<br/>[google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator)가 대용량 모델이기 때문에
<br/>분산 컴퓨팅을 설정하거나 소형 모델을 만드는 결정을 내려야 한다.

<br/><br/>
"""

cd ..

pwd

Image(filename='electra_base_discriminator_gpu.JPG')

"""<br/>"""

electra_base_discriminator_trainer.evaluate()

"""<br/>

#HuggingFrace MRPC Custom Project

GLUE MRPC Task를 Custom Project로 구성한다.

##Development Environment
"""

pwd

"""[huggingface/transformers/releases](https://github.com/huggingface/transformers/releases)
<br/>[# v4.21.1: Patch release](https://github.com/huggingface/transformers/releases/tag/v4.21.1)
"""

!pip install -e .

!pip install -q datasets huggingface_hub evaluate transformers

!pip install -q protobuf sentencepiece

import os
import json

import re
import numpy as np
import pandas as pd
from pandas import option_context

import pprint
from termcolor import colored
import matplotlib.pyplot as plt
from IPython.display import HTML
from IPython.display import Image

import tensorflow as tf
import tensorflow_datasets as tfds

import datasets
from datasets import load_dataset

from argparse import ArgumentParser
from dataclasses import asdict
from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures

pip freeze > '/content/drive/MyDrive/lms/huggingface_glue_task/transfomers_v4.21.1/library_version.txt'

library_name = ['regex=', 'ujson=', 'numpy=', 'pandas=', 'termcolor=', 'matplotlib=',
                'tensorflow=', 'tensorflow-datasets=', 'transformers=']
library_version = []
count = 0

import sys
print(sys.version)
print()

with open('/content/drive/MyDrive/lms/huggingface_glue_task/library_version.txt', 'r') as f:
    lines = f.read().splitlines() 

for i in range(len(lines)):
  for line in lines[i:i+1]:
    for library in library_name:
      if library in line:
        library_version.append(line)
        count += 1
        print(line, end = '    ')
        if count % 3 == 0:
          print()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Google Colab에서 할당된 GPU를 확인한다.
<br/>고용량 메모리 VM에 액세스한다

##Data Information
"""

data, info = tfds.load('glue/mrpc', with_info=True)

info_splits = "Train Data Size: " + str(info.splits['train'].num_examples) + \
              "\nValidation Data Size: " + str(info.splits['validation'].num_examples) + \
              "\nTest Data Size: " + str(info.splits['test'].num_examples)

reorderlist = ['name', 'full_name', 'description', 'config_description', 'supervised_keys', 'disable_shuffling',
               'homepage', 'data_path', 'file_format', 'download_size', 'dataset_size', 'features', 'supervised_keys',
               'disable_shuffling', 'splits', 'citation', 'comment']

info_dict = pd.DataFrame({'tfds.core.DatasetInfo':{'name':info.name, 'full_name':info.full_name,
                                     'description':info.description.replace('resources', 'resources<br/>'),
                                     'config_description':'''The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005)
                                     is a corpus of sentence pairs automatically extracted from online news sources, <br/>
                                     with human annotations for whether the sentences in the pair are semantically equivalent.''',
                                     'supervised_keys':info.supervised_keys, 
                                     'disable_shuffling':info.disable_shuffling, 'homepage':info.homepage, 
                                     'data_path': '~/tensorflow_datasets/glue/mrpc/2.0.0',
                                     'file_format':info.file_format, 'download_size':info.download_size,
                                     'dataset_size':info.dataset_size, 'features':info.features,
                                      'supervised_keys':info.supervised_keys, 'disable_shuffling': info.disable_shuffling,
                                      'splits': info_splits.replace('\n', '<br/>'),  'citation':info.citation.replace('\n', '<br/>'),
                                      'comment': '''Note that each GLUE dataset has its own citation. Please see the source to see
                                      the correct citation for each contained dataset.'''
                                      }})
info_df = pd.DataFrame(info_dict)
info_df = info_df.reindex(reorderlist).reset_index()

with option_context('display.max_colwidth', None):
  display(info_df.style.set_properties(**{'text-align': 'center'}))

"""<br/><br/><br/>"""

compact_dict_print(data)

"""<br/><br/>"""

examples = data['train'].take(1)
for example in examples:
    idx = example['idx']
    label = example['label']
    sentence1 = example['sentence1']
    sentence2 = example['sentence2']

    print('idx')
    print(idx)
    print('\nlabel')
    print(label)
    print('\nsentence1')
    print(sentence1)
    print('\nsentence1')
    print(sentence2)

"""<br/>

##Processor
"""

class DataProcessor:
    """Base class for data converters for sequence classification data sets."""

    def get_example_from_tensor_dict(self, tensor_dict):
        """
        Gets an example from a dict with tensorflow tensors.

        Args:
            tensor_dict: Keys and values should match the corresponding Glue
                tensorflow_dataset examples.
        """
        raise NotImplementedError()

    def get_train_examples(self, data_dir):
        """Gets a collection of :class:`InputExample` for the train set."""
        raise NotImplementedError()

    def get_dev_examples(self, data_dir):
        """Gets a collection of :class:`InputExample` for the dev set."""
        raise NotImplementedError()

    def get_test_examples(self, data_dir):
        """Gets a collection of :class:`InputExample` for the test set."""
        raise NotImplementedError()

    def get_labels(self):
        """Gets the list of labels for this data set."""
        raise NotImplementedError()

    def tfds_map(self, example):
        """
        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts
        examples to the correct format.
        """
        if len(self.get_labels()) > 1:
            example.label = self.get_labels()[int(example.label)]
        return example

    @classmethod
    def _read_tsv(cls, input_file, quotechar=None):
        """Reads a tab separated value file."""
        with open(input_file, "r", encoding="utf-8-sig") as f:
            return list(csv.reader(f, delimiter="\t", quotechar=quotechar))

"""추상클래스인 Processor를 한번 상속받은 후 Sequence Classification Task를 수행하는 모델의 Processor 추상클래스인 DataProcessor이다.
<br/>추상클래스 상태이기 때문에 그대로 사용하면 NotImplementedError를 발생시키는 메소드들이 포함되어 있다.
<br/>이 메소드들을 Override해야 실제 사용 가능한 클래스가 얻어진다.
"""

class MrpcProcessor(DataProcessor):
    """Processor for the MRPC data set (GLUE version)."""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def get_example_from_tensor_dict(self, tensor_dict):
        """See base class."""
        return InputExample(
            tensor_dict["idx"].numpy(),
            tensor_dict["sentence1"].numpy().decode("utf-8"),
            tensor_dict["sentence2"].numpy().decode("utf-8"),
            str(tensor_dict["label"].numpy()),
        )

    def get_train_examples(self, data_dir):
        """See base class."""
        print("LOOKING AT {}".format(os.path.join(data_dir, "train.tsv")))
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training, dev and test sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            text_a = line[3]
            text_b = line[4]
            label = None if set_type == "test" else line[0]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples

"""MRPC 원본 데이터셋을 처리하여 모델에 입력할 수 있도록 정리해 주는 MrpcProcessor 클래스이다."""

processor = MrpcProcessor()
examples = data['train'].take(1)

for example in examples:
    print('Original Data')
    pprint.pprint(example)  
    print("\n")
    example = processor.get_example_from_tensor_dict(example)
    print('Processed Data')
    pprint.pprint(example)

"""Processor는 'Raw Dataset를 Annotated Dataset으로 변환'하는 역할을 한다.
<br/>항목별로 text_a, text_b, label 등의 annotation이 포함된 InputExample로 변환된다.

<br/>
"""

examples = (data['train'].take(1))
for example in examples:
    example = processor.get_example_from_tensor_dict(example)
    example = processor.tfds_map(example)
    print('Processed Data')
    print(example)

"""tfds_map 메소드를 활용한 경우이다. tfds_map는 label을 가공하는 메소드이다.

<br/>
"""

label_list = processor.get_labels()
label_list

label_map = {label: i for i, label in enumerate(label_list)}
label_map

"""실제 label을 확인하여 Binary Classification 문제로 정의되는 것을 확인한다.

<br/>

##Huggingface Dataset
"""

huggingface_mrpc_dataset = load_dataset('glue', 'mrpc')

print(huggingface_mrpc_dataset)

"""Huggingface Datasets를 사용하면 DataProcessor를 사용할 필요없이 바로 사용가능하게 구성되어 있다.
<br/>Dataset Dictionary은 train dataset, validation dataset, test dataset으로 구성된다.
<br/>Dataset은 ‘sentence1’, ‘sentence2’, ‘label’, ‘idx’(인덱스)로 구성된다.

<br/>

##distilbert-base-uncased

###Tokenizer & Model
"""

from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, AutoConfig

distilbert_base_uncased_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
distilbert_base_uncased_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

"""MrpcProcessor 클래스와 Framework를 결합하는 과정을 진행한다.
<br/>모델 [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) 를 사용한다.
"""

train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)

examples = train_dataset.take(1)

print('features')
print()
for example in examples:
    print('input_ids')
    print(example[0]['input_ids'])
    print("\n")
    print('attention_mask')
    print(example[0]['attention_mask'])
    print("\n")
    print('token_type_ids')
    print(example[0]['token_type_ids'])
    print("\n")
    print('label')
    print(example[1])

"""tf_glue_convert_examples_to_features가 최종적으로 모델에 전달될 tf.data.Dataset 인스턴스를 생성하여
<br/>학습 단계 데이터셋 train_dataset이 반환한다.

<br/>
"""

train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)
train_dataset_batch = train_dataset.shuffle(100).batch(16).repeat(2)

validation_dataset = tf_glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, processor=processor)
validation_dataset_batch = validation_dataset.shuffle(100).batch(16)

test_dataset = tf_glue_convert_examples_to_features(data['test'], tokenizer, max_length=128, processor=processor)
test_dataset_batch = test_dataset.shuffle(100).batch(16)

"""###Tokenizing"""

def transform(data):
  return distilbert_base_uncased_tokenizer(
      data['sentence1'],
      data['sentence2'],
      truncation = True,
      padding = 'max_length',
      return_token_type_ids = False,
      )

"""토크나이징은 transform이라는 함수를 만들고 이전에 만들어두었던 Tokenizer를 사용하는데 이때 dataset의 형태를 확인하고 바꿀 대상을 지정한다.
<br/>mrpc의 경우 sentence1, sentence2가 토크나이징할 대상이므로 data[’sentence1’], data[’sentence2’]로 인덱싱해서 지정한다.
<br/>truncation은 특정 문장이 길어 모델을 다루기 힘들어 질 수 있으므로 짧게 자르는 것을 의미한다.
<br/>return_token_type_ids는 문장이 한개이상일 때 나뉘는걸 보여준다. (해당 내용은 task에 필요없으므로 제거한다.)
"""

examples = huggingface_mrpc_dataset['train'][:5]
examples_transformed = transform(examples)

count = 0
print("Example")
print()
pprint.pprint(examples)
print("\n")
print()

print("Tokenization Example")
print()

for i in ['input_ids', 'attention_mask']:
  if i == 'input_ids':
    print('input_ids[0]')
  else:
    count = 0
    print("\n")
    print()
    print('attention_mask[0]')

  for j in examples_transformed[i][0]:
    count += 1
    if count < len(examples_transformed[i][0]):
      print(j, end=', ')
    elif count == len(examples_transformed[i][0]):
      print(j, end=' ')  
    if (i == 'input_ids' and count == 25) or count % 50 == 0:
      print()

"""<br/><br/><br/><br/>"""

encoded_dataset = huggingface_mrpc_dataset.map(transform, batched=True)

"""데이터셋을 한번에 토크나이징할때 자주 사용하는 기법은 map이다.
<br/>map을 사용하면 Data Dictionary에 있는 모든 데이터들이 빠르게 적용시킬 수 있다.
<br/>우리는 map을 사용해 토크나이징을 진행하기 때문에 batch를 적용하므로 batched=True로 설정한다.

<br/>

###Keras Moodel
"""

num_classes = len(processor.get_labels())

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

distilbert_base_uncased_model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])
distilbert_base_uncased_model.summary()

"""<br/><br/>"""

train_history = distilbert_base_uncased_model.fit(train_dataset_batch, epochs=3, steps_per_epoch=115, 
                validation_data=validation_dataset_batch)

"""<br/><br/>"""

keras_model_train_history = {'Train Loss': train_history.history['loss'],
                          'Validation Loss': train_history.history['val_loss'],
                          'Train Accuracy': train_history.history['acc'],
                          'Validation Accuracy': train_history.history['val_acc']}

keras_model_train_history_df = pd.DataFrame(keras_model_train_history)
keras_model_train_history_df = keras_model_train_history_df.round(4)
keras_model_train_history_df['Epoch'] = keras_model_train_history_df.reset_index().index + 1
keras_model_train_history_df = keras_model_train_history_df[['Epoch', 'Train Loss', 'Validation Loss', 'Train Accuracy', 'Validation Accuracy']]
keras_model_train_history_df.to_csv("distilbert-base-uncased_mrpc/keras_model_train_history.csv", index = False)
display(keras_model_train_history_df)
print("\n" * 4)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_history.history['loss'], 'b-', label='Train Loss')
plt.plot(train_history.history['val_loss'], 'r--', label='Validation Loss')
plt.title("Loss")
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_history.history['acc'], 'g-', label='Train Accuracy')
plt.plot(train_history.history['val_acc'], 'k--', label='Validation Accuracy')
plt.title("Accuracy")
plt.xlabel('Epoch')
plt.legend()

plt.show()

"""<br/><br/>"""

eval_history  = distilbert_base_uncased_model.evaluate(test_dataset_batch)

"""<br/>"""

output_eval_file  = 'distilbert-base-uncased_mrpc/eval_results.txt'

with open(output_eval_file, "w") as writer:
    for i, v in enumerate(list(np.round(eval_history, 4))) :
        if i == 0 :
            writer.write("Test Loss = %f\t" %(v))
        if i == 1 :
            writer.write("Test Accuracy = %f\n" %(v))

!cat distilbert-base-uncased_mrpc/eval_results.txt

"""<br/>"""

keras_model_test_history = {'Epoch':1,
                               'Test Loss': eval_history[0],
                               'Test Accuracy': eval_history[1]}

keras_model_test_history_df = pd.DataFrame(keras_model_test_history, index=[0])
keras_model_test_history_df = keras_model_test_history_df.round(4)
keras_model_test_history_df.to_csv("distilbert-base-uncased_mrpc/keras_model_test_history.csv", index = False)
display(keras_model_test_history_df)

"""<br/>

###Huggingface Trainer
"""

#메모리를 비운다.
del distilbert_base_uncased_model
del train_dataset_batch
del validation_dataset_batch
del test_dataset_batch

pwd

from transformers import Trainer, TrainingArguments
output_dir = 'distilbert_base_uncased_trainer_mrpc'
metric_name = 'accuracy'

distilbert_base_uncased_arg = TrainingArguments(
    output_dir, # output이 저장될 경로
    evaluation_strategy="epoch", #evaluation하는 빈도
    learning_rate = 2e-5, #learning_rate
    per_device_train_batch_size = 16, # 각 device 당 batch size
    per_device_eval_batch_size = 16, # evaluation 시에 batch size
    num_train_epochs = 3, # train 시킬 총 epochs
    weight_decay = 0.01, # weight decay
    optim="adamw_torch", # defaults to 'adamw_hf'
)

"""Trainer를 사용하기 위해서는 TrainingArguments를 통해 학습 관련 설정을 미리 지정한다.

FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version.
<br/>Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning

FutureWarning을 해결하기 위해 TrainingArguments에 optim="adamw_torch"를 지정한다.

<br/>
"""

from datasets import load_metric
metric = load_metric('glue', 'mrpc')

def compute_metrics(eval_pred):    
    predictions,labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references = labels)

"""Trainer의 인자로 넘겨주어야 할 것 중에 compute_metrics 메소드가 있다.
<br/>task가 classification인지 regression인지에 따라 모델의 출력 형태가 달라지므로
<br/>task별로 적합한 출력 형식을 고려해 모델의 성능을 계산하는 방법을 미리 지정한다.
"""

distilbert_base_uncased_trainer = Trainer(
    model=distilbert_base_uncased_model,                           # 학습시킬 model
    args=distilbert_base_uncased_arg,                  # TrainingArguments을 통해 설정한 arguments
    train_dataset=encoded_dataset['train'],    # training dataset
    eval_dataset=encoded_dataset['validation'],       # evaluation dataset
    compute_metrics=compute_metrics,
)

"""/usr/local/lib/python3.7/dist-packages/transformers/optimization.py"""

distilbert_base_uncased_trainer.train()

"""<br/><br/>"""

def read_model_result_json(json_path):
    with open(json_path, 'r') as f:
        json_data = json.load(f)
    return json_data

json_path = 'distilbert_base_uncased_trainer_mrpc/checkpoint-500/trainer_state.json'

json_data = read_model_result_json(json_path)
huggingface_trainer_history_df = pd.DataFrame.from_dict(json_data['log_history'])
huggingface_trainer_history_df = huggingface_trainer_history_df[['epoch', 'eval_loss', 'eval_accuracy']]
huggingface_trainer_history_df.columns = ['Epoch', 'Train Loss', 'Accuracy']
huggingface_trainer_history_df = huggingface_trainer_history_df.dropna()
huggingface_trainer_history_df.loc[2] = [3, 0.435300, 0.848039]
huggingface_trainer_history_df.insert(2,'Validation Loss',[0.394289, 0.365772,0.402529],True)
huggingface_trainer_history_df.insert(4,'F1 Score',[0.864111, 0.898305, 0.895270],True)
huggingface_trainer_history_df = huggingface_trainer_history_df.round(4)
huggingface_trainer_history_df = huggingface_trainer_history_df.astype({'Epoch':'int'})

huggingface_trainer_history_df.to_csv("distilbert_base_uncased_trainer_mrpc/huggingface_trainer_history.csv", index = False)
display(huggingface_trainer_history_df)
print("\n" * 4)

plt.figure(figsize=(24, 4))
epoch = list(huggingface_trainer_history_df['Epoch'])
train_loss = tuple(huggingface_trainer_history_df['Train Loss'])
validation_loss = tuple(huggingface_trainer_history_df['Validation Loss'])
loss = [train_loss, validation_loss]
accuracy = list(huggingface_trainer_history_df['Accuracy'])
f1_score = list(huggingface_trainer_history_df['F1 Score'])

plt.subplot(1, 3, 1)
plt.plot(train_loss, 'b-', label='Train Loss')
plt.plot(validation_loss, 'r--', label='Validation Loss')
plt.title("Loss")
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(accuracy, label='Accuracy')
plt.title("Accuracy")
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(f1_score, label='F1 Score')
plt.title("F1 Score")
plt.xlabel('Epoch')
plt.legend()

plt.show()

"""Trainer에 model, arguments, train_dataset, eval_dataset, compute_metrics를 넣고 train을 진행한다.

<br/><br/>
"""

eval_history = distilbert_base_uncased_trainer.evaluate(encoded_dataset['test'])

"""<br/>"""

eval_history

"""<br/>"""

huggingface_trainer_test_history = {'Epoch': 1,
                                    'Test Loss': eval_history['eval_loss'],
                                    'Test Accuracy': eval_history['eval_accuracy'],
                                    'F1 Score': eval_history['eval_f1']}

huggingface_trainer_test_history_df = pd.DataFrame(huggingface_trainer_test_history, index=[0])
huggingface_trainer_test_history_df = huggingface_trainer_test_history_df.round(4)
huggingface_trainer_test_history_df = huggingface_trainer_test_history_df.astype({'Epoch':'int'})
huggingface_trainer_test_history_df.to_csv("distilbert_base_uncased_trainer_mrpc/huggingface_trainer_test_history.csv", index = False)
display(huggingface_trainer_test_history_df)

"""<br/>"""

cd ..

pwd

Image(filename='distillbert_base_uncased_gpu.JPG')

"""<br/>

###Model Evaluation
"""

CSS = """.output {flex-direction: row;}"""
HTML('<style>{}</style>'.format(CSS))

print(colored("Keras Model Result", attrs=['bold']))
print()
display(keras_model_train_history_df)
print("\n" * 2)
display(keras_model_test_history_df)
print("\n" * 4)

print(colored("HuggingFace Trainer Result", attrs=['bold']))
print()
display(huggingface_trainer_history_df)
print("\n" * 2)
display(huggingface_trainer_test_history_df)

"""<br/><br/>

#Conclusion

**Virtual Environment VS Github Contributor**

HuggingFace에 등록된 MRPC 모델의 CLI 코드를 실행했다.
<br/>bert-base-cased MRPC Finetune
<br/>bert-base-uncased MRPC Finetune
<br/>bert_uncased_L-12_H-768_A-12 MRPC Finetune

Tensorflow 버전 차이 때문에 AttributeError, TyperError가 발생한다.
<br/>py 파일을 여러 번 수정해도 또 다른 에러가 떠서 아예 코드 전체를 바꿔야 하는 디버깅 작업이 요구된다.

이러한 상황에서 그 작동 코드의 옛날 버전에 맞는 가상환경을 구축하거나
<br/>코드 전체를 바꿔서 원작자에게 연락하여 Github Contributor가 되는 문제 해결 방법이 있는 것인가?
<br/>개발자는 모델 개발과 배포 프로세스에서 발생한 이슈에 어떻게 대처하는가?

**Distributed Computing VS Mini Model**

[facebook/bart-large](https://huggingface.co/facebook/bart-large)에서 RuntimeError로 인해 batch_size = 1, num_train_epochs=1로 설정했는데도
<br/>모델 학습 예상 시간이 106시간이 넘어서 작업을 중지했다.

[google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator)에서
<br/>RuntimeError로 인해 batch_size = 16, num_train_epochs=3으로 설정했는데도 
<br/>모델 학습 예상 시간이 17시간이 넘어서 작업을 중지했다.

[facebook/bart-large](https://huggingface.co/facebook/bart-large)와 [google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator)가 대용량 모델이기 때문에
<br/>분산 컴퓨팅을 설정하거나 소형 모델을 만드는 결정을 내려야 한다.

<br/>

#Reference

**LMS**
<br/>[mjmingd](https://github.com/mjmingd)

<br/>**공식 사이트**
<br/>DACON
<br/>[뉴스 토픽 분류 AI 경진대회 private 27위 TwoKimBrothers팀 코드공유](https://dacon.io/competitions/official/235747/codeshare/3050)
<br/><br/>Korpora
<br/>[청와대 국민청원](https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/korean_petitions.html)
<br/><br/>파이토치 한국 사용자 모임
<br/>[cuda out of memory 오류 해결](https://discuss.pytorch.kr/t/cuda-out-of-memory/55)

<br/>**Github**
<br/>HuggingFace
<br/>[Transformers Release](https://github.com/huggingface/transformers/releases)
<br/><br/>sgugger
<br/>[# v4.21.1: Patch release](https://github.com/huggingface/transformers/releases/tag/v4.21.1)
<br/><br/>Beomi
<br/>[2021-03-15-kcbert-mlm-finetune-with-petition-dataset.ipynb](https://gist.github.com/Beomi/972c6442a9c15a22dfd1903d0bb0f577)
<br/>[KcBERT MLM Finetune으로 Domain adaptation하기](https://beomi.github.io/2021/03/15/KcBERT-MLM-Finetune/)
<br/><br/>ko-nlp
<br/>[Korpora: Korean Corpora Archives](https://github.com/ko-nlp/Korpora)
<br/><br/>lovit
<br/>[청와대 국민청원 데이터 아카이브](https://github.com/lovit/petitions_archive)
<br/><br/>jacobdevlin-google
<br/>[google-research/bert/Sentence (and sentence-pair) classification tasks](https://github.com/google-research/bert)
<br/><br/>W4ngatan
<br/>[download_glue_data.py](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)
<br/><br/>vlasenkoalexe
<br/>[download_glue_data.py (fixed script)](https://gist.github.com/vlasenkoalexey/fef1601580f269eca73bf26a198595f3)
<br/><br/>birdmw
<br/>[AttributeError: module 'tensorflow' has no attribute 'flags' #1754](https://github.com/tensorflow/tensor2tensor/issues/1754)
<br/><br/>Gabriel-Molinas
<br/>[AttributeError: module 'tensorflow' has no attribute 'app' #34431](https://github.com/tensorflow/tensorflow/issues/34431)
<br/><br/>Zhen-Dong
<br/>[QBERT/NLP-quantization/Fine-tuning with BERT: examples](https://githubmemory.com/index.php/repo/Zhen-Dong/QBERT)
<br/><br/>wmathor
<br/>[How to prevent tokenizer from outputting certain information #14285](https://github.com/huggingface/transformers/issues/14285)
<br/><br/>ssdorsey 
<br/>[XLMRobertaTokenizer vocab size #2976](https://github.com/huggingface/transformers/issues/2976)
<br/><br/>sgugger
<br/>[text_classification.ipynb](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)
<br/><br/>yeoooo
<br/>[(GD9)Hugging_face_yeoooo_2021-05-18.ipynb](https://github.com/yeoooo/Aiffel)
<br/><br/>suheeeee
<br/>[GD_NLP_6_transformer_chatbot.ipynb](https://github.com/suheeeee/my_deeplearning_project/blob/main/going_deeper/GD_NLP_9_huggingface_custom.ipynb)

<br/>**Hugging Face**
<br/>[Installation](https://huggingface.co/docs/transformers/installation#installing-from-source%60)
<br/>[Transformers Examples](https://huggingface.co/transformers/v2.9.1/examples.html)
<br/>[Process](https://huggingface.co/docs/datasets/process)
<br/>[beomi/kcbert-base](https://huggingface.co/beomi/kcbert-base)
<br/>[bert-base-cased](https://huggingface.co/bert-base-cased)
<br/>[bert-base-uncased](https://huggingface.co/bert-base-uncased)
<br/>[google/bert_uncased_L-12_H-768_A-12](https://huggingface.co/google/bert_uncased_L-12_H-768_A-12/tree/main)
<br/>[facebook/bart-large](https://huggingface.co/facebook/bart-large)
<br/>[google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator)
<br/>[distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)
<br/>[Huggingface transformers longformer optimizer warning AdamW](https://discuss.huggingface.co/t/huggingface-transformers-longformer-optimizer-warning-adamw/14711/3)
<br/>[run_glue.py with my own dataset of one-sentence input](https://discuss.huggingface.co/t/run-glue-py-with-my-own-dataset-of-one-sentence-input/3098/6)

<br/>**Kaggle**
<br/>[huggingface transformer basic usage](https://www.kaggle.com/code/nageshsingh/huggingface-transformer-basic-usage)
<br/>[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)

<br/>**웹사이트**
<br/>[PYTORCH / HUGGINGFACE CUSTOM DATASET으로 BERT 학습하기 – GPU](https://cryptosalamander.tistory.com/141)
<br/>[git 특정 tag clone 하기. git clone -b tag repositor y](https://junho85.pe.kr/433)
<br/>[Error in google colab(python) while transliterating data into Indian language](https://stackoverflow.com/questions/73024976/error-in-google-colabpython-while-transliterating-data-into-indian-language)
<br/>[Pretty printing newlines inside a string in a Pandas DataFrame](https://stackoverflow.com/questions/34322448/pretty-printing-newlines-inside-a-string-in-a-pandas-dataframe)
<br/>[Python: how to slice a dictionary based on the values of its keys?](https://stackoverflow.com/questions/40440373/python-how-to-slice-a-dictionary-based-on-the-values-of-its-keys)
<br/>[Error importing BERT: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'](https://stackoverflow.com/questions/61250311/error-importing-bert-module-tensorflow-api-v2-train-has-no-attribute-optimi)
<br/>[AttributeError: module 'tensorflow' has no attribute 'logging'](https://systemout.tistory.com/8)
<br/>[Wikipedia Apollo program](https://en.wikipedia.org/wiki/Apollo_program)
<br/>[How to reorder indexed rows based on a list in Pandas data frame](https://www.google.com/search?q=info.splits&oq=info.splits&aqs=chrome..69i57.395j0j9&sourceid=chrome&ie=UTF-8)
<br/>[Print the structure of large nested dictionaries in a compact way without printing all elements](https://stackoverflow.com/questions/39334638/print-the-structure-of-large-nested-dictionaries-in-a-compact-way-without-printi)
<br/>[Python Pandas: Convert nested dictionary to dataframe](https://stackoverflow.com/questions/31460234/python-pandas-convert-nested-dictionary-to-datafram)
<br/>[TPU를 이용한 BERT GLUE task Train, Evaluation 해보기](https://choice-life.tistory.com/77)
<br/>[How to reorder a python ordered dict based on array?](https://stackoverflow.com/questions/18405537/how-to-reorder-a-python-ordered-dict-based-on-array)
<br/>[Convert Ordereddict into dataframe in python](https://stackoverflow.com/questions/68797498/convert-ordereddict-into-dataframe-in-python)
<br/>[Constructing pandas DataFrame from values in variables gives "ValueError: If using all scalar values, you must pass an index"](https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi)
<br/>[matplotlib label doesn't work](https://stackoverflow.com/questions/14657169/matplotlib-label-doesnt-work)
<br/>[Huggingface Hub에 모델을 업로드하는 3가지 방법](https://zerolang.tistory.com/65)

<br/>**공부**
<br/>[HuggingFace 내 토크나이저 종류 살펴보기](https://huffon.github.io/2020/07/05/tokenizers/)
<br/>[Huggingface transformer 설계구조 살펴보기](https://velog.io/@taekkim/Hugginface-transformer-%EC%84%A4%EA%B3%84%EA%B5%AC%EC%A1%B0-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0)
<br/>[허깅 페이스 BERT 및 ‘가중치 및 편향(W&B)를 통한 문장 분류](https://wandb.ai/wandb_fc/korean/reports/-BERT-W-B---VmlldzozNzAzNzY)
<br/>[toeicbert - pytorch-pretrained-BERT 모델을 사용하여 TOEIC (Test of English for International Communication) 해결.](https://www.wenyanet.com/opensource/ko/6053b6057387cd19e3640ca5.html)
<br/><br/>Dacon
<br/>[Hugging Face를 활용한 Modeling(public: 0.841)](https://dacon.io/competitions/official/235875/codeshare/4520)
<br/><br/>ratsgo
<br/>[학습 파이프라인 소개](https://ratsgo.github.io/nlpbook/docs/introduction/pipeline/)
<br/>[나만의 질의 응답 모델 만들기](https://ratsgo.github.io/nlpbook/docs/qa/detail/)
<br/>[나만의 문서 분류 모델 만들기](https://ratsgo.github.io/nlpbook/docs/doc_cls/detail/)
<br/>[나만의 개체명 인식 모델 만들기](https://ratsgo.github.io/nlpbook/docs/ner/detail/)
<br/>[나만의 문장 쌍 분류 모델 만들기](https://ratsgo.github.io/nlpbook/docs/pair_cls/detail/)
<br/>[문장 쌍 분류 모델 학습하기](https://ratsgo.github.io/nlpbook/docs/pair_cls/train/)
<br/>[Disaster Tweets : multiple vectorizers and models](https://www.kaggle.com/code/zinebkhanjari/disaster-tweets-multiple-vectorizers-and-models/notebook?scriptVersionId=52721572)
"""