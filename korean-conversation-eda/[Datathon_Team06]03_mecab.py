# -*- coding: utf-8 -*-
"""[Datathon_Team06]03_Mecab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y5UQz9ZkfOKUHTrd71wW61iLjLIylmuD

#데이터톤

[AI HUB 한국어 대화 요약](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=116&topMenu=100&aihubDataSe=ty&dataSetSn=117)

개발 환경
<br/>데이터 불러오기
<br/>전처리
* Null 값 제외
* 동의어 제외

토큰화
* MeCab
* 명사/형태소 빈도수 csv 파일을 바탕으로 불용어 리스트 수정

형태소 빈도수 csv 파일
<br/>명사 빈도수 csv 파일 (한 글자 단어 제외)

토큰화
* MeCab

MeCab + wordlist_to_indexlist
* 정제
* 토큰화
* 단어 인덱스 생성
* 정수 인코딩

시각화
* 문장 길이 및 분포
* 적절한 최대 문장 길이 지정

* LDA 토픽 모델링
> 한 글자 단어 제외한 토큰화
<br/>훈련 및 시각화

* 워드클라우드 
> 한 글자 단어 제외한 토큰화
<br/>시각화

* Folium
> 행정구역(SHP) 데이터
<br/>행정구역명 데이터
<br/>행정구역별 지역명 빈도수 통계 데이터
<br/>행정구역별 지역명 빈도수 지도
<br/>버블 차트
<br/>히트맵

Okt. Mecab 비교
* Mecab으로 분리된 단어 개수
* 원문과 요약문 - 코사인 유사도

<br/>참고문헌

#개발 환경
"""

import pandas as pd
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

import os
# install konlpy, jdk, JPype
!pip install konlpy
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip3 install JPype1-py3

# install mecab-ko
os.chdir('/tmp/')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
!tar zxfv mecab-0.996-ko-0.9.2.tar.gz
os.chdir('/tmp/mecab-0.996-ko-0.9.2')
!./configure
!make
!make check
!make install

# install mecab-ko-dic
!apt-get install automake
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
!tar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.1.1-20180720')
!./autogen.sh
!./configure
!make
!make install

# install mecab-python
os.chdir('/content')
!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git
os.chdir('/content/mecab-python-0.996')
!python3 setup.py build
!python3 setup.py install

import konlpy
from konlpy.tag import Mecab

print(pd.__version__)
print(np.__version__)
print(keras.__version__)
print(konlpy.__version__)

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""#데이터 불러오기"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Dataton/event_conversation.csv")

data

data.head

print(data[0:1]['text'])

"""#전처리

##Null 값 제외
"""

data.dropna(axis=0, inplace=True)
print('Null 값 제거 후 전체 샘플 수:', (len(data)))

"""##동의어 제외"""

contractions = {"엄마": "어머니", "아빠": "아버지", "여보": "여봉", "친구": "칭구", "이번주": "요번주", "보자" : "보쟈", 
                "보자" : "보장", "아마": "아마도", "근데": "그런데", "그냥": "구냥", "그니까": "그러니까", "머": "머머",
                "얼른": "언능", "언제": "언제쯤", "괜찮아": "괜츈", "오키오키": "오키도키", "알았어": "아라써",
                "알았어": "알써", "그래": "그려", "아냐": "아냐아냐", "어차피": "어짜피", "마자": "마쟈", "조아": "죠아",
                "허허허": "허허", "흠": "훔", "미안": "미안해"}
                
print("정규화 사전 수: ", len(contractions))

def preprocess_sentence(sentence):
  sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(" ")])
  return sentence

clean_text = []
for s in data['text']:
    clean_text.append(preprocess_sentence(s))

print("text 전처리 후 결과: ", clean_text[:1])

data['text'] = clean_text

"""#토큰화

##MeCab
"""

tokenizer = Mecab()
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']

def mecab_morphs(data, num_words=10000):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
  
    X_mecab_morph_list = []
    for sentence in data['text']:
        temp_X = tokenizer.morphs(sentence) 
        temp_X = [word for word in temp_X if not word in stopwords] 
        X_mecab_morph_list.append(temp_X)
    return X_mecab_morph_list

X_mecab_morphs = mecab_morphs(data)

type(X_mecab_morphs[:1])

X_mecab_morphs[1:2][0]

for i in range(0, len(X_mecab_morphs[:1][0])):
  if i % 4 == 0 :
    print() 
  print(X_mecab_morphs[:1][0][i], end=' ')

"""#형태소 빈도수 csv 파일"""

import itertools
from collections import Counter

X_mecab_morphs = mecab_morphs(data)

onedim_morph_words = list(itertools.chain(*X_mecab_morphs))

morph_count = Counter(onedim_morph_words)
morph_words = dict(morph_count.most_common())

type(morph_words)

morph_words['왜']

morph_series = pd.Series(morph_words)

morph_series

morph_df = pd.DataFrame(morph_series).reset_index()
morph_df.columns = ['형태소', '빈도수']

morph_df.head()

morph_df.to_csv('/content/drive/MyDrive/Dataton/morph_dict.csv', index=False)

morph_dict_path = '/content/drive/MyDrive/Dataton/morph_dict.csv'
morph_dict_csv = pd.read_csv(morph_dict_path)

morph_dict_csv

"""#명사 빈도수 csv 파일 (한 글자 단어 제외)

곰, 문, 달, 발 등 길이가 1인 단어는 제외
"""

def mecab_nouns_list(data):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
    temp_X_list = []
    for i in range(len(data['text'])):
      temp_X = tokenizer.nouns(data['text'][i])
      two = [x for x in temp_X if len(x) > 1 and not x in stopwords]
      temp_X_list.append(two)
         
    return temp_X_list

X_mecab_nouns_two = mecab_nouns_list(data)

import itertools
from collections import Counter

onedim_X_mecab_nouns_two = list(itertools.chain(*X_mecab_nouns_two))

nouns_count = Counter(onedim_X_mecab_nouns_two)
nouns_words = dict(nouns_count.most_common())

type(nouns_words)

nouns_words['서울']

nouns_series = pd.Series(nouns_words)

nouns_series

nouns_df = pd.DataFrame(nouns_series).reset_index()
nouns_df.columns = ['명사', '빈도수']

nouns_df.head()

nouns_df.to_csv('/content/drive/MyDrive/Dataton/nouns_dict.csv', index=False)

nouns_dict_path = '/content/drive/MyDrive/Dataton/nouns_dict.csv'
nouns_dict_csv = pd.read_csv(nouns_dict_path)

nouns_dict_csv

"""#토큰화

##MeCab

##명사/형태소 빈도수 csv 파일을 바탕으로 불용어 리스트 수정
"""

tokenizer = Mecab()
revised_stopwords = ['의','가','이','은','들','는','좀','잘','걍','과''도','를','으로','자','에','와','한','하다'
,' 박 일', '이름', '계정', '전번', '주소', '소속', '기타', '이모티콘', '시스템 사진',
' 월 일', ' 일날', ' 시', ' 시 반', ' 분', ' 이랑', ' 랑',]

def revised_mecab_morphs(data, num_words=10000):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
  
    X_mecab_morph_list = []
    for sentence in data['text']:
        temp_X = tokenizer.morphs(sentence) 
        temp_X = [word for word in temp_X if not word in revised_stopwords] 
        X_mecab_morph_list.append(temp_X)
    return X_mecab_morph_list

revised_X_mecab_morphs = revised_mecab_morphs(data)

type(revised_X_mecab_morphs[:1])

revised_X_mecab_morphs[1:2][0]

for i in range(0, len(revised_X_mecab_morphs[:1][0])):
  if i % 4 == 0 :
    print() 
  print(revised_X_mecab_morphs[:1][0][i], end=' ')

"""#MeCab + wordlist_to_indexlist"""

def revised_Tokenizer(data, num_words=10000):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
  
    X = []
    for sentence in data['text']:
        temp_X = tokenizer.morphs(sentence) 
        temp_X = [word for word in temp_X if not word in revised_stopwords] 
        X.append(temp_X)
    
    words = np.concatenate(X).tolist()
    counter = Counter(words)
    counter = counter.most_common(10000-4)
    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]
    word_to_index = {word:index for index, word in enumerate(vocab)}
        
    def wordlist_to_indexlist(wordlist):
        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]
        
    X = list(map(wordlist_to_indexlist, X))
        
    return X, word_to_index

"""###정제

drop_duplicates는 데이터의 중복을 제거한다.
<br/>dropna는 NaN 결측치를 제거한다.

###토큰화

tokenizer는 Mecab 형태소 분석기를 이용해 토큰화하고
<br/>불용어(Stopwords)는 토큰화에 포함시키지 않는다.

###단어 인덱스 생성

concatenate는 단어 numpy 배열을 하나로 합친다.
 <br/>counter는 단어의 수를 합산한다.
 <br/>enumerate는 반복문 사용 시 몇 번째 반복문인지 확인한다.
 <br/>이를 통해 단어 사전 word_to_index를 구성한다.
 <br/>텍스트 스트링을 사전 인덱스 스트링으로 변환한다.
"""

X, word_to_index = revised_Tokenizer(data)

index_to_word = {index:word for word, index in word_to_index.items()}

index_to_word

"""##정수 인코딩"""

def get_encoded_sentence(sentence, word_to_index):
    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]

def get_encoded_sentences(sentences, word_to_index):
    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]

def get_decoded_sentence(encoded_sentence, index_to_word):
    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])

def get_decoded_sentences(encoded_sentences, index_to_word):
    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]

"""#시각화

##대화 길이 및 분포
"""

total_data_text = list(X)
num_tokens = [len(tokens) for tokens in total_data_text]
num_tokens = np.array(num_tokens)

print('대화 개수 : ',  len(num_tokens))
print('최대 길이 : ', np.max(num_tokens))
print('최소 길이 : ', np.min(num_tokens))
print('평균 : ', np.mean(num_tokens))
print('표준편차 : ', np.std(num_tokens))

plt.hist([s for s in num_tokens if s >= 1], bins=50)
plt.xlabel('length of sample')
plt.ylabel('number of sample')
plt.show()

print('최대 : {}'.format(np.max(num_tokens)))
print('평균 : {}'.format(np.mean(num_tokens)))
print('중앙값 : {}'.format(np.median(num_tokens)))

plt.boxplot(num_tokens)
plt.show

"""##적절한 최대 대화 길이 지정"""

max_tokens = np.mean(num_tokens) + np.std(num_tokens)
text_max_len = int(max_tokens)
print('pad_sequences maxlen : ', text_max_len)
print('전체 대화의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))

"""평균에 표준편차를 더한 것으로 최대 대화 길이를 지정한다.

이 길이를 선택했을 때, 얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인한다.

##LDA 토픽 모델링

사람들은 행사의 어떤 주제에 대해 관심을 갖는가?
<br/>대화 텍스트에서 중심 키워드를 추출하여 사람들의 관심사를 알아본다.

**참고사이트**
<br/>[유원준 외 1명, 딥러닝을 이용한 자연어 처리 입문, wikidocs, 2022](https://wikidocs.net/30708)

토픽 모델링은 문서의 집합에서 토픽을 찾아내는 프로세스를 말한다.
<br/>이는 검색 엔진, 고객 민원 시스템 등과 같이 문서의 주제를 알아내는 일이 중요한 곳에서 사용된다.
<br/>잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)은 토픽 모델링의 대표적인 알고리즘이다.
"""

tokenizer = Mecab()
revised_stopwords = ['의','가','이','은','들','는','좀','잘','걍','과''도','를','으로','자','에','와','한','하다'
,' 박 일', '이름', '계정', '전번', '주소', '소속', '기타', '이모티콘', '시스템 사진',
' 월 일', ' 일날', ' 시', ' 시 반', ' 분', ' 이랑', ' 랑',]

len(data['text'])

"""###한 글자 단어 제외한 토큰화
<br/>곰, 문, 달, 발 등 길이가 1인 단어는 제외
"""

tokenizer = Mecab()
revised_stopwords = ['의','가','이','은','들','는','좀','잘','걍','과''도','를','으로','자','에','와','한','하다'
,' 박 일', '이름', '계정', '전번', '주소', '소속', '기타', '이모티콘', '시스템 사진',
' 월 일', ' 일날', ' 시', ' 시 반', ' 분', ' 이랑', ' 랑',]

def revised_mecab_nouns_list(data):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
    temp_X_list = []
    for i in range(len(data['text'])):
      temp_X = tokenizer.nouns(data['text'][i])
      two = [x for x in temp_X if len(x) > 1 and not x in revised_stopwords]
      temp_X_list.append(two)
         
    return temp_X_list

revised_X_mecab_nouns_two = revised_mecab_nouns_list(data)

"""자료형은 중첩리스트이다.

[ [], [], [], [] ]
"""

type(revised_X_mecab_nouns_two)

type(revised_X_mecab_nouns_two[:2])

print(revised_X_mecab_nouns_two[:2], end='')

type(revised_X_mecab_nouns_two[:2][0][0])

print(revised_X_mecab_nouns_two[:1][0][:3], end='')

"""단어 단위로 토크나이즈한 토큰 집합을 가져온다."""

from gensim import corpora
dictionary = corpora.Dictionary(revised_X_mecab_nouns_two)
corpus = [dictionary.doc2bow(text) for text in revised_X_mecab_nouns_two]
print(corpus[1]) # 수행된 결과에서 두번째 대화 출력. 첫번째 대화의 인덱스는 0

"""각 단어에 정수 인코딩을 하는 동시에, 각 뉴스에서의 단어의 빈도수를 기록한다.
<br/>여기서는 각 단어를 (word_id, word_frequency)의 형태로 바꾼다.

word_id는 단어가 정수 인코딩된 값
 <br/>word_frequency는 해당 뉴스에서의 해당 단어의 빈도수를 의미한다.
"""

print(dictionary[13])

"""두 번째 대화에서 12로 인코딩된 숫자가 1번 나왔다."""

len(dictionary)

"""한 글자 단어 제외 시 총 학습된 단어의 개수 12147개"""

def revised_mecab_nouns_list(data, num_words=10000):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
  
    X_mecab_nouns_list2 = []
    for sentence in data['text']:
        temp_X = tokenizer.nouns(sentence) 
        temp_X = [word for word in temp_X if not word in revised_stopwords] 
        X_mecab_nouns_list2.append(temp_X)
    return X_mecab_nouns_list2

revised_X_mecab_nouns_included_one = revised_mecab_nouns_list(data)

from gensim import corpora
dictionary_included_one = corpora.Dictionary(revised_X_mecab_nouns_included_one)
corpus_included_one = [dictionary.doc2bow(text) for text in revised_X_mecab_nouns_included_one]

len(dictionary_included_one)

"""한 글자 단어 포함시 총 학습된 단어의 개수 13198개"""

len(dictionary_included_one) - len(dictionary)

"""LDA를 위해 제외된 한 글자 단어의 개수는 1051개

###훈련 및 시각화
"""

import gensim
NUM_TOPICS = 40 # 40개의 토픽, k=40
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
topics = ldamodel.print_topics(num_words=4)

for topic in topics:
    print(topic)

pip install pyLDAvis

import pyLDAvis.gensim_models

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)
pyLDAvis.display(vis)

from IPython.display import Image
Image("/content/drive/MyDrive/Dataton/Mecab_LDA_Topic1.JPG", width= 1236, height = 771)

Image("/content/drive/MyDrive/Dataton/Mecab_LDA_Topic2.JPG", width= 1252, height = 756)

def make_topictable_per_doc(ldamodel, corpus):
    topic_table = pd.DataFrame()

    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.
    for i, topic_list in enumerate(ldamodel[corpus]):
        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            
        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)
        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.
        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), 
        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)
        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.

        # 모든 문서에 대해서 각각 아래를 수행
        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.
            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽
                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)
                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.
            else:
                break
    return(topic_table)

topictable = make_topictable_per_doc(ldamodel, corpus)
topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.
topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']
topictable[:10]

"""##워드클라우드

**참고 사이트**
<br/>[카톡 데이터로 워드 클라우드 그리기](https://m.blog.naver.com/nilsine11202/221834254905)

###한 글자 단어 제외한 토큰화
<br/>곰, 문, 달, 발 등 길이가 1인 단어는 제외
"""

tokenizer = Mecab()
revised_stopwords = ['의','가','이','은','들','는','좀','잘','걍','과''도','를','으로','자','에','와','한','하다'
,' 박 일', '이름', '계정', '전번', '주소', '소속', '기타', '이모티콘', '시스템 사진',
' 월 일', ' 일날', ' 시', ' 시 반', ' 분', ' 이랑', ' 랑',]

def revised_mecab_nouns_list(data):
    data.drop_duplicates(subset=['text'], inplace=True)
    data = data.dropna(how = 'any') 
    temp_X_list = []
    for i in range(len(data['text'])):
      temp_X = tokenizer.nouns(data['text'][i])
      two = [x for x in temp_X if len(x) > 1 and not x in revised_stopwords]
      temp_X_list.append(two)
         
    return temp_X_list

revised_X_mecab_nouns_two = revised_mecab_nouns_list(data)

import itertools
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

revised_onedim_X_mecab_nouns_two = list(itertools.chain(*revised_X_mecab_nouns_two))

"""하나의 리스트로 모은다."""

count = Counter(revised_onedim_X_mecab_nouns_two)
words = dict(count.most_common())

print(words.keys(), end='')

print(words.values(), end='')

"""###시각화"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
import matplotlib 
from IPython.display import set_matplotlib_formats 
matplotlib.rc('font',family = 'Malgun Gothic') 
set_matplotlib_formats('retina') 
matplotlib.rc('axes',unicode_minus = False)

!apt-get update -qq
!apt-get install fonts-nanum* -qq

font = '/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf'

"""코랩에서 한글 표기를 위해 폰트 설치"""

wordcloud = WordCloud(font_path=font,
                      background_color='white', colormap = "hot",width=3000, height=2000).generate_from_frequencies(words)

plt.imshow(wordcloud) 
figure = plt.gcf() # get current figure
figure.set_size_inches(8, 6)
plt.axis('off') 
plt.show()

"""##Folium

folium를 활용해 지도에 지역구 영역을 표시하고 행사 현황을 시각화해보자.

행사 현황 = 총 대화에서 지역명이 언급된 횟수

예를 들어 데이터에서 서울이 45번, 부산이 20번 언급됐으면
<br/>지도에서 서울이 부산보다 더 진하게 표시된다.

위치정보와 지역명 빈도 데이터 병합

**참고 사이트**
<br/>[전국 인구 현황 지도 시각화](https://mkjjo.github.io/python/2019/08/18/korea_population.html)
<br/>[Folium을 이용한 데이타 시각화](https://blog.naver.com/PostView.nhn?blogId=kcchang61&logNo=221350672356)
<br/>[대한민국 최신 행정구역(SHP) 다운로드](http://www.gisdeveloper.co.kr/?p=2332)
<br/>[대한민국 행정구역 경계 JSON 파일 변환 및 다운받기](https://yeomss.tistory.com/267)
<br/>[대한민국 행정구역(시도, 시군구) GeoJSON 파일 다운로드 및 SHP 파일 단순화 후 변환 방법 설명](https://neurowhai.tistory.com/350)
"""

import requests
import pandas as pd
import numpy as np
from pandas.io.json import json_normalize
import os
import webbrowser
import folium
from folium import plugins

print(folium.__version__)

"""####행정구역(SHP) 데이터

행정구역 시/군/구 정보를 담고 있는 Json 파일 불러오기
"""

state_geo = '/content/drive/MyDrive/Dataton/TL_SCCO_SIG.json'

"""####행정구역명 데이터

[통계청 오픈 데이터](https://kosis.kr/statHtml/statHtml.do?orgId=101&tblId=DT_1BPA002&vw_cd=&list_id=&scrId=&seqNo=&lang_mode=ko&obj_var_id=&itm_id=&conn_path=K1)에서 지역명 리스트 가져오기
"""

population_path = '/content/drive/MyDrive/Dataton/행정구역_시군구_별_주민등록세대수_20220601135321.csv'
population_csv = pd.read_csv(population_path, encoding='CP949')

area_csv = population_csv['행정구역(시군구)별']

area_csv.to_csv("/content/drive/MyDrive/Dataton/area.csv", index = False)

area_path = '/content/drive/MyDrive/Dataton/area.csv'
area_data = pd.read_csv(area_path)

area_data.head

len(area_data)

area_data[1:5]

"""####행정구역별 지역명 빈도수 통계 데이터

행정구역별 지역명 언급 빈도 통계 데이터 csv 파일 만들기
"""

count = Counter(revised_onedim_X_mecab_nouns_two)
words = dict(count.most_common())

nouns_words_series = pd.Series(words)

nouns_words_series

nouns_words_df = pd.DataFrame(nouns_words_series).reset_index()
nouns_words_df.columns = ['명사', '빈도수']

nouns_words_df.to_csv('/content/drive/MyDrive/Dataton/nouns_words_dict.csv', index=False)

nouns_words_dict_path = '/content/drive/MyDrive/Dataton/nouns_words_dict.csv'
nouns_words_dict_csv = pd.read_csv(nouns_words_dict_path)

nouns_words_dict_csv[:3]

nouns_words_dict_csv[nouns_words_dict_csv['명사'].str.contains('제주', na = False)]

sum(nouns_words_dict_csv[nouns_words_dict_csv['명사'].str.contains('제주', na = False)]['빈도수'])

area_name_list = ['서울', '종로', '용산', '성동', '광진', '동대문', '중랑', '성북', '강북', '도봉', '노원', '은평', '서대문', '마포', '양천',
'강서', '구로', '금천', '영등포', '동작', '관악', '서초', '강남', '송파', '강동', '부산', '영도', '동래', '해운대', '사하', '금정', '연제', 
'사상', '기장', '대구', '수성', '달서', '달성', '인천', '미추홀', '연수', '남동', '부평', '계양', '강화', '옹진', '광주', '광산', '대전', '유성',
'대덕', '울산', '울주', '세종', '수원', '장안', '권선', '팔달', '영통', '성남', '중원', '분당', '의정부', '안양', '만안', '동안', '부천', '광명', 
'평택', '동두천', '안산', '상록', '단원', '고양', '덕양', '일산', '과천', '구리', '남양주', '오산', '시흥', '군포', '의왕', '하남', '용인', '처인',
'기흥', '수지', '파주', '이천', '안성', '김포', '화성', '광주', '양주', '포천', '여주', '연천', '가평', '양평', '춘천', '원주', '강릉', '동해', '태백',
'속초', '삼척', '홍천', '횡성', '영월', '평창', '정선', '철원', '화천', '양구', '인제', '고성', '양양', '청주', '상당', '흥덕', '청원', '충주',
'제천', '보은', '옥천', '영동', '증평', '진천', '괴산', '음성', '단양', '천안', '공주', '보령', '아산', '서산', '논산', '계룡', '당진', '금산', '부여', '서천',
'청양', '홍성', '예산', '태안', '전주', '완산', '덕진', '군산', '익산', '정읍', '남원', '김제', '완주', '진안', '무주', '임실', '순창', '고창', '부안',
'목포', '여수', '순천', '나주', '광양', '담양', '곡성', '구례', '고흥', '보성', '화순', '장흥', '강진', '해남', '영암', '무안', '함평', '영광', '장성',
'완도', '진도', '신안', '포항', '경주', '김천', '안동', '구미', '영주', '영천', '상주', '문경', '경산', '군위', '의성', '청송', '영양', '영덕', '청도',
'고령', '성주', '칠곡', '예천', '봉화', '울진', '울릉', '창원', '의창', '성산', '영양', '영덕', '청도', '고령', '성주', '칠곡', '마산', '진해',
'진주', '통영', '사천', '김해', '밀양', '거제', '양산', '의령', '함안', '창녕', '고성', '남해', '하동', '산청', '함양', '거창', '합천', '제주', '서귀포']

len(area_name_list)

area_df = pd.DataFrame({'지역명': ['지역'],
                             '빈도수':[0]})

area_df2 = pd.DataFrame({'지역명': ['지역'],
                             '빈도수':[0]})

for i in range(236):
  area_df = area_df.append(area_df2)

len(area_df)

for i in range(len(area_name_list)):
  area_df['지역명'][i:i+1] = area_name_list[i]
  area_df['빈도수'][i:i+1]= sum(nouns_words_dict_csv[nouns_words_dict_csv['명사'].str.contains(area_name_list[i], na = False)]['빈도수'])

area_df

area_descend_df = area_df.sort_values(by = ['빈도수'], ascending=[False])

area_descend_df.to_csv('/content/drive/MyDrive/Dataton/area_name_frequency_dict.csv', index=False)

area_code_list = [11140, 11110, 11170, 11200, 11215, 11230, 11260, 11290, 11305, 11320, 11350, 11380, 11410, 11440, 11470,
11500, 11530, 11545, 11560, 11590, 11620, 11650, 11680, 11710, 11740 , 26110, 26200, 26260, 26350, 26380, 26410, 26470,
26530, 26710, 27110, 27290, 27290, 27710, 28110, 28170, 28185, 28200, 28237, 28245, 28710, 28720, 29110, 29200, 30140, 30200,
30230, 31110, 31710, 29010, 41110, 41111, 41113, 41115, 41117, 41130, 41133, 41135, 41150, 41170, 41171, 41173, 41190, 41210, 
41220, 41250, 41270, 41271, 41273, 41280, 41281, 41285, 41290, 41310, 41360, 41370, 41390, 41410, 41430, 41450, 41460, 41461,
41463, 41465, 41480, 41500, 41550, 41570, 41590, 41610, 41630, 41650, 41730, 41800, 41820, 41830, 42110, 42130, 42150, 42170, 42190,
42210, 42230, 42720, 42730, 42750, 42760, 42770, 42780, 42790, 42800, 42810, 42820, 42830, 43110, 43111, 43113, 43710, 43130,
43150, 43720, 43730, 43740, 43745, 43750, 43760, 43770, 43800, 44130, 44150, 44180, 44200, 44210, 44230, 44250, 44830, 44710, 44760, 44770,
44790, 44800, 44810, 44825, 45110, 45111, 45113, 45130, 45140, 45180, 45190, 45210, 45710, 45720, 45730, 45750, 45770, 45790, 45800,
46110, 46130, 46150, 46170, 46230, 46710, 46720, 46730, 46770, 46780, 46790, 46800, 46810, 46820, 46830, 46840, 46860, 46870, 46880,
46890, 46900, 46910, 47110, 47130, 47150, 47170, 47190, 47210, 47230, 47250, 47280, 47290, 47720, 47730, 47750, 47760, 47770, 47820,
47830, 47840, 47850, 47900, 47920, 47930, 47940, 48120, 48121, 48123, 47760, 47770, 47820, 47830, 47840, 47850, 48125, 48129,
48170, 48220, 48240, 48250, 48270, 48310, 48330, 48720, 48730, 48740, 48820, 48840, 48850, 48860 , 48870, 48880, 48890, 50110, 50130]

len(area_code_list)

for i in range(len(area_name_list)):
  area_df['지역명'][i:i+1] = area_code_list[i]

area_df

"""###"""

area_df['지역명'] = area_df.지역명.map(lambda x : str(x).zfill(5))

"""###행정구역별 지역명 빈도수 지도"""

basic_map = folium.Map(location=[36, 127], tiles="OpenStreetMap", zoom_start=7.4)

basic_map.choropleth(
    geo_data=state_geo,
    name='지역명 언급 빈도수',
    data=area_df,
    columns=['지역명', '빈도수'],
    key_on='feature.properties.SIG_CD',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.3,
    color = 'gray',
    legend_name = 'Frequency'
)

folium.LayerControl().add_to(basic_map)

basic_map

basic_map.save('/content/drive/MyDrive/Dataton/area_name_frequency_basic_map.html')

Image("/content/drive/MyDrive/Dataton/Mecab_Folium_basic_map.JPG", width= 828, height = 663)

"""###버블 차트"""

import json
json_data=open(state_geo).read()
jsonResult = json.loads(json_data)

def center_calc(points_df):
    x = points_df.x
    y = points_df.y

    X = (max(x)+min(x))/2.
    Y = (max(y)+min(y))/2.

    return X, Y

# 다중 Array 구조 이중으로 변환
def points_array(points):

    final_points = []

    for x in range(0, len(points)):

        if len(points[x]) == 2:
            final_points.append(points[x])
        else:
            target = points[x]
            for y in range(0, len(target)):
                final_points.append(target[y])

    return final_points

center_locations = pd.DataFrame()
codes = []
names = []
x_list = []
y_list = []
for x in range(0, len(jsonResult['features'])):
    code = jsonResult['features'][x]['properties']['SIG_CD']
    name = jsonResult['features'][x]['properties']['SIG_KOR_NM']
    # 중앙값 생성
    points = jsonResult['features'][x]['geometry']['coordinates'][0]
    points = points_array(points)
    points_df = pd.DataFrame(points)
    points_df.columns = ['x','y']
    X, Y = center_calc(points_df)

    # 결과
    codes.append(code)
    names.append(name)
    x_list.append(X)
    y_list.append(Y)

# 데이터 프레임 생성
center_locations['지역명'] = codes
center_locations['NAME'] = names
center_locations['X'] = x_list
center_locations['Y'] = y_list

target_df = pd.merge(area_df,center_locations, how = 'left', on = '지역명')
target_df = target_df[~np.isnan(target_df['X'])] # 위치 정보 없는 값 제외

# Initialize the map:
bubble_m = folium.Map(location=[36, 127], tiles="OpenStreetMap", zoom_start=7)


# I can add marker one by one on the map
for i in range(0,len(target_df)):
    latitude = target_df.iloc[i]['Y']
    longitude = target_df.iloc[i]['X']
    location=(latitude, longitude)
    folium.CircleMarker(location, radius=target_df.iloc[i]['빈도수']/25000,color='#3186cc',fill_color='#3186cc', popup=target_df.iloc[i]['NAME']).add_to(bubble_m)


folium.LayerControl(collapsed=False).add_to(bubble_m)


bubble_m

# Save to html
bubble_m.save('/content/drive/MyDrive/Dataton/area_name_frequency_bubble_map.html')

Image("/content/drive/MyDrive/Dataton/Mecab_Folium_bubble_map.JPG", width= 873, height = 677)

"""###히트맵"""

from folium import plugins
from folium.plugins import HeatMap

heat_map = folium.Map(location=[36, 127], tiles="OpenStreetMap", zoom_start=7)

heat_df = target_df[['Y', 'X']]

# List comprehension to make out list of lists
heat_data = [[row['Y'],row['X']] for index, row in heat_df.iterrows()]

# Plot it on the map
HeatMap(heat_data).add_to(heat_map)

# Display the map
heat_map

Image("/content/drive/MyDrive/Dataton/Mecab_Folium_heat_map.JPG", width= 838, height = 688)

"""#Okt. Mecab 비교

##Mecab으로 추출된 명사 개수
"""

len(nouns_words_dict_csv)

nouns_words_dict_csv.head

nouns_words_dict_csv[:50]

"""##원문과 요약문 - 코사인 유사도

참고 코드
<br/>딥 러닝을 이용한 자연어 처리 입문
<br/>https://wikidocs.net/24603

원문 대화에서 뽑아낸 명사들이 많이 포함될수록 코사인 유사도가 높다.
"""

revised_X_mecab_nouns_two

dialogue_nouns_series = pd.Series(revised_X_mecab_nouns_two)

dialogue_nouns_series[:1]

dialogue_nouns_words_df = pd.DataFrame(dialogue_nouns_series.reset_index())
dialogue_nouns_words_df.columns = ['index', 'dialogue_noun']

for i in range(len(dialogue_nouns_words_df)):
  dialogue_nouns_words_df['dialogue_noun'][i] = set(dialogue_nouns_words_df['dialogue_noun'][i])
  dialogue_nouns_words_df['dialogue_noun'][i] = list(dialogue_nouns_words_df['dialogue_noun'][i])

type(dialogue_nouns_words_df["dialogue_noun"][0])

for i in range(len(dialogue_nouns_words_df["dialogue_noun"])):
  dialogue_nouns_words_df["dialogue_noun"][i] = ' '.join(map(str, dialogue_nouns_words_df["dialogue_noun"][i]))

dialogue_nouns_words_df[:5]

dialogue_nouns_words_df.to_csv('/content/drive/MyDrive/Dataton/dialogue_nouns_words.csv', index=False)

dialogue_nouns_words_df = pd.read_csv("/content/drive/MyDrive/Dataton/dialogue_nouns_words.csv")

event_summary_df = pd.read_csv("/content/drive/MyDrive/Dataton/event_summary.csv")

event_summary_df

import re

repl =''
pattern = '[^가-히\s]' # 특수기호 제거

for i in range(len(event_summary_df['text'])):
  event_summary_df['text'][i] = re.sub(pattern= pattern, repl=repl, string=event_summary_df['text'][i])

event_summary_df

def summary_mecab_nouns_list(data):
    temp_X_list = []
    for i in range(len(data['text'])):
      temp_X = tokenizer.nouns(data['text'][i])
      two = [x for x in temp_X if len(x) > 1 and not x in revised_stopwords]
      temp_X_list.append(two)
         
    return temp_X_list

event_summary_df2 = summary_mecab_nouns_list(event_summary_df)

event_summary_series = pd.Series(event_summary_df2)

event_summary_df = pd.DataFrame(event_summary_series.reset_index())
event_summary_df.columns = ['index', 'summary']

for i in range(len(event_summary_df)):
  event_summary_df['summary'][i] = set(event_summary_df['summary'][i])
  event_summary_df['summary'][i] = list(event_summary_df['summary'][i])

for i in range(len( event_summary_df['summary'])):
   event_summary_df['summary'][i] = ' '.join(map(str,  event_summary_df['summary'][i]))

event_summary_df

event_summary_df['summary'][1]

text_comparision_df = pd.merge(dialogue_nouns_words_df, event_summary_df)

text_comparision_df

text_comparision_df['summary'].replace('', np.nan, inplace=True)

text_comparision_df['dialogue_noun'].replace('', np.nan, inplace=True)

text_comparision_df = text_comparision_df.dropna()

text_comparision_df

text_comparision_df['dialogue_noun'].replace('', np.nan, inplace=True)

text_comparision_df = text_comparision_df.dropna()

text_comparision_df = text_comparision_df.reset_index(drop=True)

text_comparision_df.to_csv('/content/drive/MyDrive/Dataton/text_comparision_df.csv', index=False)

from sklearn.feature_extraction.text import CountVectorizer

import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

cos_sim_list = []

for i in range(len(text_comparision_df)):
  text = []
  text.append(text_comparision_df['dialogue_noun'][i])
  text.append(text_comparision_df['summary'][i])
  
  count_vec = CountVectorizer()
  m = count_vec.fit_transform(text)
  m_array = m.toarray()
  cos_sim_list.append(cos_sim(m_array[0], m_array[1]))

cos_sim_df = pd.Series(cos_sim_list)

cos_sim_df

print('최대 코사인 유사도 : ', np.max(cos_sim_df))
print('평균 : ', np.mean(cos_sim_df))
print('표준편차 : ', np.std(cos_sim_df))

plt.hist([s for s in cos_sim_df], bins=50)
plt.xlabel('cosine similarity')
plt.ylabel('number of sample')
plt.show()

"""#참고문헌

[ukairia777](https://github.com/ukairia777)
<br/>[coffeedjimmy](https://github.com/coffeedjimmy)

<br/>유원준 외 1명, 딥러닝을 이용한 자연어 처리 입문, wikidocs, 2022
<br/>[잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)](https://wikidocs.net/30708)
<br/>[코사인 유사도(Cosine Similarity)](https://wikidocs.net/24603)
<br/><br/>[카톡 데이터로 워드 클라우드 그리기](https://m.blog.naver.com/nilsine11202/221834254905)

<br/>[전국 인구 현황 지도 시각화](https://mkjjo.github.io/python/2019/08/18/korea_population.html)
<br/>[Folium을 이용한 데이타 시각화](https://blog.naver.com/PostView.nhn?blogId=kcchang61&logNo=221350672356)
<br/>[대한민국 최신 행정구역(SHP) 다운로드](http://www.gisdeveloper.co.kr/?p=2332)
<br/>[대한민국 행정구역 경계 JSON 파일 변환 및 다운받기](https://yeomss.tistory.com/267)
<br/>[대한민국 행정구역(시도, 시군구) GeoJSON 파일 다운로드 및 SHP 파일 단순화 후 변환 방법 설명](https://neurowhai.tistory.com/350)
"""