# -*- coding: utf-8 -*-
"""[E_13]BERT_KorQ&A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iJi3QQkYWNKFjlzR8ZXPsIvfIRcWN3EU

##인간보다 퀴즈를 잘푸는 인공지능

개발 환경
<br/>데이터 정보

데이터 전처리
<br/>띄어쓰기 단위 정보관리
<br/>Subword Segmentation
<br/>정답 토큰 범위
<br/>데이터 분리

데이터 탐색
<br/>Histogram. Box Plot
<br/>Wordcloud

데이터 로더

모델 구성

Position Embedding
<br/>Masked
<br/>Shared Embedding
<br/>MultiHead Attention
<br/>Position Wise Feed Forward
<br/>Encoder Layer

BERT
<br/>Non Pretrained Model
<br/>Pretrained Model

모델 학습
<br/>인퍼런스
<br/>모델 평가
<br/>결론
<br/>참고문헌

# 개발 환경
"""

!pip install -q tensorflow-addons sentencepiece

import os
from importlib_metadata import version
from __future__ import absolute_import, division, print_function, unicode_literals

import pickle
import json

import pandas as pd
import numpy as np
import collections
from datetime import datetime
from tqdm.notebook import tqdm

import tensorflow
import tensorflow as tf
import tensorflow.keras.backend as K
import tensorflow_addons as tfa

import re
import sentencepiece as spm
from wordcloud import WordCloud

import matplotlib.pyplot as plt
import seaborn as sns

import random
random_seed = 1234
random.seed(random_seed)
np.random.seed(random_seed)
tf.random.set_seed(random_seed)

from google.colab import drive
drive.mount('/content/drive')

pip freeze > '/content/drive/MyDrive/lms/library_version.txt'

library_name = ['importlib-metadata', 'future', 'pickle', 'json', 'pandas=', 'numpy=', 
                'collections', 'date', 'tensorflow=', 'sentencepiece', 'tqdm', 'keras',
                'addons', 'tensorflow.keras', 'matplotlib=', 'seaborn=']
library_version = []
count = 0

import sys
print(sys.version)
print()

with open('/content/drive/MyDrive/lms/library_version.txt', 'r') as f:
    lines = f.read().splitlines() 

for i in range(len(lines)):
  for line in lines[i:i+1]:
    for library in library_name:
      if library in line:
        library_version.append(line)
        count += 1
        print(line, end = '    ')
        if count % 3 == 0:
          print()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Google Colab에서 할당된 GPU를 확인한다.
<br/>고용량 메모리 VM에 액세스한다.

#데이터 정보

[KorQuAD 1.0](https://korquad.github.io/KorQuad%201.0/)

한국어 Machine Reading Comprehension을 위해 만든 데이터셋이다.
<br/>모든 질의에 대한 답변은 해당 Wikipedia article 문단의 일부 하위 영역으로 이루어진다.
<br/>Stanford Question Answering Dataset(SQuAD) v1.0과 동일한 방식으로 구성되었다.
"""

def print_json_tree(data, indent=""):
    for key, value in data.items():
        if type(value) == list:     
            print(f'{indent}- {key}: [{len(value)}]')
            print_json_tree(value[0], indent + "  ")
        else:
            print(f'{indent}- {key}: {value}')

""" KorQuAD 데이터처럼 json 포맷으로 이루어진 데이터에서
 <br/>list의 첫 번째 item의 실제 내용을 간단히 확인한다.
"""

data_dir = '/content/drive/MyDrive/lms/bert_qna/data'
model_dir = '/content/drive/MyDrive/lms/bert_qna/model'

train_json_path = data_dir + '/KorQuAD_v1.0_train.json'
with open(train_json_path) as f:
    train_json = json.load(f)
    print_json_tree(train_json)

dev_json_path = data_dir + '/KorQuAD_v1.0_dev.json'
with open(dev_json_path) as f:
    dev_json = json.load(f)
    print_json_tree(dev_json)

"""훈련데이터와 검증데이터를 확인한다."""

print(json.dumps(train_json["data"][0]["paragraphs"][0]["qas"][0], indent=2, ensure_ascii=False))

"""#데이터 전처리

##띄어쓰기 단위 정보관리
"""

def _is_whitespace(c):
    if c == " " or c == "\t" or c == "\r" or c == "\n" or ord(c) == 0x202F:
        return True
    return False

def _tokenize_whitespace(string):
    word_tokens = []
    char_to_word = []
    prev_is_whitespace = True

    for c in string:
        if _is_whitespace(c):
            prev_is_whitespace = True
        else:
            if prev_is_whitespace:
                word_tokens.append(c)
            else:
                word_tokens[-1] += c
            prev_is_whitespace = False    
        char_to_word.append(len(word_tokens) - 1)
    
    return word_tokens, char_to_word

"""SQuAD 유형의 문제를 풀 때 글자 혹은 subword 단위로 token이 분리되는 것에 대비해서
<br/>데이터가 띄어쓰기 단위로 어떠했었는지 word token 영역별로 추가 정보를 관리한다.
<br/>띄어쓰기 단위로 token을 정리한 후, word token 영역별로 유니크한 숫자(어절 번호)를 부여한다.
<br/>글자별로 word_token 영역을 표시해 주는 char_to_word list를 관리한다.
"""

string = '1839년 파우스트를 읽었다.'

word_tokens = []
char_to_word = []
prev_is_whitespace = True

for c in string:
    if _is_whitespace(c):
        prev_is_whitespace = True
    else:
        if prev_is_whitespace:
            word_tokens.append(c)
        else:
            word_tokens[-1] += c
        prev_is_whitespace = False    
    char_to_word.append(len(word_tokens) - 1)
    print(f'\'{c}\' : {word_tokens} : {char_to_word}')

"""문장(string)에 대해 띄어쓰기 영역 정보를 표시한다."""

word_tokens, char_to_word = _tokenize_whitespace(string)
for n, (c, i) in enumerate(zip(list(string), char_to_word)):
    print(f'\'{c}\' : {i}', end='       ')
    if n % 4 == 0 and n > 0:
      print()

word_tokens, char_to_word

"""'1839년 파우스트를 읽었다.'에 대한 띄어쓰기 단위 정보를 확인한다.

##Subword Segmentation

SentencePiece 모델을 이용하여 Subword Segmentation을 한다.
<br/>SentencePiece는 언어마다 다른 문법 규칙을 활용하지 않고 적절한 Subword 분절 규칙을 학습하거나
<br/>자주 사용되는 구문을 하나의 단어로 묶어내는 등 통계적인 방법을 사용한다.

Subword Tokenizer는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만
<br/>자주 등장하지 않는 단어의 경우에는 더 작은 단위인 Subword로 분리되어
<br/>Subword들이 단어 집합에 추가된다는 아이디어를 갖고 있다.
<br/>이렇게 단어 집합이 만들어지고 나면, 이 단어 집합을 기반으로 토큰화를 수행한다.
"""

vocab = spm.SentencePieceProcessor()
vocab.load(f"{model_dir}/ko_32000.model")

def _tokenize_vocab(vocab, context_words):
    word_to_token = []
    context_tokens = []
    for (i, word) in enumerate(context_words):
        word_to_token.append(len(context_tokens))
        tokens = vocab.encode_as_pieces(word)
        for token in tokens:
            context_tokens.append(token)
    return context_tokens, word_to_token

"""vocab을 불러온다.
<br/>word를 subword로 변경하면서 index 저장한다.
<br/>SentencePiece를 사용해 Subword로 쪼갠다.
"""

print(word_tokens)

context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)
context_tokens, word_to_token

"""'1839년 파우스트를 읽었다.'에 대한 Subword 단위로 토큰화한 결과를 확인한다.

##정답 토큰 범위

KorQuAD 데이터셋에서 question, context, answer를 추출하기 위해서는
<br/>정답에 해당하는 지문 영역을 정확히 찾아내는 전처리 작업이 필요하다.
"""

context = train_json['data'][0]['paragraphs'][0]['context']
question = train_json['data'][0]['paragraphs'][0]['qas'][0]['question']
answer_text = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']
answer_start = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']
answer_end = answer_start + len(answer_text) - 1

print('[context] ', context[:70])
print('[question] ', question)
print('[answer] ', answer_text)
print('[answer_start] index: ', answer_start, 'character: ', context[answer_start])
print('[answer_end]index: ', answer_end, 'character: ', context[answer_end])

assert context[answer_start:answer_end + 1] == answer_text

"""answer_text에 해당하는 context 영역을 정확히 찾아내고자 한다. """

word_tokens, char_to_word = _tokenize_whitespace(context)

for i in range(0, 18, 4):
  if i + 4 > 18:
    print(str(word_tokens[i : 18]), end=' ')
  elif i + 4 <= 18:
    print(str(word_tokens[i : i+4]), end=' ')
  if i % 4 == 0:
    print()

char_to_word[:18], context[:18]

"""context를 띄어쓰기(word) 단위로 토큰화한 결과를 확인한다."""

context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)
for i in range(min(18, len(word_to_token) - 1)):
    print(word_to_token[i], context_tokens[word_to_token[i]:word_to_token[i + 1]], end='    ')
    if i % 4 == 0 and i > 0:
      print()

"""띄어쓰기(word) 단위로 쪼개진 context(word_tokens)를 Subword로 토큰화한 결과를 확인한다. """

word_start = char_to_word[answer_start]
word_end = char_to_word[answer_end]
word_start, word_end, answer_text, word_tokens[word_start:word_end + 1]

"""answer_start와 answer_end로부터 word_start와 word_end를 구한다.
<br/>정답은 15번째 어절(index=14)에 위치한다.
<br/>그러나 조사가 포함되어 명사가 아닌 '교향곡을'이다.
"""

token_start = word_to_token[word_start]
if word_end < len(word_to_token) - 1:
    token_end = word_to_token[word_end + 1] - 1
else:
    token_end = len(context_tokens) - 1
token_start, token_end, context_tokens[token_start:token_end + 1]

"""word_start로부터 word_end까지의 context를 Subword 단위로 토큰화한 결과를 확인한다."""

token_answer = " ".join(vocab.encode_as_pieces(answer_text))
token_answer

"""실제 정답인 answer_text도 Subword 기준으로 토큰화한다. """

for new_start in range(token_start, token_end + 1):
    for new_end in range(token_end, new_start - 1, -1):
        text_span = " ".join(context_tokens[new_start : (new_end + 1)])
        if text_span == token_answer:   
            print("O >>", (new_start, new_end), text_span)
        else:
            print("X >>", (new_start, new_end), text_span)

"""정답이 될수 있는 new_start와 new_end의 경우를 순회탐색한다.
<br/>정답과 일치하는 경우 O, 다른 경우 X를 출력한다.
"""

def _improve_span(vocab, context_tokens, token_start, token_end, char_answer):
    token_answer = " ".join(vocab.encode_as_pieces(char_answer))
    for new_start in range(token_start, token_end + 1):
        for new_end in range(token_end, new_start - 1, -1):
            text_span = " ".join(context_tokens[new_start : (new_end + 1)])
            if text_span == token_answer:
                return (new_start, new_end)
    return (token_start, token_end)

"""context_tokens에서 char_answer의 위치를 찾아 리턴하는 함수이다."""

token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, answer_text)
print('token_start:', token_start, ' token_end:', token_end)
context_tokens[token_start:token_end + 1]

"""##데이터 분리"""

def dump_korquad(vocab, json_data, out_file):
    with open(out_file, "w") as f:
        for data in tqdm(json_data["data"]):
            title = data["title"]
            for paragraph in data["paragraphs"]:
                context = paragraph["context"]
                context_words, char_to_word = _tokenize_whitespace(context)

                for qa in paragraph["qas"]:
                    assert len(qa["answers"]) == 1
                    qa_id = qa["id"]
                    question = qa["question"]
                    answer_text = qa["answers"][0]["text"]
                    answer_start = qa["answers"][0]["answer_start"]
                    answer_end = answer_start + len(answer_text) - 1

                    assert answer_text == context[answer_start:answer_end + 1]

                    word_start = char_to_word[answer_start]
                    word_end = char_to_word[answer_end]

                    word_answer = " ".join(context_words[word_start:word_end + 1])
                    char_answer = " ".join(answer_text.strip().split())
                    assert char_answer in word_answer

                    context_tokens, word_to_token = _tokenize_vocab(vocab, context_words)

                    token_start = word_to_token[word_start]
                    if word_end < len(word_to_token) - 1:
                        token_end = word_to_token[word_end + 1] - 1
                    else:
                        token_end = len(context_tokens) - 1

                    token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, char_answer)

                    data = {"qa_id": qa_id, "title": title,
                            "question": vocab.encode_as_pieces(question), "context": context_tokens, "answer": char_answer,
                            "token_start": token_start, "token_end":token_end}
                            
                    f.write(json.dumps(data, ensure_ascii=False))
                    f.write("\n")

dump_korquad(vocab, train_json, f"{data_dir}/korquad_train.json")
dump_korquad(vocab, dev_json, f"{data_dir}/korquad_dev.json")

"""데이터를 훈련 데이터, 평가 데이터로 분리한다.
<br/>_improve_span() 함수를 이용해 전처리 후 파일로 저장한다.
"""

def print_file(filename, count=3):

    with open(filename) as f:
        for i, line in enumerate(f):
            if count <= i:
                break
            print(line.strip())

print_file(f"{data_dir}/korquad_train.json")

"""전처리한 파일 내용을 확인한다.

#데이터 탐색

##Histogram. Box Plot
"""

questions = []
with open(f"{data_dir}/korquad_train.json") as f:
    for i, line in enumerate(f):
        data = json.loads(line)
        questions.append(data["question"])
        if i < 10:
            print(data["question"])

train_question_counts = [len(question) for question in questions]
train_question_counts[:10]

"""token을 count한다."""

plt.figure(figsize=(8, 4))
plt.hist(train_question_counts, bins=100, range=[0, 100], facecolor='b', label='train')

plt.title('Count of question')
plt.xlabel('Number of question')
plt.ylabel('Count of question')
plt.show()

print(f"question 길이 최대:    {np.max(train_question_counts):4d}")
print(f"question 길이 최소:    {np.min(train_question_counts):4d}")
print(f"question 길이 평균:    {np.mean(train_question_counts):7.2f}")
print(f"question 길이 표준편차: {np.std(train_question_counts):7.2f}")

percentile25 = np.percentile(train_question_counts, 25)
percentile50 = np.percentile(train_question_counts, 50)
percentile75 = np.percentile(train_question_counts, 75)
percentileIQR = percentile75 - percentile25
percentileMAX = percentile75 + percentileIQR * 1.5

print(f"question 25/100분위:  {percentile25:7.2f}")
print(f"question 50/100분위:  {percentile50:7.2f}")
print(f"question 75/100분위:  {percentile75:7.2f}")
print(f"question IQR:        {percentileIQR:7.2f}")
print(f"question MAX/100분위: {percentileMAX:7.2f}")
print()

plt.figure(figsize=(4, 6))
plt.boxplot(train_question_counts, labels=['token counts'], showmeans=True)
plt.show()

contexts = []
with open(f"{data_dir}/korquad_train.json") as f:
    for i, line in enumerate(f):
        data = json.loads(line)
        contexts.append(data["context"])
        if i < 1:
            print(data["context"])

train_context_counts = [len(context) for context in contexts]
train_context_counts[:10]

"""token을 count한다."""

plt.figure(figsize=(8, 4))
plt.hist(train_context_counts, bins=900, range=[100, 1000], facecolor='r', label='train')
plt.title('Count of context')
plt.xlabel('Number of context')
plt.ylabel('Count of context')
plt.show()

print(f"context 길이 최대:    {np.max(train_context_counts):4d}")
print(f"context 길이 최소:    {np.min(train_context_counts):4d}")
print(f"context 길이 평균:    {np.mean(train_context_counts):7.2f}")
print(f"context 길이 표준편차: {np.std(train_context_counts):7.2f}")

percentile25 = np.percentile(train_context_counts, 25)
percentile50 = np.percentile(train_context_counts, 50)
percentile75 = np.percentile(train_context_counts, 75)
percentileIQR = percentile75 - percentile25
percentileMAX = percentile75 + percentileIQR * 1.5

print(f"context 25/100분위:  {percentile25:7.2f}")
print(f"context 50/100분위:  {percentile50:7.2f}")
print(f"context 75/100분위:  {percentile75:7.2f}")
print(f"context IQR:        {percentileIQR:7.2f}")
print(f"context MAX/100분위: {percentileMAX:7.2f}")
print()

plt.figure(figsize=(4, 6))
plt.boxplot(train_context_counts, labels=['token counts'], showmeans=True)
plt.show()

token_starts = []
with open(f"{data_dir}/korquad_train.json") as f:
    for i, line in enumerate(f):
        data = json.loads(line)
        token_starts.append(data["token_start"])
        if i < 10:
            print(data["token_start"], end='  ')

train_answer_starts = token_starts
train_answer_starts[:10]

"""token을 count한다."""

plt.figure(figsize=(8, 4))
plt.hist(train_answer_starts, bins=500, range=[0, 500], facecolor='g', label='train')
plt.title('Count of answer')
plt.xlabel('Number of answer')
plt.ylabel('Count of answer')
plt.show()

print(f"answer 위치 최대:    {np.max(train_answer_starts):4d}")
print(f"answer 위치 최소:    {np.min(train_answer_starts):4d}")
print(f"answer 위치 평균:    {np.mean(train_answer_starts):7.2f}")
print(f"answer 위치 표준편차: {np.std(train_answer_starts):7.2f}")

percentile25 = np.percentile(train_answer_starts, 25)
percentile50 = np.percentile(train_answer_starts, 50)
percentile75 = np.percentile(train_answer_starts, 75)
percentileIQR = percentile75 - percentile25
percentileMAX = percentile75 + percentileIQR * 1.5

print(f"answer 25/100분위:  {percentile25:7.2f}")
print(f"answer 50/100분위:  {percentile50:7.2f}")
print(f"answer 75/100분위:  {percentile75:7.2f}")
print(f"answer IQR:        {percentileIQR:7.2f}")
print(f"answer MAX/100분위: {percentileMAX:7.2f}")
print()

plt.figure(figsize=(4, 6))
plt.boxplot(train_answer_starts, labels=['token counts'], showmeans=True)
plt.show()

"""##Wordcloud"""

documents = []

for data in tqdm(train_json["data"]):
    title = data["title"]
    documents.append(title)
    for paragraph in data["paragraphs"]:
        context = paragraph["context"]
        documents.append(context)

        for qa in paragraph["qas"]:
            assert len(qa["answers"]) == 1
            question = qa["question"]
            documents.append(question)

"""train documents를 생성한다.
<br/>전체 데이터에서 title, context, question 문장을 모두 추출한다. 
<br/>
"""

" ".join(documents[:5])

"""title, context, question 문장에서 맨 앞 5개만 확인한다.
<br/>documents를 전부 이어 하나의 문장으로 만든다.
"""

!apt-get update -qq
!apt-get install fonts-nanum* -qq

font = '/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf'

wordcloud = WordCloud(width = 800, height = 800, font_path = font).generate(" ".join(documents))
plt.figure(figsize = (10, 10))
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.show()

"""WordCloud로 " ".join(documents)를 처리한다.
<br/>image 출력, interpolation 이미지 시각화 옵션을 설정한다.

#데이터 로더
"""

train_json = os.path.join(data_dir, "korquad_train.json")
dev_json = os.path.join(data_dir, "korquad_dev.json")

class Config(dict):
  
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__

args = Config({
    'max_seq_length': 384,
    'max_query_length': 64,
})
args

"""json을 config 형태로 사용하기 위한 Class이다.
<br/>param dict은 dictionary를 config한다.

"""

def load_data(args, filename):
    inputs, segments, labels_start, labels_end = [], [], [], []

    n_discard = 0
    with open(filename, "r") as f:
        for i, line in enumerate(tqdm(f, desc=f"Loading ...")):
            data = json.loads(line)
            token_start = data.get("token_start")
            token_end = data.get("token_end")
            question = data["question"][:args.max_query_length]
            context = data["context"]
            answer_tokens = " ".join(context[token_start:token_end + 1])
            context_len = args.max_seq_length - len(question) - 3

            if token_end >= context_len:
                n_discard += 1
                continue
            context = context[:context_len]
            assert len(question) + len(context) <= args.max_seq_length - 3

            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']
            ids = [vocab.piece_to_id(token) for token in tokens]
            ids += [0] * (args.max_seq_length - len(ids))
            inputs.append(ids)
            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)
            segs += [0] * (args.max_seq_length - len(segs))
            segments.append(segs)
            token_start += (len(question) + 2)
            labels_start.append(token_start)
            token_end += (len(question) + 2)
            labels_end.append(token_end)
    print(f'n_discard: {n_discard}')

    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))

"""생성한 데이터셋 파일을 메모리에 로드하는 함수이다.
<br/>최대 길이내에 token이 들어가지 않은 경우 처리하지 않는다.
"""

train_inputs, train_labels = load_data(args, train_json)
print(f"train_inputs: {train_inputs[0].shape}")
print(f"train_inputs: {train_inputs[1].shape}")
print(f"train_labels: {train_labels[0].shape}")
print(f"train_labels: {train_labels[1].shape}")

"""훈련 데이터를 로드한다."""

dev_inputs, dev_labels = load_data(args, dev_json)
print(f"dev_inputs: {dev_inputs[0].shape}")
print(f"dev_inputs: {dev_inputs[1].shape}")
print(f"dev_labels: {dev_labels[0].shape}")
print(f"dev_labels: {dev_labels[1].shape}")

"""평가 데이터를 로드한다."""

train_inputs[:1],

train_labels[:1]

len(train_inputs[0][0])

train_inputs[0][0][:20]

"""Question과 Context가 포함된 입력데이터 1번째이다."""

len(train_inputs[1][0])

train_inputs[1][0][0:20]

"""Question을 0으로, Context를 1로 구분해 준 Segment 데이터 1번째이다."""

train_labels[0][0], train_labels[1][0]

"""Answer위치의 시작점과 끝점 라벨 1번째이다.

#모델 구성

##Position Embedding

포지션 임베딩(Position Embedding)은 학습을 통해서 단어의 위치 정보를 얻는다.

###Masked
"""

def get_pad_mask(tokens, i_pad=0):
    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)
    mask = tf.expand_dims(mask, axis=1)
    return mask

def get_ahead_mask(tokens, i_pad=0):
    n_seq = tf.shape(tokens)[1]
    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)
    ahead_mask = tf.expand_dims(ahead_mask, axis=0)
    pad_mask = get_pad_mask(tokens, i_pad)
    mask = tf.maximum(ahead_mask, pad_mask)
    return mask

"""마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)한다.
<br/>여기서 마스킹이란 원래의 단어가 무엇이었는지 모르게 한다는 뜻으로
<br/>문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 빈칸 채우기 문제로 비유할 수 있다.
"""

@tf.function(experimental_relax_shapes=True)
def gelu(x):
    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))

def kernel_initializer(stddev=0.02):
    return tf.keras.initializers.TruncatedNormal(stddev=stddev)

def bias_initializer():
    return tf.zeros_initializer

"""gelu activation 함수를 정의한다.
<br/>parameter initializer를 생성한다.
<br/>bias initializer를 생성한다.
"""

class Config(dict):
 
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__

    @classmethod
    def load(cls, file):
        with open(file, 'r') as f:
            config = json.loads(f.read())
            return Config(config)

"""json을 config 형태로 사용하기 위한 Class이다.

###Shared Embedding
"""

class SharedEmbedding(tf.keras.layers.Layer):
  
    def __init__(self, config, name="weight_shared_embedding"):
        super().__init__(name=name)
        self.n_vocab = config.n_vocab
        self.d_model = config.d_model
    

    def build(self, input_shape):
        with tf.name_scope("shared_embedding_weight"):
            self.shared_weights = self.add_weight(
                "weights",
                shape=[self.n_vocab, self.d_model],
                initializer=kernel_initializer()
            )


    def call(self, inputs, mode="embedding"):

        if mode == "embedding":
            return self._embedding(inputs)
  
        elif mode == "linear":
            return self._linear(inputs)
       
        else:
            raise ValueError(f"mode {mode} is not valid.")
    

    def _embedding(self, inputs):
        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))
        return embed


    def _linear(self, inputs):  
        n_batch = tf.shape(inputs)[0]
        n_seq = tf.shape(inputs)[1]
        inputs = tf.reshape(inputs, [-1, self.d_model])  
        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)
        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab]) 
        return outputs

"""mode == "embedding" 일 경우 Token Embedding Layer 로 사용되는 layer 클래스이다. """

class PositionalEmbedding(tf.keras.layers.Layer):

    def __init__(self, config, name="position_embedding"):
        super().__init__(name=name)
        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model,
                                                   embeddings_initializer=kernel_initializer())


    def call(self, inputs):
        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)
        embed = self.embedding(position)
        return embed

class ScaleDotProductAttention(tf.keras.layers.Layer):

    def __init__(self, name="scale_dot_product_attention"):
        super().__init__(name=name)

    def call(self, Q, K, V, attn_mask):
        attn_score = tf.matmul(Q, K, transpose_b=True)
        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))
        attn_scale = tf.math.divide(attn_score, scale)
        attn_scale -= 1.e9 * attn_mask
        attn_prob = tf.nn.softmax(attn_scale, axis=-1)
        attn_out = tf.matmul(attn_prob, V)
        return attn_out

"""###MultiHead Attention"""

class MultiHeadAttention(tf.keras.layers.Layer):

    def __init__(self, config, name="multi_head_attention"):

        super().__init__(name=name)

        self.d_model = config.d_model
        self.n_head = config.n_head
        self.d_head = config.d_head

        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head,
                                         kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        
        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head,
                                         kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        
        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head,
                                         kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        
        self.attention = ScaleDotProductAttention(name="self_attention")
        
        self.W_O = tf.keras.layers.Dense(config.d_model,
                                         kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())


    def call(self, Q, K, V, attn_mask):
        
        batch_size = tf.shape(Q)[0]
        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  
        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  
        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  
        attn_mask_m = tf.expand_dims(attn_mask, axis=1)
        
        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  
        
        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  
        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  
        attn_out = self.W_O(attn_out) 

        return attn_out

"""###Position Wise Feed Forward"""

class PositionWiseFeedForward(tf.keras.layers.Layer):

    def __init__(self, config, name="feed_forward"):

        super().__init__(name=name)

        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu,
                                         kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        
        self.W_2 = tf.keras.layers.Dense(config.d_model,
                                         kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())


    def call(self, inputs):
        ff_val = self.W_2(self.W_1(inputs))
        return ff_val

"""###Encoder Layer"""

class EncoderLayer(tf.keras.layers.Layer):

    def __init__(self, config, name="encoder_layer"):

        super().__init__(name=name)

        self.self_attention = MultiHeadAttention(config)
        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)

        self.ffn = PositionWiseFeedForward(config)
        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)

        self.dropout = tf.keras.layers.Dropout(config.dropout)
 
 
    def call(self, enc_embed, self_mask):

        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)
        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))

        ffn_val = self.ffn(norm1_val)
        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))

        return enc_out

"""##BERT"""

class BERT(tf.keras.layers.Layer):

    def __init__(self, config, name="bert"):

        super().__init__(name=name)

        self.i_pad = config.i_pad
        self.embedding = SharedEmbedding(config)
        self.position = PositionalEmbedding(config)
        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())
        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)
        
        self.encoder_layers = [EncoderLayer(config, name=f"encoder_layer_{i}") for i in range(config.n_layer)]

        self.dropout = tf.keras.layers.Dropout(config.dropout)


    def call(self, enc_tokens, segments):
       
        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)

        enc_embed = self.get_embedding(enc_tokens, segments)

        enc_out = self.dropout(enc_embed)
        for encoder_layer in self.encoder_layers:
            enc_out = encoder_layer(enc_out, enc_self_mask)

        logits_cls = enc_out[:,0]
        logits_lm = enc_out
        return logits_cls, logits_lm
    

    def get_embedding(self, tokens, segments):
        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)
        embed = self.norm(embed)
        return embed

"""BERT는 총 3개의 임베딩 층이 사용된다.
<br/>WordPiece Embedding Layer는 실질적인 입력이 되는 워드 임베딩을 한다.
<br/>Position Embedding Layer는 위치 정보를 학습하기 위한 임베딩을 한다.
<br/>Segment Embedding Layer는 두 개의 문장을 구분하기 위한 임베딩을 한다.

#Non Pretrained Model

##모델 학습
"""

class BERT4KorQuAD(tf.keras.Model):
    def __init__(self, config):
        super().__init__(name='BERT4KorQuAD')

        self.bert = BERT(config)
        self.dense = tf.keras.layers.Dense(2)
    
    def call(self, enc_tokens, segments):
        logits_cls, logits_lm = self.bert(enc_tokens, segments)

        hidden = self.dense(logits_lm) 
        start_logits, end_logits = tf.split(hidden, 2, axis=-1) 

        start_logits = tf.squeeze(start_logits, axis=-1)
        start_outputs = tf.keras.layers.Softmax(name="start")(start_logits)

        end_logits = tf.squeeze(end_logits, axis=-1)
        end_outputs = tf.keras.layers.Softmax(name="end")(end_logits)

        return start_outputs, end_outputs

config = Config({"d_model": 512, "n_head": 8, "d_head": 64, "dropout": 0.1, "d_ff": 1024,
                 "layernorm_epsilon": 0.001, "n_layer": 6, "n_seq": 384, "n_vocab": 0, "i_pad": 0})
config.n_vocab = len(vocab)
config.i_pad = vocab.pad_id()
config

bert_batch_size = 32 

train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)
dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)

model = BERT4KorQuAD(config)

def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):
    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')
    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')
    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')
    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')

    p_bar = tqdm(dataset)
    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):
        with tf.GradientTape() as tape:
            start_outputs, end_outputs = model(enc_tokens, segments)

            start_loss = loss_fn(start_labels, start_outputs)
            end_loss = loss_fn(end_labels, end_outputs)
            loss = start_loss + end_loss

            start_acc = acc_fn(start_labels, start_outputs)
            end_acc = acc_fn(end_labels, end_outputs)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        metric_start_loss(start_loss)
        metric_end_loss(end_loss)
        metric_start_acc(start_acc)
        metric_end_acc(end_acc)
        if batch % 10 == 9:
            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, \
                                    acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')

    p_bar.close()

    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()

def eval_epoch(model, dataset, loss_fn, acc_fn):
    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')
    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')
    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')
    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')

    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):
        start_outputs, end_outputs = model(enc_tokens, segments)

        start_loss = loss_fn(start_labels, start_outputs)
        end_loss = loss_fn(end_labels, end_outputs)

        start_acc = acc_fn(start_labels, start_outputs)
        end_acc = acc_fn(end_labels, end_outputs)

        metric_start_loss(start_loss)
        metric_end_loss(end_loss)
        metric_start_acc(start_acc)
        metric_end_acc(end_acc)

    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()

loss_fn = tf.keras.losses.sparse_categorical_crossentropy
acc_fn = tf.keras.metrics.sparse_categorical_accuracy

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4) 

best_acc = .0
patience = 0


history = {'train_start_loss': [], 
           'train_end_loss': [], 
           'train_start_acc': [], 
           'train_end_acc': [],
           'val_start_loss': [], 
           'val_end_loss': [],
           'val_start_acc': [], 
           'val_end_acc': []}


for epoch in range(3):
  
    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, train_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    
    history['train_start_loss'].append(start_loss)
    history['train_end_loss'].append(end_loss)
    history['train_start_acc'].append(start_acc)
    history['train_end_acc'].append(end_acc)
    
    train_epoch(model, dev_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    
    history['val_start_loss'].append(start_loss)
    history['val_end_loss'].append(end_loss)
    history['val_start_acc'].append(start_acc)
    history['val_end_acc'].append(end_acc)

    acc = start_acc + end_acc
    if best_acc < acc:
        patience = 0
        best_acc = acc
        model.save_weights(os.path.join(data_dir, "korquad_bert_none_pretrain.hdf5"))
        print(f'save best model')
    else:
        patience += 1
    if 2 <= patience :
        print(f'early stopping')
        break

"""learning rate 5e-4에서 3e-4로 낮추고 Optimzer로 Adam을 사용한다.
<br/>훈련 데이터와 검증 데이터에 대한 accuracy와 loss를 구한다.

##인퍼런스

인퍼런스(추론)은 학습을 마친 모델로 실제 과제를 수행하는 행위 혹은 그 과정이다.
<br/>학습이 완료된 model을 활용하여 실제 퀴즈 풀이 결과를 확인한다.
"""

def do_predict(model, question, context):

    q_tokens = vocab.encode_as_pieces(question)[:args.max_query_length]
    c_tokens = vocab.encode_as_pieces(context)[:args.max_seq_length - len(q_tokens) - 3]
    tokens = ['[CLS]'] + q_tokens + ['[SEP]'] + c_tokens + ['[SEP]']
    token_ids = [vocab.piece_to_id(token) for token in tokens]
    segments = [0] * (len(q_tokens) + 2) + [1] * (len(c_tokens) + 1)

    y_start, y_end = model(np.array([token_ids]), np.array([segments]))

    y_start_idx = K.argmax(y_start, axis=-1)[0].numpy()
    y_end_idx = K.argmax(y_end, axis=-1)[0].numpy()
    answer_tokens = tokens[y_start_idx:y_end_idx + 1]

    return vocab.decode_pieces(answer_tokens)

dev_json = os.path.join(data_dir, "korquad_dev.json")

with open(dev_json) as f:
    for i, line in enumerate(f):
        data = json.loads(line)
        question = vocab.decode_pieces(data['question'])
        context = vocab.decode_pieces(data['context'])
        answer = data['answer']
        answer_predict = do_predict(model, question, context)
        if answer in answer_predict:
          if 570 <= i <= 600:
            print(i)
            print("질문 : ", question)
            print("지문 : ", context)
            print("정답 : ", answer)
            print("예측 : ", answer_predict, "\n")
        if i > 600:
            break

"""입력에 대한 답변을 생성하는 함수를 정의한다.
<br/>question, context를 입력하여 answer를 생성한다.

#Pretrained Model

##모델 학습

Pretrained Model Finetune은 BERT에 태스크의 데이터를 추가로 학습시켜서 테스트하는 단계이다.
"""

model = BERT4KorQuAD(config)

checkpoint_file = os.path.join(model_dir, 'bert_pretrain_32000.hdf5')

if os.path.exists(checkpoint_file):
    enc_tokens = np.random.randint(0, len(vocab), (4, 10))
    segments = np.random.randint(0, 2, (4, 10))
    model(enc_tokens, segments)
    
    model.load_weights(os.path.join(model_dir, "bert_pretrain_32000.hdf5"), by_name=True)

    model.summary()
else:
    print('No Pretrained Model')

"""checkpoint 파일로부터 필요한 layer를 불러온다. """

loss_fn = tf.keras.losses.sparse_categorical_crossentropy
acc_fn = tf.keras.metrics.sparse_categorical_accuracy

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4) 

best_acc = .0
patience = 0


history = {'pre_train_start_loss': [], 
           'pre_train_end_loss': [], 
           'pre_train_start_acc': [], 
           'pre_train_end_acc': [],
           'pre_val_start_loss': [], 
           'pre_val_end_loss': [],
           'pre_val_start_acc': [], 
           'pre_val_end_acc': []}


for epoch in range(3):
  
    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, train_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    
    history['pre_train_start_loss'].append(start_loss)
    history['pre_train_end_loss'].append(end_loss)
    history['pre_train_start_acc'].append(start_acc)
    history['pre_train_end_acc'].append(end_acc)
    
    train_epoch(model, dev_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    
    history['pre_val_start_loss'].append(start_loss)
    history['pre_val_end_loss'].append(end_loss)
    history['pre_val_start_acc'].append(start_acc)
    history['pre_val_end_acc'].append(end_acc)

    acc = start_acc + end_acc
    if best_acc < acc:
        patience = 0
        best_acc = acc
        model.save_weights(os.path.join(data_dir, "korquad_bert_new_pretrain.hdf5"))
        print(f'save best model')
    else:
        patience += 1
    if 2 <= patience :
        print(f'early stopping')
        break

"""##인퍼런스"""

dev_json = os.path.join(data_dir, "korquad_dev.json")

with open(dev_json) as f:
    for i, line in enumerate(f):
        data = json.loads(line)
        question = vocab.decode_pieces(data['question'])
        context = vocab.decode_pieces(data['context'])
        answer = data['answer']
        answer_predict = do_predict(model, question, context)
        if answer in answer_predict:
          if 610 < i <= 640 :
            print(i)
            print("질문 : ", question)
            print("지문 : ", context)
            print("정답 : ", answer)
            print("예측 : ", answer_predict, "\n")
        if i > 640:
            break

"""#모델 평가

pretrained model 사용 여부에 따라 학습 수행 경과가 어떻게 달라지는지를
<br/>시각화를 포함하여 비교 분석을 진행한다.
"""

plt.figure(figsize=(16, 10))

plt.subplot(2, 2, 1)
plt.plot(history['train_start_loss'], 'g-', label='train_start_loss')
plt.plot(history['val_start_loss'], 'y--', label='val_start_loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('start-loss')

plt.subplot(2, 2, 2)
plt.plot(history['train_end_loss'], 'g-', label='train_end_loss')
plt.plot(history['val_end_loss'], 'y--', label='val_end_loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('end-loss')

plt.subplot(2, 2, 3)
plt.plot(history['train_start_acc'], 'g-', label='train_start_acc')
plt.plot(history['val_start_acc'], 'y--', label='val_start_acc')
plt.xlabel('Epoch')
plt.legend()
plt.title('start-accuracy')


plt.subplot(2, 2, 4)
plt.plot(history['train_end_acc'], 'g-', label='train_end_acc')
plt.plot(history['val_end_acc'], 'y--', label='val_end_acc')
plt.xlabel('Epoch')
plt.legend()
plt.title('end-accuracy')

plt.figure(figsize=(16, 10))

plt.subplot(2, 2, 1)
plt.plot(history['pre_train_start_loss'], 'g-', label='pre_train_start_loss')
plt.plot(history['pre_val_start_loss'], 'y--', label='pre_val_start_loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('pretrained_model_start-loss')

plt.subplot(2, 2, 2)
plt.plot(history['pre_train_end_loss'], 'g-', label='pre_train_end_loss')
plt.plot(history['pre_val_end_loss'], 'y--', label='pre_val_end_loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('pretrained_model_end-loss')

plt.subplot(2, 2, 3)
plt.plot(history['pre_train_start_acc'], 'g-', label='pre_train_start_acc')
plt.plot(history['pre_val_start_acc'], 'y--', label='pre_val_start_acc')
plt.xlabel('Epoch')
plt.legend()
plt.title('pretrained_model_start-accuracy')

plt.subplot(2, 2, 4)
plt.plot(history['pre_train_end_acc'], 'g-', label='pre_train_end_acc')
plt.plot(history['pre_val_end_acc'], 'y--', label='pre_val_end_acc')
plt.xlabel('Epoch')
plt.legend()
plt.title('pretrained_model_end-accuracy')

plt.show()

"""#결론

앞으로 LP 모델을 공부하면서 코드에 대한 설명을 추가할 예정이다.
<br/>밑바닥부터 시작한 딥러닝 2권 1회독을 하고 1차 수정을 한다.
<br/>구글 BERT의 정석 1회독을 하고 2차 수정을 한다.
<br/>BERT : Pre-training of deep bidirectional transformers for language understanding
<br/>논문을 정독하고 3차 수정을 한다.
<br/>그리고 KorQuAD 모델의 validation accuracy가 안정적으로 증가하고
<br/> 원래의 정답과 유사한 인퍼런스 결과가 출력되게 한다.

#참고문헌

**LMS**
<br/>[cchyun](https://github.com/paul-hyun)

<br/>**단행본**
<br/>유원준 외 1명, 『딥러닝을 이용한 자연어 처리 입문』, Wikidocs, 2022
<br/>[NLP에서의 사전 훈련(Pre-training)](https://wikidocs.net/108730)
<br/>[버트(Bidirectional Encoder Representations from Transformers, BERT)](https://wikidocs.net/115055)

<br/>**공식 사이트**
<br/>[KorQuAD 1.0](https://korquad.github.io/KorQuad%201.0/)

<br/>**Github**
<br/>PEBpung
<br/>[E26.KorQuAD Task using BERT.ipynb](https://github.com/PEBpung/Aiffel/blob/master/Project/Exploration/E26.KorQuAD%20Task%20using%20BERT.ipynb)
<br/><br/>miinkang
<br/>[(E-19)KorQuAD_BERT.ipynb](https://github.com/miinkang/AI_Project_AIFFEL/blob/main/%5BE-19%5DKorQuAD_BERT.ipynb)

<br/>**웹사이트**
<br/>[인공지능 퀴즈 풀기|KorQuAD_BERT](https://zzcojoa.tistory.com/91)
<br/>[학습 마친 모델을 실전 투입하기](https://ratsgo.github.io/nlpbook/docs/ner/inference/)
"""