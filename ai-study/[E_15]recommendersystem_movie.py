# -*- coding: utf-8 -*-
"""[E_15]RecommenderSystem_Movie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eZ81TD3e8kyYMT5WiRVmbvocJYxe5Qb8

##스타워즈 팬이 좋아할 만한 다른 영화 찾기

개발 환경
<br/>데이터 정보

데이터 전처리
<br/>암묵적 피드백
<br/>고유값
<br/>결측치 처리

데이터 탐색
<br/>영화 랭킹
<br/>영화 리뷰

모델 구성
<br/>CSR Matrix
<br/>Hyperparameter

모델 학습
<br/>모델 평가
<br/>Similar Items
<br/>Recommendation
<br/>결론
<br/>참고문헌

# 개발 환경
"""

!pip install implicit

import os
import random
import pandas as pd
import numpy as np

import scipy
from scipy.sparse import csr_matrix

import implicit
from implicit.als import AlternatingLeastSquares

os.environ['OPENBLAS_NUM_THREADS']='1'
os.environ['KMP_DUPLICATE_LIB_OK']='True'
os.environ['MKL_NUM_THREADS']='1'

from google.colab import drive
drive.mount('/content/drive')

pip freeze > '/content/drive/MyDrive/lms/library_version.txt'

library_name = ['pandas=', 'numpy=', 'scipy=', 'implicit=']
library_version = []
count = 0

import sys
print(sys.version)
print()

with open('/content/drive/MyDrive/lms/library_version.txt', 'r') as f:
    lines = f.read().splitlines() 

for i in range(len(lines)):
  for line in lines[i:i+1]:
    for library in library_name:
      if library in line:
        library_version.append(line)
        count += 1
        print(line, end = '    ')
        if count % 3 == 0:
          print()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Google Colab에서 할당된 GPU를 확인한다.
<br/>고용량 메모리 VM에 액세스한다.

#데이터 정보

[MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/)

GroupLens Research가 [MovieLens](https://movielens.org)에서  영화 3,952편에 대한 6,040명의 사용자의 평점 1,000,209개를 수집한 데이터셋이다.
<br/>논문 [「The MovieLens Datasets: History and Contex」](https://grouplens.org/blog/movielens-datasets-context-and-history/)에서는 MovieLens 데이터셋의 역사에 대해서 이야기한다.
"""

rating_file_path = '/content/drive/MyDrive/lms/recommender_system/ml-1m/ratings.dat'
ratings_cols = ['user_id', 'movie_id', 'ratings', 'timestamp']
ratings = pd.read_csv(rating_file_path, sep='::', names=ratings_cols, engine='python', encoding = "ISO-8859-1")
orginal_data_size = len(ratings)
ratings.head()

readme_path = '/content/drive/MyDrive/lms/recommender_system/ml-1m/README'
f = open(readme_path, 'r')
line_num = 0
while True:
    line = f.readline()
    line_num += 1
    if 76 <= line_num <= 77 or 84 <= line_num <= 87:
      print(line)
    elif not line: break

f.close()

movie_file_path = '/content/drive/MyDrive/lms/recommender_system/ml-1m/movies.dat'
cols = ['movie_id', 'title', 'genre'] 
movies = pd.read_csv(movie_file_path, sep='::', names=cols, engine='python', encoding='ISO-8859-1')
movies.head()

readme_path = '/content/drive/MyDrive/lms/recommender_system/ml-1m/README'
f = open(readme_path, 'r')
line_num = 0
while True:
    line = f.readline()
    line_num += 1
    if 137 <= line_num <= 138 or 168 <= line_num <= 170:
      print(line)
    elif not line: break

f.close()

"""#데이터 전처리

##암묵적 피드백
"""

ratings = ratings[ratings['ratings']>=3]
filtered_data_size = len(ratings)

print(f'orginal_data_size: {orginal_data_size}, filtered_data_size: {filtered_data_size}')
print(f'Ratio of Remaining Data is {filtered_data_size / orginal_data_size:.2%}')

"""현재 가지고 있는 데이터는 사용자의 영화 평점 정보이다.
<br/>이렇게 서비스를 사용하면서 자연스럽게 발생하는 암묵적(implicit)인 피드백도
<br/>사용자의 아이템에 대한 평가를 알 수 있는 단서가 될 수 있다.

앞으로 만들어갈 모델에서는 암묵적 데이터의 해석을 위해 다음과 같은 규칙을 적용한다.
<br/>3점이 넘으면 선호한다고 판단한다.
<br/>평점을 높게 준 영화에 대해 가중치를 주어서 더 확실히 좋아한다고 판단한다.

##고유값
"""

num_user = ratings['user_id'].nunique()
num_movie = ratings['movie_id'].nunique()

num_user

num_movie

"""##결측치 처리"""

user = ratings['user_id'].unique()
movie = ratings['movie_id'].unique()

user_to_idx = {v:k  for k,v in enumerate(user)}
title_to_idx = {v:k for k,v in enumerate(movie)}

for key, value in user_to_idx.items():
    if 3595 <= key <= 3600: 
      print(key, value)

condition = (ratings['user_id'] == 3598)
ratings[condition]

"""데이터프레임의 빈 행이 존재한다."""

ratings.isnull().sum()

"""그러나 결측치가 없다고 하는 이상한 결과가 나온다.

#데이터 탐색

##영화 랭킹
"""

movies_ratings = pd.merge(movies, ratings)
cols = ['user_id','title', 'ratings']
movies_ratings = movies_ratings[cols]

movies_ratings

movies_ratings['title'] = movies_ratings['title'].str.lower() 
movies_count = movies_ratings.groupby('title')['user_id'].count()
popluar_thirty_movies = movies_count.sort_values(ascending=False)[:30]

popluar_thirty_movies = pd.DataFrame(popluar_thirty_movies.reset_index())
popluar_thirty_movies.index = popluar_thirty_movies.index + 1
popluar_thirty_movies = popluar_thirty_movies.reset_index()

popluar_thirty_movies.columns = ['rank', 'title', 'counts']
popluar_thirty_movies = popluar_thirty_movies[['title', 'rank', 'counts']]

popluar_thirty_movies

"""##영화 리뷰"""

favorite_movies_search_keyword = ['star wars: episode v', 'space odyssey', 'titanic', 'lion king', 'pulp fiction']
keyword = '|'.join(favorite_movies_search_keyword)
favorite_movies_ratings = movies_ratings[movies_ratings['title'].str.contains(keyword)]

favorite_movies_count = favorite_movies_ratings.groupby('title')['user_id'].count()
favorite_movies_search = favorite_movies_count.sort_values(ascending=False)
favorite_movies_search = pd.DataFrame(favorite_movies_search.reset_index())
favorite_movies_search.columns = ['title', 'counts']

favorite_movies_search

"""좋아하는 영화가 데이터셋에 존재하는지 검색한다."""

favorite_title_list = ['star wars: episode v - the empire strikes back (1980)', 'pulp fiction (1994)',
                       '2001: a space odyssey (1968)', 'titanic (1997)', 'lion king, the (1994)']

favorite_ratings_list = []
ratings_list = [4, 5]

for i in range(5):
  if i == 2:
    out = random.sample(ratings_list, i)
    favorite_ratings_list.append(out)
  elif i >= 3:
    out = random.sample(ratings_list, i-2)
    favorite_ratings_list.append(out)

favorite_ratings_list = sum(favorite_ratings_list, [])

favorite_ratings_list

"""좋아하는 영화에 랜덤으로 평점을 부여한다."""

favorite_movies = pd.DataFrame({'user_id': ['6041']*5,
                            'title': favorite_title_list,
                            'ratings':favorite_ratings_list})

if not movies_ratings.isin({'user_id':['6041']})['user_id'].any():  
    movies_ratings_update = movies_ratings.append(favorite_movies) 

movies_ratings_update = movies_ratings_update.astype({'user_id':'int'})
movies_ratings_update = movies_ratings_update.sort_values(by='user_id')
movies_ratings_update = movies_ratings_update.reset_index()
movies_ratings_update = movies_ratings_update[['user_id', 'title', 'ratings']]

movies_ratings_update

"""좋아하는 영화 5편을 데이터셋에 추가한다.
<br/>새로운 사용자의 데이터가 추가됐으므로 user_id는 6041이다.

#모델 구성

##CSR Matrix

Matrix Factorization(MF)

협업 필터링(Collaborative Filtering)은 평가 행렬을 전제로 한다.

m명의 사용자가 n편의 영화에 대해
<br/>평가한 데이터를 포함한 (m,n) 사이즈의 평가 행렬(Rating Matrix)을 만든다.

MF 모델은 Rating Matrix R을 두 개의 Feature Matrix P와 Q로 분해한다.

벡터를 만드는 기준은 사용자 i의 벡터(U_i)와 아이템 j의 벡터(I_j)를 내적했을 때 <br/>사용자 i가 아이템 j에 대해 평가한 수치(M_ij)와 비슷한지 아닌지이다.

CSR(Compressed Sparse Row) Matrix

사용자는 6,040명이고 영화는 3,559편이다.
<br/>이를 행렬로 표현하고 행렬의 각 원소에 정수 한 개 (1byte)가 들어간다면
<br/>6040 * 3559 * 1byte ≈ 21.50MB가 필요하다.

모델 학습의 input으로 사용할 데이터 타입을 CSR Matrix로 한다.
<br/>CSR Matrix는 Sparse한 matrix에서 0이 아닌 유효한 데이터로 채워지는 
<br/>데이터의 값과 좌표 정보만으로 구성하여 메모리 사용량을 최소화하면서도
<br/>Sparse한 matrix와 동일한 행렬을 표현할 수 있도록 하는 데이터 구조이다.
<br/>CSR Matrix는 data, indices, indptr 로 행렬을 압축하여 표현한다.
"""

movies_ratings_preprocess = movies_ratings_update.copy()

user_unique = movies_ratings_preprocess['user_id'].unique()
title_unique = movies_ratings_preprocess['title'].unique()

user_to_idx = {v:k for k,v in enumerate(user_unique)}
title_to_idx = {v:k for k,v in enumerate(title_unique)}

temp_user_data = movies_ratings_preprocess['user_id'].map(user_to_idx.get).dropna()

if len(temp_user_data) == len(movies_ratings_preprocess):   
    movies_ratings_preprocess['user_id'] = temp_user_data

temp_title_data = movies_ratings_preprocess['title'].map(title_to_idx.get)

if len(temp_title_data) == len(movies_ratings_preprocess):
    movies_ratings_preprocess['title'] = temp_title_data

movies_ratings_preprocess

num_user = movies_ratings_preprocess['user_id'].nunique()
num_movie = movies_ratings_preprocess['title'].nunique()

csr_data = csr_matrix((movies_ratings_preprocess.ratings, (movies_ratings_preprocess.user_id, movies_ratings_preprocess.title)), shape= (num_user, num_movie))

csr_data

"""##Hyperparameter"""

als_model = AlternatingLeastSquares(factors=100,
                                    regularization=0.01,
                                    use_gpu=False,
                                    iterations=15,
                                    dtype=np.float32)

"""Alternating Least Squares 모델을 구성한다.

factors : 유저와 아이템의 벡터의 차원
<br/>regularization : 과적합을 방지하기 위한 정규화 값
<br/>use_gpu : GPU 사용 여부
<br/>iterations(epochs) :  학습 횟수

#모델 학습
"""

als_model.fit(csr_data)

"""#모델 평가"""

new_user = list(user_to_idx.items())[-1][0]
new_user_vector = als_model.user_factors[-1]

favorite_title_list = ['star wars: episode v - the empire strikes back (1980)', 'pulp fiction (1994)',
                       '2001: a space odyssey (1968)', 'titanic (1997)', 'lion king, the (1994)']

favorite_vector_list = []

for i in favorite_title_list:
  favorite_movie_idx = title_to_idx[i]
  favorite_movie_vector = als_model.item_factors[favorite_movie_idx]
  favorite_vector_list.append(favorite_movie_vector)

favorite_preference = pd.DataFrame(index=range(0, 1), columns = {'0'})

for i in range(len(favorite_vector_list)):
  favorite_preference_list = []
  favorite_preference_list.append(favorite_title_list[i])
  favorite_preference_list.append(np.dot(new_user_vector, favorite_vector_list[i]))
  favorite_preference_df = pd.DataFrame(favorite_preference_list).transpose()
  favorite_preference =  favorite_preference.append(favorite_preference_df)

favorite_movie_preference = favorite_preference.iloc[1:, :-1]
favorite_movie_preference.columns = ['title', 'preference']

favorite_movie_preference

"""item_factors 메서드에서 좋아하는 영화에 대한 선호도를 예측한다.

#Similar Items
"""

title_unique = movies_ratings_update['title'].unique()
title_to_idx = {v:k for k,v in enumerate(title_unique)}
idx_to_title = {v:k for k,v in title_to_idx.items()}

def get_similar_title(title_name: str):
    title_id = title_to_idx[title_name]
    similar_title = als_model.similar_items(title_id)
    similar_title = [i for i in similar_title]
    
    a = []
    
    for i, j in enumerate(similar_title):
      a.append(j[1:6])

    favorite_similar = pd.DataFrame(index=range(0, 1), columns = {'0'})

    for i in range(2):
      favorite_similar_list = []
      favorite_similar_list.append(title_unique[a[0]])
      favorite_similar_list.append(a[1])
      favorite_similar_df = pd.DataFrame(favorite_similar_list).transpose()
      favorite_similar =  favorite_similar.append(favorite_similar_df)
      favorite_movie_similar = favorite_similar.iloc[1:6, :-1]
      favorite_movie_similar.columns = ['title', 'similarity']

    return favorite_movie_similar

"""similar_items 메서드에서 좋아하는 영화와 비슷한 영화를 찾는다."""

get_similar_title('star wars: episode v - the empire strikes back (1980)')

"""스타워즈와 비슷한 영화 5편을 출력한다."""

favorite_title_list = ['star wars: episode v - the empire strikes back (1980)', 'pulp fiction (1994)',
                       '2001: a space odyssey (1968)', 'titanic (1997)', 'lion king, the (1994)']

favorite_movie_similar_collection = pd.DataFrame(index=range(0, 1), columns = {'title'})

for i in favorite_title_list:
  favorite_movie_similar_collection = favorite_movie_similar_collection.append({'similarity':'', 'title': i},ignore_index=True)
  favorite_movie_similar_collection = favorite_movie_similar_collection.append(get_similar_title(i))

favorite_movie_similar_collection = favorite_movie_similar_collection.iloc[1:, :]
favorite_movie_similar_collection = favorite_movie_similar_collection.reset_index()
favorite_movie_similar_collection = favorite_movie_similar_collection [['title', 'similarity']]

def draw_color_cell(x,color):
    color = f'background-color:{color}'
    return color

color_column = []

for i in range(len(favorite_title_list)):
  color_column.append(6*i)

favorite_movie_similar_collection.style.applymap(draw_color_cell,color='#767575',subset=pd.IndexSlice[color_column,'title':'similarity'])

"""좋아하는 영화 리스트에 있는 각 작품과 비슷한 영화 5편씩 추천한다.

#Recommendation
"""

new_user = list(user_to_idx.items())[-3][0]

movies_recommended = als_model.recommend(new_user, csr_data, N=20, filter_already_liked_items=True)

"""als_model.recommend에서 ValueError: user_items must contain 1 row for every user in userids가 발생한다.
<br/>라이브러리 implicit 버전 업데이트가 되면서 발생한 에러로 보인다.
"""

movies_recommended = als_model.recommend(new_user, csr_data[new_user], N=20, filter_already_liked_items=True)

"""에러에 대한 해결책이 Github Conversation [#365](https://github.com/benfred/implicit/issues/365) [#389](https://github.com/benfred/implicit/pull/389)에서 제시된다.
<br/>csr_data[new_user]인 경우에는 에러가 발생하지 않는다.

"""

favorite_recommended = pd.DataFrame(index=range(0, 1), columns = {'0'})

for i in range(20):
  favorite_recommended_list = []
  favorite_recommended_list.append(idx_to_title[movies_recommended[0][i]])
  favorite_recommended_list.append(movies_recommended[1][i])
  favorite_recommended_df = pd.DataFrame(favorite_recommended_list).transpose()
  favorite_recommended =  favorite_recommended.append(favorite_recommended_df)

favorite_movie_recommended = favorite_recommended.iloc[1:, :-1]
favorite_movie_recommended.columns = ['title', 'preference']

favorite_movie_recommended

"""recommend 메서드에서 영화를 추천한다."""

odyssey = title_to_idx['raiders of the lost ark (1981)']
explain = als_model.explain(new_user, csr_data, itemid=odyssey)

b = []
    
for i, j in enumerate(explain):
  b.append(j)

favorite_explained = pd.DataFrame(index=range(0, 1), columns = {'0'})

for i in range(len(b[1])):
  favorite_explained_list = []
  favorite_explained2_list = []
  favorite_explained_list.append(title_unique[b[1][i][0]])
  favorite_explained_list.append(b[1][i][1])
  favorite_explained_df = pd.DataFrame(favorite_explained_list).transpose()
  favorite_explained =  favorite_explained.append(favorite_explained_df)

favorite_movie_explained = favorite_explained.iloc[1:, :-1]
favorite_movie_explained.columns = ['title', 'proportion']

favorite_movie_explained

"""explain 메서드에서 영화 raiders of the lost ark (1981) 추천 점수에 기여한 다른 영화 콘텐츠의 기여도를 반환한다.

#결론

**영화 추천 시스템**

Alternating Least Squares 모델의 item_factors 메서드에서 좋아하는 영화에 대한 선호도를 예측한다.
<br/>similar_items 메서드에서 좋아하는 영화와 비슷한 영화를 찾는다.
<br/>recommend 메서드에서 영화를 추천한다.
<br/>explain 메서드에서 영화 추천 점수에 기여한 다른 영화 콘텐츠의 기여도를 반환한다.

**결측치 처리**

ratings[ratings['user_id'] == 3598]에서 데이터프레임 빈 행이 존재하는데도
<br/>ratings.isnull().sum()하면 결측치가 없다는 이상한 결과가 나온다.
<br/>나중에 결측치를 주제로 공부하여 문제 원인을 분석하고 디버깅한다.

#참고문헌

**LMS**
<br/>[ziminpark](https://github.com/ZiminPark)

<br/>**공식 사이트**
<br/>GroupLens
<br/>[MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/)

<br/>**Github**
<br/>PEBpung
<br/>[E7.영화 추천 시스템.ipynb](https://github.com/PEBpung/Aiffel/blob/master/Project/Exploration/E7.%20%EC%98%81%ED%99%94%20%EC%B6%94%EC%B2%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C.ipynb)
<br/><br/>benfred
<br/>[filter_already_liked_items Working for ALS? #365](https://github.com/benfred/implicit/issues/365)
<br/>[Fix rank items in ItemItemRecommender #389](https://github.com/benfred/implicit/pull/389)

<br/>**웹사이트**
<br/>[python 여러 문자열 포함하는 모든 행 검색 - pandas](https://hyang2data.tistory.com/31)
<br/>[파이썬 2차원 리스트를 1차원으로 (flatten)](https://wyatt37.tistory.com/16)
<br/>[Python에서 목록을 Float로 변환](https://www.delftstack.com/ko/howto/python/convert-list-to-float-python/)
<br/>[python 파이썬, pandas 판다스 데이터 값 변경하기, 바꾸기](https://sunning-10.tistory.com/m/entry/python-%ED%8C%8C%EC%9D%B4%EC%8D%AC-pandas-%ED%8C%90%EB%8B%A4%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%92-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0-%EB%B0%94%EA%BE%B8%EA%B8%B0)
<br/>[Pandas 11. 데이터프레임 셀 스타일 변경하기](https://zephyrus1111.tistory.com/62)
<br/>[Day18 넷플릭스 추천 알고리즘을 만들어 볼까?](https://softwareeng.tistory.com/entry/Day18-%EB%84%B7%ED%94%8C%EB%A6%AD%EC%8A%A4-%EC%B6%94%EC%B2%9C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%84-%EB%A7%8C%EB%93%A4%EC%96%B4-%EB%B3%BC%EA%B9%8C)
"""