# -*- coding: utf-8 -*-
"""[E_06]TextGenerator_LyricsBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mR79nKW1bLNWQhvdLSdplmcBeEH-GzOP

##작사가 인공지능 만들기

개발 환경
<br/>데이터 정보
<br/>데이터 탐색
<br/>데이터 시각화

데이터셋의 한계
<br/>문장 토큰화
<br/>데이터셋 노이즈

데이터 전처리
<br/>데이터 분리
<br/>모델 구성
<br/>모델 학습
<br/>하이퍼파라미터
<br/>모델 평가
<br/>가사 생성
<br/>결론
<br/>참고문헌

#개발 환경
"""

import os
import re 
import glob
import matplotlib.pyplot as plt

"""os(Operating System)는 운영체제에서 제공되는 여러 기능을 파이썬에서 수행한다. <br/>예를 들어, 파일 복사, 디렉터리 생성, 파일 목록을 구할 수 있다.

re(regex)는 특정 문자 또는 문자열이 존재하는지나 어느 위치에 있는지와 같은 기능을 제공하는 정규표현식 라이브러리이다.

glob은 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다.
<br/>단, 조건에 정규식을 사용할 수 없으며 엑셀 등에서도 사용할 수 있는 '*'와 '?'같은 와일드카드만을 지원한다.

matplotlib은 다양한 데이터와 학습 모델을 시각화한다.
"""

import tensorflow as tf
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split

"""tensorflow는 구글이 개발한 오픈소스 소프트웨어 딥러닝 및 머신러닝 라이브러리이다.
<br/>수학 계산식과 데이터의 흐름을 노드와 엣지를 사용한 방향성 그래프, 데이터 플로우 그래프로 나타낸다.

numpy는 array 단위로 벡터와 행렬을 계산한다. 이 라이브러리를 사용하기 위해서는 선형대수학 지식이 필요하다.

sklearn.model_selection은 데이터를 훈련 데이터, 시험 데이터로 분리한다.
"""

from google.colab import drive
drive.mount('/content/drive')

pip freeze > '/content/drive/MyDrive/lms/library_version.txt'

library_name = ['pandas=', 'numpy=', 'matplotlib=', 'sklearn=', 'regex=']
library_version = []
count = 0

import sys
print(sys.version)
print()

with open('/content/drive/MyDrive/lms/library_version.txt', 'r') as f:
    lines = f.read().splitlines() 

for i in range(len(lines)):
  for line in lines[i:i+1]:
    for library in library_name:
      if library in line:
        library_version.append(line)
        count += 1
        print(line, end = '    ')
        if count % 3 == 0:
          print()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Google Colab에서 할당된 GPU를 확인한다.
<br/>고용량 메모리 VM에 액세스한다.

#데이터 정보

[song_lyrics](https://www.kaggle.com/datasets/paultimothymooney/poetry)

49명의 가수의 영어 노래 가사를 수집한 텍스트 파일 모음 데이터셋이다.
<br/>가사 생성기(lyrics Generator)를 만드는 데 사용된다.

#데이터 탐색
"""

txt_file_path = '/content/drive/MyDrive/lms/song_lyrics/*'

txt_list = glob.glob(txt_file_path)

raw_corpus = []

for txt_file in txt_list:
    with open(txt_file, "r") as f:
        raw = f.read().splitlines()
        raw_corpus.extend(raw)

print("데이터 크기:", len(raw_corpus))
print("Examples:\n", raw_corpus[:3])

"""이 데이터셋은 187088 문장으로 구성되어 있다.

주의사항
<br/>주소를 적을 때 txt_file_path = '.../*'의 끝에 별 *를 적어야 한다.
<br/>별을 빠뜨리면 IsADirectoryError: [Errno 21]라는 에러가 발생한다.

glob.glob은 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다. <br/>특정 파일 경로 안에 있는 파일명을 불러왔다.

splitlines는 줄 단위로 문자열을 리스트로 변환한다.
<br/>그런데 split("\n')도 splitlines()와 동일한 결과값을 보여주기 때문에
<br/>문자열을 리스트로 변경할 때는 대부분 split()을 사용한다.

extend는 확장 함수로 다른 리스트를 연결한다.
<br/>여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담는다.

extend와 append, insert(a,b)는 다른 형태의 추가 함수이니 때에 따라 다르게 쓴다.
<br/>append는 리스트의 끝에 x 값을 추가한다.
<br/>insert(a,b)는 리스트의 a 위치에 b 값을 추가한다.

#데이터 시각화

**matplotlib**을 이용해 문장 길이의 빈도 분포를 시각화한다.
"""

max_len = max(len(l) for l in raw_corpus)
print('최대 길이 : %d' % max_len)
print('최소 길이 : %d' % min(len(l) for l in raw_corpus if len(l) >= 1))
print('평균 길이 : %f' % (sum(map(len, raw_corpus))/len(raw_corpus)))
plt.hist([len(s) for s in raw_corpus if len(s) >= 1], bins=50)
plt.xlabel('length of sample')
plt.ylabel('number of sample')
plt.show()

"""1,000이 넘는 지나치게 긴 길이가 있는 문장이 존재한다.
<br/>왜 그럴까?

print를 이용해 직접 1,000이 넘는 문장을 출력해서 문제 원인을 확인한다.
"""

for i in raw_corpus:
  if len(i) >= 1000:
    print(i)

"""데이터셋 자체의 문제이다.
<br/>웹크롤링 당시 줄바꿈되지 않은 채 여러 문장이 하나의 문장으로 이어져 txt 파일에 담긴 것이다.

#데이터셋의 한계

##문장 토큰화

문장 토큰화를 하는 함수 nltk.tokenize의 sent_tokenize가 있으나
<br/>마침표를 기준으로 문장을 나누기 때문에 길이가 1,000이 넘는 문장을 구분할 수 없다.
여러 문장으로 구성된 이 글 안에는 마침표가 없는 문장이 있으며 마침표가 있는 문장도 존재한다.
<br/>토큰화의 기준인 문장 부호가 제역할을 하지 못하는 것이다.

> WRITERS RUSSELL BROWN, IRWIN LEVINE I'm comin' home, I've done my time Now I've got to know what is and isn't mine If you receive (생략)

##데이터셋 노이즈
"""

entire_sentence_num = 0
for i in raw_corpus:
  entire_sentence_num += 1

print('전체 문장 개수 : %d' % entire_sentence_num)

fifteen_sentence_num = 0
for i in raw_corpus:
  if len(i) <= 70:
    fifteen_sentence_num += 1

print('문장 길이 70 미만의 문장 개수 : %d' % fifteen_sentence_num)

num_difference = entire_sentence_num - fifteen_sentence_num
print('문장 길이 70 기준 전처리 이후 제외되는 문장 개수 : %d' % num_difference)

"""전처리를 통해 지나치게 긴 문장을 제외한 상황을 가정한다.

문장의 길이 70을 기준으로 전처리 이후 제외된 문장의 개수는 9367개이다.

9300개가 넘는 문장의 데이터를 잃지만 전처리를 통해 정제된 데이터를 얻을 수 있다.
"""

delete_data_rate = round((num_difference / entire_sentence_num) * 100, 2)
print('전처리로 제외된 데이터 비율 : %.2f ' % delete_data_rate)

"""전처리로 전체 데이터의 5.01%가 제외된다.

웹크롤링으로 가져온 데이터셋의 노이즈가 그만큼 많다는 것이다.

문장의 길이가 70을 넘어가는 문장을 제외하는 방법이 아닌 대안이 있어야하지 않을까?

웹에 업로드되어있는 데이터 자체의 문제로 인해 문장의 끝에 마침표가 없어서 
<br/>여러 문장이 합쳐져 '지나치게 긴 하나의 문장'이 된 노이즈를 
<br/>여러 문장으로 분할하여 '하나의 문장'이라는 정제된 데이터로 변환시켜주는 
<br/>전처리 기술이 개발되어야하지 않을까?

그 전처리 기술이 발달이 되면 5.01%와 같은 데이터양의 축소를 하지 않고
<br/>더 많은 웹크롤링 텍스트 데이터의 양을 확보할 수 있을 것이다.
"""

max_len = max([len(i) for i in raw_corpus if len(i) <= 70])
sum_len = sum([len(i) for i in raw_corpus if 1 <= len(i) <= 70])
mean_len = sum_len / fifteen_sentence_num

print('최대 길이 : %d' % max_len)
print('최소 길이 : %d' % min([len(l) for l in raw_corpus if len(l) >= 1]))
print('평균 길이 : %f' % mean_len)
plt.hist([len(s) for s in raw_corpus if 1 <= len(s) <= 70], bins=50)
plt.xlabel('length of sample')
plt.ylabel('number of sample')
plt.show()

"""#데이터 전처리"""

def preprocess_sentence(sentence):
    sentence = sentence.lower().strip() 
    sentence = re.sub(r"([?.!,¿])", r" \1 ", sentence)
    sentence = re.sub(r'[" "]+', " ", sentence)
    sentence = re.sub(r"[^a-zA-Z?.!,¿]+", " ", sentence)
    sentence = sentence.strip()
    sentence = '<start> ' + sentence + ' <end>'
    return sentence


print(preprocess_sentence("This @_is ;;;sample        sentence."))

"""lower.strip은 소문자로 바꾸고, 양쪽 공백을 지운다.

sub을 이용해 특수문자 양쪽에 공백을 넣는다.
<br/>여러개의 공백은 하나의 공백으로 바꾼다.
<br/>a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다.

strip은 다시 양쪽 공백을 지운다.

문장 시작에는 start, 끝에는 end를 추가한다.
"""

corpus = []

for sentence in raw_corpus:
    if len(sentence) == 0: continue
    if sentence[-1] == ":": continue
    
    preprocessed_sentence = preprocess_sentence(sentence)
    corpus.append(preprocessed_sentence)
        
corpus[:3]

"""corpus 리스트에 정제된 문장을 담는다.

문장의 길이가 0인 문장은 넘어간다.

마지막에 기호 :가 있는 문장은 리스트에 담지 않고 넘어간다.
"""

def tokenize(corpus):
    tokenizer = tf.keras.preprocessing.text.Tokenizer(
        num_words=12000, 
        filters=' ',
        oov_token="<unk>"
    )
    
    tokenizer.fit_on_texts(corpus)
    tensor = tokenizer.texts_to_sequences(corpus)   
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  
    
    print(tensor,tokenizer)
    return tensor, tokenizer

tensor, tokenizer = tokenize(corpus)

"""tokenizer는 내부 단어장의 크기를 12000로 갖는다.
<br/>단어장에 포함되지 못한 단어는 < unk >로 저장한다.

준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.

padding='post'는 입력 데이터의 시퀀스 길이를 일정하게 맞춘다.
<br/>만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.

maxlen=15는 토큰의 개수가 15개 넘어가는 문장을 제외한다.
<br/>지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거할 필요가 있다.
"""

print(tensor[:3, :10])

"""생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력한다."""

for idx in tokenizer.index_word:
    print(idx, ":", tokenizer.index_word[idx])

    if idx >= 10: break

""" tokenizer에 구축된 단어 사전의 인덱스를 통해 단어 사전이 어떻게 구축되었는지 확인한다."""

src_input = tensor[:, :-1]  
tgt_input = tensor[:, 1:]    

print(src_input[0])
print(tgt_input[0])

"""tensor에서 마지막 토큰을 잘라내서 소스 문장 src_input을 생성한다.

마지막 토큰은 end가 아니라 pad일 가능성이 높다.

tensor에서 start를 잘라내서 타겟 문장 tgt_input을 생성한다.

#데이터 분리
"""

enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, 
                                                    tgt_input,    
                                                    test_size=0.2,   
                                                    random_state=1)  

print('enc_train 개수: ', len(enc_train),', enc_val 개수: ', len(enc_val))

"""sklearnn 모듈의 train_test_split를 이용해 데이터 분리를 한다.
<br/>소스 문장 src_input을 특징 데이터, 타겟 문장 tgt_input을 정답 데이터로 사용한다.
<br/>전체의 20%를 평가 데이터로 사용한다.
<br/>random_state는 데이터를 무작위로 정렬한다.
"""

print('Source Train: ', enc_train.shape)
print('Target Train: ', dec_train.shape)

BUFFER_SIZE = len(src_input)
BATCH_SIZE = 256
steps_per_epoch = len(src_input) // BATCH_SIZE
VOCAB_SIZE = tokenizer.num_words + 1   

dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))
dataset = dataset.shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)
dataset

val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))
val_dataset = val_dataset.shuffle(BUFFER_SIZE)
val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)
val_dataset

"""batch size는 한 번에 네트워크에 넘겨주는 데이터의 수이다.
<br/>trade off로서 컴퓨터의 메모리 문제 때문에 분할해서 학습하는 것이다.

<br/>140599 ≥ 140544 = 256 x 549
<br/>train data num = batch size x step

<br/>35150 = 256 x 137 ≥ 35072
<br/>validation data num = batch size x step

<br/>주의할 점은 epoch와 batch size는 다른 개념이다.
<br/>epoch = batch size x step

vocab size는 tokenizer가 구축한 단어사전 내 7000개, 0 : pad를 포함하여 7001개를 포함한다.

#모델 구성

tf.keras.Model을 Subclassing하는 방식으로 만든다.
<br/>Embedding 레이어 1개, LSTM 레이어 2개, Dense 레이어 1개로 구성된다
"""

class TextGenerator(tf.keras.Model):
    def __init__(self, vocab_size, embedding_size, hidden_size):
        super().__init__()
        
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)
        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)
        self.linear = tf.keras.layers.Dense(vocab_size)
        
    def call(self, x):
        out = self.embedding(x)
        out = self.rnn_1(out)
        out = self.rnn_2(out)
        out = self.linear(out)
        
        return out

"""학습 모델 model1
<br/>하이퍼파라미터를 다르게 설정한 학습 모델 model2
<br/>평가 모델 model3으로 구성한다.
"""

embedding_size = 256
hidden_size = 1024
model1 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)

"""Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꾼다.
<br/>워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용된다.
"""

for src_sample, tgt_sample in dataset.take(1): break

model1(src_sample)

"""데이터셋에서 데이터 한 배치만 불러온다.
<br/>한 배치만 불러온 데이터를 모델에 넣는다.

model의 input shape가 결정되면서 model.build()가 자동으로 호출된다.
<br/>
"""

model1.summary()

"""#모델 학습"""

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True,
    reduction='none'
)

model1.compile(loss=loss, optimizer=optimizer)
model1.fit(dataset, epochs=10)

"""model1의 train loss는 2.2413이다.

#하이퍼파라미터

model2에서 하이퍼파라미터를 다음과 같이 설정한다.
> embedding_size = 256 → 600
<br/>hidden_size = 1024 → 2048
"""

embedding_size = 600
hidden_size = 2048
model2 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)

for src_sample, tgt_sample in dataset.take(1): break

model2(src_sample)

model2.summary()

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True,
    reduction='none'
)

model2.compile(loss=loss, optimizer=optimizer)
model2.fit(dataset, epochs=10)

"""model2의 train loss는 1.2162이다.

#모델 평가

성능이 높은 model2의 하이퍼파라미터 값을 평가 모델 model3에 적용한다.
> embedding_size = 600
<br/>hidden_size = 2048
"""

embedding_size = 600
hidden_size = 2048
model3 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)

for src_sample, tgt_sample in val_dataset.take(1): break

model3(src_sample)

model3.summary()

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True,
    reduction='none'
)

model3.compile(loss=loss, optimizer=optimizer)
model3.fit(val_dataset, epochs=10)

"""model3의 valadation loss는 1.7857이다.

#가사 생성
"""

def generate_text(model, tokenizer, init_sentence="<start>", max_len=20):
   
    test_input = tokenizer.texts_to_sequences([init_sentence])
    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)
    end_token = tokenizer.word_index["<end>"]

    while True:
        # 1
        predict = model(test_tensor) 
        # 2
        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] 
        # 3 
        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)
        # 4
        if predict_word.numpy()[0] == end_token: break
        if test_tensor.shape[1] >= max_len: break

    generated = ""
    for word_index in test_tensor[0].numpy():
        generated += tokenizer.index_word[word_index] + " "

    return generated

"""tf.convert_to_tensor는 테스트를 위해서 입력받은 init_sentence도 텐서로 변환한다.

while Loop에서 단어를 하나씩 예측해 문장을 만든다.
<br/>1) 입력받은 문장의 텐서를 입력한다.
<br/>2) 예측된 값 중 가장 높은 확률인 word index를 뽑아낸다.
<br/>3) 2에서 예측된 word index를 문장 뒤에 붙인다.
<br/>4) 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마친다.

tokenizer를 이용해 word index를 단어로 하나씩 변환합니다

성능이 높은 학습 모델 model2를 사용해 가사를 생성한다.
"""

generate_text(model2, tokenizer, init_sentence="<start> i love")

generate_text(model2, tokenizer, init_sentence="<start> good")

generate_text(model2, tokenizer, init_sentence="<start> out")

generate_text(model2, tokenizer, init_sentence="<start> see")

generate_text(model2, tokenizer, init_sentence="<start> where")

"""#결론

**하이퍼 파라미터**

model2에서 하이퍼파라미터를 다음과 같이 설정한다.
> embedding_size = 256 → 600
<br/>hidden_size = 1024 → 2048

성능이 높은 model2의 하이퍼파라미터 값을 평가 모델 model3에 적용한다.
> embedding_size = 600
<br/>hidden_size = 2048

model1의 train loss는 2.2413이다.
<br/>model2의 train loss는 1.2162이다.
<br/>model3의 valadation loss는 1.7857이다.

하이퍼파라미터 설정으로 train loss는 감소했지만
<br/>train loss와 valadation loss의 차이는 0.5695이다.

embedding_size가 커지고 hidden_size가 깊어져서
과적합이 발생한 것인가?
<br/>하이퍼파라미터 값이 크다고 성능이 개선될거라는 보장은 없다.

또한 튜닝에 있어서 무슨 하이퍼파라미터를 선택해야 하는가?
<br/>embedding_size, hidden_size 튜닝은 무슨 시사점을 남기는가?

하이퍼파라미터 튜닝 결과에 대한 분석 방법을 알고자 튜닝 관련 논문을 찾아보았다.

[김준용, 박구락, 「RNN모델에서 하이퍼파라미터 변화에 따른 정확도와 손실 성능」, 중소기업융합학회, 『분석융합정보논문지 v.11 no.7』, 2021, pp.31 - 38](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002740372)

위의 논문에서는 LSTM의 unit, batch_size, embedding_size를 설정한다.

unit, batch_size, embedding_size 튜닝은 무슨 시사점을 남기는가?
> embedding_size가 가장 영향력이 큰 하이퍼파라미터였다.
<br/>Unit의 개수가 적을수록 batch_size가 클수록 성능이 높아진다.
<br/>Embedding size가 커지면서 성능이 높아졌다가 낮아진다.

이렇게 하이퍼파라미터 튜닝의 결과에 대해 분석할 수 있었던 이유는
<br/>연구자가 하이퍼파라미터 변화에 따른 Loss와 Accuracy의 추이를 표로 정리했기 때문이다.

모델의 변화에 따른 학습 결과를 문서로 정리할 수 있는 알고리즘을 짜는 방법을 알고 싶다.

**텍스트 데이터셋**

데이터 노이즈에 대한 사전지식을 갖추고 웹크롤링과 데이터 전처리를 진행해야한다.

문장 부호가 작성되지 않았거나 띄어쓰기가 되지 않은 상황에서 토큰의 단위에 따라 어떻게 분리하는가?

#참고문헌

**LMS**
<br/>[dev-sngwn](https://github.com/dev-sngwn)

<br/>**논문**
<br/>[김준용, 박구락, 「RNN모델에서 하이퍼파라미터 변화에 따른 정확도와 손실 성능」, 중소기업융합학회, 『분석융합정보논문지 v.11 no.7』, 2021, pp.31 - 38](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002740372)

<br/>**단행본**
<br/>유원준 외 1명, 『PyTorch로 시작하는 딥러닝 입문』, Wikidocs, 2019
<br/>[자연어 처리 전처리 이해하기](https://wikidocs.net/64517)
<br/><br/>조대표, 『파이썬으로 배우는 알고리즘 트레이딩』, 위키북스, 2019
<br/>[OS 모듈](https://wikidocs.net/3141)

<br/>**Github**
<br/>trekhleb 
<br/>[Shakespeare Text Generation (using RNN LSTM)](https://github.com/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)

<br/>**Kaggle**
<br/>[song_lyrics](https://www.kaggle.com/datasets/paultimothymooney/poetry)

<br/>**웹사이트**
<br/>[Python glob.glob() 사용법](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=siniphia&logNo=221397012627)
<br/>[Python re 모듈 사용법](https://brownbears.tistory.com/506)
<br/>[Python 리스트(List)와 리스트 메소드(append, insert, remove, pop, extend)](https://velog.io/@falling_star3/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%98-%EB%A6%AC%EC%8A%A4%ED%8A%B8List%EC%99%80-%EA%B4%80%EB%A0%A8-%ED%95%A8%EC%88%98%EB%93%A4append-insert-remove-pop-extend)
<br/>[파이썬 문자열을 리스트로 만들기 – split, splitlines](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=debolm74&logNo=221953881991)
<br/>[텍스트 전처리(정규화)](https://blockchainstudy.tistory.com/58)
<br/>[PYTHON / NLTK 텍스트 파일 문장 단위로 분해하기 (SENTENCE TOKENIZE)](https://cryptosalamander.tistory.com/140)
<br/>[Python에서 문자열의 단어 계산](https://www.delftstack.com/ko/howto/python/python-count-words-in-string/)
<br/>[15.Batch size & Batch Norm](https://nittaku.tistory.com/293)
"""